======================================================================
paper_id: 80628431; YEAR: 2019
TITLE: Studying the Inductive Biases of RNNs with Synthetic Variations of Natural Languages
ABSTRACT: background_label: How do typological properties such as word order and morphological case marking affect the ability of neural sequence models to acquire the syntax of a language?
background_label: Cross-linguistic comparisons of RNNs' syntactic performance (e.g., on subject-verb agreement prediction) are complicated by the fact that any two languages differ in multiple typological properties, as well as by differences in training corpus.
method_label: We propose a paradigm that addresses these issues: we create synthetic versions of English, which differ from English in one or more typological parameters, and generate corpora for those languages based on a parsed English corpus.
method_label: We report a series of experiments in which RNNs were trained to predict agreement features for verbs in each of those synthetic languages.
result_label: Among other findings, (1) performance was higher in subject-verb-object order (as in English) than in subject-object-verb order (as in Japanese), suggesting that RNNs have a recency bias; (2) predicting agreement with both subject and object (polypersonal agreement) improves over predicting each separately, suggesting that underlying syntactic knowledge transfers across the two tasks; and (3) overt morphological case makes agreement prediction significantly easier, regardless of word order.
===================================
paper_id: 52113185; YEAR: 2018
adju relevance: Identical (+3)
difference: 2; annotator2: 1; annotator3: 3
sources: cited - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Targeted Syntactic Evaluation of Language Models
ABSTRACT: background_label: We present a dataset for evaluating the grammaticality of the predictions of a language model.
background_label: We automatically construct a large number of minimally different pairs of English sentences, each consisting of a grammatical and an ungrammatical sentence.
background_label: The sentence pairs represent different variations of structure-sensitive phenomena: subject-verb agreement, reflexive anaphora and negative polarity items.
method_label: We expect a language model to assign a higher probability to the grammatical sentence than the ungrammatical one.
method_label: In an experiment using this data set, an LSTM language model performed poorly on many of the constructions.
result_label: Multi-task training with a syntactic objective (CCG supertagging) improved the LSTM's accuracy, but a large gap remained between its performance and the accuracy of human participants recruited online.
result_label: This suggests that there is considerable room for improvement over LSTMs in capturing syntax in a language model.

===================================
paper_id: 28123397; YEAR: 2017
adju relevance: Similar (+2)
difference: 2; annotator2: 0; annotator3: 2
sources: title_tfidf
TITLE: Generating synthetic mobility traffic using RNNs
ABSTRACT: background_label: Mobility trajectory datasets are fundamental for system evaluation and experimental reproducibility.
background_label: Privacy concerns today however, have restricted sharing of such datasets.
background_label: This has led to the development of synthetic traffic generators, which simulate moving entities to create pseudo-realistic trajectory datasets.
background_label: Existing work on traffic generation, superficially matches a-priori modeled mobility characteristics, which lacks realism and does not capture the substantive properties of human mobility.
background_label: Critical applications however, require data that contains these complex, candid and hidden mobility patterns.
objective_label: To this end, we investigate the effectiveness of Recurrent Neural Networks (RNN) to learn these hidden patterns contained in an original dataset to produce a realistic synthetic dataset.
method_label: We observe that, the ability of RNNs to learn and model problems over sequential data having long-term temporal dependencies is ideal for capturing the inherent properties of location traces.
method_label: Additionally, the lack of intuitive high-level spatiotemporal structure and instability, guarantees trajectories that are different from the ones seen in the training dataset.
result_label: Our preliminary evaluation results show that, our model effectively captures the sleep cycles and stay-points commonly observed in the considered training dataset, along with preserving the statistical characteristics and probability distributions of the movement transitions.
result_label: Although, many questions remain to be answered, we show that generating synthetic traffic by learning the innate structure of human mobility through RNNs is a promising approach.

===================================
paper_id: 4460159; YEAR: 2018
adju relevance: Similar (+2)
difference: 1; annotator2: 1; annotator3: 2
sources: cited - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Colorless green recurrent networks dream hierarchically
ABSTRACT: background_label: Recurrent neural networks (RNNs) have achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties of language.
objective_label: We investigate here to what extent RNNs learn to track abstract hierarchical syntactic structure.
method_label: We test whether RNNs trained with a generic language modeling objective in four languages (Italian, English, Hebrew, Russian) can predict long-distance number agreement in various constructions.
method_label: We include in our evaluation nonsensical sentences where RNNs cannot rely on semantic or lexical cues ("The colorless green ideas I ate with the chair sleep furiously"), and, for Italian, we compare model performance to human intuitions.
result_label: Our language-model-trained RNNs make reliable predictions about long-distance agreement, and do not lag much behind human performance.
result_label: We thus bring support to the hypothesis that RNNs are not just shallow-pattern extractors, but they also acquire deeper grammatical competence.

===================================
paper_id: 49363457; YEAR: 2018
adju relevance: Similar (+2)
difference: 1; annotator2: 1; annotator3: 2
sources: cited - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Assessing Composition in Sentence Vector Representations
ABSTRACT: background_label: An important component of achieving language understanding is mastering the composition of sentence meaning, but an immediate challenge to solving this problem is the opacity of sentence vector representations produced by current neural sentence composition models.
method_label: We present a method to address this challenge, developing tasks that directly target compositional meaning information in sentence vector representations with a high degree of precision and control.
method_label: To enable the creation of these controlled tasks, we introduce a specialized sentence generation system that produces large, annotated sentence sets meeting specified syntactic, semantic and lexical constraints.
method_label: We describe the details of the method and generation system, and then present results of experiments applying our method to probe for compositional information in embeddings from a number of existing sentence composition models.
result_label: We find that the method is able to extract useful information about the differing capacities of these models, and we discuss the implications of our results with respect to these systems' capturing of sentence information.
result_label: We make available for public use the datasets used for these experiments, as well as the generation system.

===================================
paper_id: 52165443; YEAR: 2018
adju relevance: Similar (+2)
difference: 2; annotator2: 0; annotator3: 2
sources: specter
TITLE: RNNs as psycholinguistic subjects: Syntactic state and grammatical dependency
ABSTRACT: background_label: Recurrent neural networks (RNNs) are the state of the art in sequence modeling for natural language.
background_label: However, it remains poorly understood what grammatical characteristics of natural language they implicitly learn and represent as a consequence of optimizing the language modeling objective.
method_label: Here we deploy the methods of controlled psycholinguistic experimentation to shed light on to what extent RNN behavior reflects incremental syntactic state and grammatical dependency representations known to characterize human linguistic behavior.
method_label: We broadly test two publicly available long short-term memory (LSTM) English sequence models, and learn and test a new Japanese LSTM.
result_label: We demonstrate that these models represent and maintain incremental syntactic state, but that they do not always generalize in the same way as humans.
result_label: Furthermore, none of our models learn the appropriate grammatical dependency configurations licensing reflexive pronouns or negative polarity items.

===================================
paper_id: 16562917; YEAR: 2016
adju relevance: Similar (+2)
difference: 0; annotator2: 2; annotator3: 2
sources: title_tfidfcbow200 - title_cbow200
TITLE: Structural priming in artificial languages and the regularisation of unpredictable variation
ABSTRACT: background_label: We present a novel experimental technique using artificial language learning to investigate the relationship between structural priming during communicative interaction, and linguistic regularity.
background_label: We use unpredictable variation as a test-case, because it is a well-established paradigm to study learners’ biases during acquisition, transmission and interaction.
method_label: We trained participants on artificial languages exhibiting unpredictable variation in word order, and subsequently had them communicate using these artificial languages.
method_label: We found evidence for structural priming in two different grammatical constructions and across human-human and human-computer interaction.
method_label: Priming occurred regardless of behavioral convergence: communication led to shared word order use only in human-human interaction, but priming was observed in all conditions.
method_label: Furthermore, interaction resulted in the reduction of unpredictable variation in all conditions, suggesting a role for communicative interaction in eliminating unpredictable variation.
result_label: Regularisation was strongest in human-human interaction and in a condition where participants believed they were interacting with a human but were in fact interacting with a computer.
result_label: We suggest that participants recognize the counter-functional nature of unpredictable variation and thus act to eliminate this variability during communication.
result_label: Furthermore, reciprocal priming occurring in human-human interaction drove some pairs of participants to converge on maximally regular, highly predictable linguistic systems.
result_label: Our method offers potential benefits to both the artificial language learning and the structural priming fields, and provides a useful tool to investigate communicative processes that lead to language change and ultimately language design.

===================================
paper_id: 3580012; YEAR: 2018
adju relevance: Similar (+2)
difference: 2; annotator2: 0; annotator3: 2
sources: cited - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Revisiting the poverty of the stimulus: hierarchical generalization without a hierarchical bias in recurrent neural networks
ABSTRACT: background_label: Syntactic rules in natural language typically need to make reference to hierarchical sentence structure.
background_label: However, the simple examples that language learners receive are often equally compatible with linear rules.
background_label: Children consistently ignore these linear explanations and settle instead on the correct hierarchical one.
background_label: This fact has motivated the proposal that the learner's hypothesis space is constrained to include only hierarchical rules.
method_label: We examine this proposal using recurrent neural networks (RNNs), which are not constrained in such a way.
method_label: We simulate the acquisition of question formation, a hierarchical transformation, in a fragment of English.
method_label: We find that some RNN architectures tend to learn the hierarchical rule, suggesting that hierarchical cues within the language, combined with the implicit architectural biases inherent in certain RNNs, may be sufficient to induce hierarchical generalizations.
result_label: The likelihood of acquiring the hierarchical generalization increased when the language included an additional cue to hierarchy in the form of subject-verb agreement, underscoring the role of cues to hierarchy in the learner's input.

===================================
paper_id: 10817864; YEAR: 2017
adju relevance: Similar (+2)
difference: 1; annotator2: 1; annotator3: 2
sources: cited - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: The Galactic Dependencies Treebanks: Getting More Data by Synthesizing New Languages
ABSTRACT: background_label: We release Galactic Dependencies 1.0---a large set of synthetic languages not found on Earth, but annotated in Universal Dependencies format.
objective_label: This new resource aims to provide training and development data for NLP methods that aim to adapt to unfamiliar languages.
method_label: Each synthetic treebank is produced from a real treebank by stochastically permuting the dependents of nouns and/or verbs to match the word order of other real languages.
method_label: We discuss the usefulness, realism, parsability, perplexity, and diversity of the synthetic languages.
method_label: As a simple demonstration of the use of Galactic Dependencies, we consider single-source transfer, which attempts to parse a real target language using a parser trained on a"nearby"source language.
result_label: We find that including synthetic source languages somewhat increases the diversity of the source pool, which significantly improves results for most target languages.

===================================
paper_id: 24461982; YEAR: 2018
adju relevance: Related (+1)
difference: 2; annotator2: 0; annotator3: 2
sources: cited - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: What you can cram into a single vector: Probing sentence embeddings for linguistic properties
ABSTRACT: background_label: Although much effort has recently been devoted to training high-quality sentence embeddings, we still have a poor understanding of what they are capturing.
background_label: "Downstream"tasks, often based on sentence classification, are commonly used to evaluate the quality of sentence representations.
background_label: The complexity of the tasks makes it however difficult to infer what kind of information is present in the representations.
method_label: We introduce here 10 probing tasks designed to capture simple linguistic features of sentences, and we use them to study embeddings generated by three different encoders trained in eight distinct ways, uncovering intriguing properties of both encoders and training methods.

===================================
paper_id: 5421814; YEAR: 2013
adju relevance: Related (+1)
difference: 0; annotator2: 1; annotator3: 1
sources: cited - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: A noisy-channel account of crosslinguistic word-order variation.
ABSTRACT: background_label: The distribution of word orders across languages is highly nonuniform, with subject-verb-object (SVO) and subject-object-verb (SOV) orders being prevalent.
background_label: Recent work suggests that the SOV order may be the default in human language.
background_label: Why, then, is SVO order so common?
objective_label: We hypothesize that SOV/SVO variation can be explained by language users' sensitivity to the possibility of noise corrupting the linguistic signal.
method_label: In particular, the noisy-channel hypothesis predicts a shift from the default SOV order to SVO order for semantically reversible events, for which potential ambiguity arises in SOV order because two plausible agents appear on the same side of the verb.
result_label: We found support for this prediction in three languages (English, Japanese, and Korean) by using a gesture-production task, which reflects word-order preferences largely independent of native language.
result_label: Other patterns of crosslinguistic variation (e.g., the prevalence of case marking in SOV languages and its relative absence in SVO languages) also straightforwardly follow from the noisy-channel hypothesis.

===================================
paper_id: 52191945; YEAR: 2018
adju relevance: Related (+1)
difference: 2; annotator2: 0; annotator3: 2
sources: cited - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Can LSTM Learn to Capture Agreement? The Case of Basque
ABSTRACT: background_label: Sequential neural networks models are powerful tools in a variety of Natural Language Processing (NLP) tasks.
background_label: The sequential nature of these models raises the questions: to what extent can these models implicitly learn hierarchical structures typical to human language, and what kind of grammatical phenomena can they acquire?
objective_label: We focus on the task of agreement prediction in Basque, as a case study for a task that requires implicit understanding of sentence structure and the acquisition of a complex but consistent morphological system.
result_label: Analyzing experimental results from two syntactic prediction tasks -- verb number prediction and suffix recovery -- we find that sequential models perform worse on agreement prediction in Basque than one might expect on the basis of a previous agreement prediction work in English.
result_label: Tentative findings based on diagnostic classifiers suggest the network makes use of local heuristics as a proxy for the hierarchical structure of the sentence.
result_label: We propose the Basque agreement prediction task as challenging benchmark for models that attempt to learn regularities in human language.

===================================
paper_id: 7821379; YEAR: 2017
adju relevance: Related (+1)
difference: 0; annotator2: 1; annotator3: 1
sources: cited - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Exploring the Syntactic Abilities of RNNs with Multi-task Learning
ABSTRACT: background_label: Recent work has explored the syntactic abilities of RNNs using the subject-verb agreement task, which diagnoses sensitivity to sentence structure.
background_label: RNNs performed this task well in common cases, but faltered in complex sentences (Linzen et al., 2016).
objective_label: We test whether these errors are due to inherent limitations of the architecture or to the relatively indirect supervision provided by most agreement dependencies in a corpus.
method_label: We trained a single RNN to perform both the agreement task and an additional task, either CCG supertagging or language modeling.
result_label: Multi-task training led to significantly lower error rates, in particular on complex sentences, suggesting that RNNs have the ability to evolve more sophisticated syntactic representations than shown before.
result_label: We also show that easily available agreement training data can improve performance on other syntactic tasks, in particular when only a limited amount of training data is available for those tasks.
result_label: The multi-task paradigm can also be leveraged to inject grammatical knowledge into language models.

===================================
paper_id: 3502245; YEAR: 2018
adju relevance: Related (+1)
difference: 1; annotator2: 2; annotator3: 1
sources: abs_cbow200 - abs_tfidf - abs_tfidfcbow200 - specter
TITLE: From Phonology to Syntax: Unsupervised Linguistic Typology at Different Levels with Language Embeddings
ABSTRACT: background_label: A core part of linguistic typology is the classification of languages according to linguistic properties, such as those detailed in the World Atlas of Language Structure (WALS).
background_label: Doing this manually is prohibitively time-consuming, which is in part evidenced by the fact that only 100 out of over 7,000 languages spoken in the world are fully covered in WALS.
method_label: We learn distributed language representations, which can be used to predict typological properties on a massively multilingual scale.
method_label: Additionally, quantitative and qualitative analyses of these language embeddings can tell us how language similarities are encoded in NLP models for tasks at different typological levels.
method_label: The representations are learned in an unsupervised manner alongside tasks at three typological levels: phonology (grapheme-to-phoneme prediction, and phoneme reconstruction), morphology (morphological inflection), and syntax (part-of-speech tagging).
method_label: We consider more than 800 languages and find significant differences in the language representations encoded, depending on the target task.
result_label: For instance, although Norwegian Bokm{\aa}l and Danish are typologically close to one another, they are phonologically distant, which is reflected in their language embeddings growing relatively distant in a phonological task.
result_label: We are also able to predict typological features in WALS with high accuracies, even for unseen language families.

===================================
paper_id: 52090220; YEAR: 2018
adju relevance: Related (+1)
difference: 0; annotator2: 1; annotator3: 1
sources: cited - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Under the Hood: Using Diagnostic Classifiers to Investigate and Improve how Language Models Track Agreement Information
ABSTRACT: background_label: How do neural language models keep track of number agreement between subject and verb?
background_label: We show that `diagnostic classifiers', trained to predict number from the internal states of a language model, provide a detailed understanding of how, when, and where this information is represented.
method_label: Moreover, they give us insight into when and where number information is corrupted in cases where the language model ends up making agreement errors.
method_label: To demonstrate the causal role played by the representations we find, we then use agreement information to influence the course of the LSTM during the processing of difficult sentences.
result_label: Results from such an intervention reveal a large increase in the language model's accuracy.
result_label: Together, these results show that diagnostic classifiers give us an unrivalled detailed look into the representation of linguistic information in neural models, and demonstrate that this knowledge can be used to improve their performance.

===================================
paper_id: 6352511; YEAR: 2012
adju relevance: Related (+1)
difference: 1; annotator2: 1; annotator3: 2
sources: specter - abs_tfidf
TITLE: Learning biases predict a word order universal.
ABSTRACT: background_label: How recurrent typological patterns, or universals, emerge from the extensive diversity found across the world's languages constitutes a central question for linguistics and cognitive science.
background_label: Recent challenges to a fundamental assumption of generative linguistics-that universal properties of the human language acquisition faculty constrain the types of grammatical systems which can occur-suggest the need for new types of empirical evidence connecting typology to biases of learners.
method_label: Using an artificial language learning paradigm in which adult subjects are exposed to a mix of grammatical systems (similar to a period of linguistic change), we show that learners' biases mirror a word-order universal, first proposed by Joseph Greenberg, which constrains typological patterns of adjective, numeral, and noun ordering.
result_label: We briefly summarize the results of a probabilistic model of the hypothesized biases and their effect on learning, and discuss the broader implications of the results for current theories of the origins of cross-linguistic word-order preferences.

===================================
paper_id: 145055053; YEAR: 2018
adju relevance: Related (+1)
difference: 1; annotator2: 1; annotator3: 0
sources: specter
TITLE: Measuring Semantic Abstraction of Multilingual NMT with Paraphrase Recognition and Generation Tasks
ABSTRACT: background_label: In this paper, we investigate whether multilingual neural translation models learn stronger semantic abstractions of sentences than bilingual ones.
background_label: We test this hypotheses by measuring the perplexity of such models when applied to paraphrases of the source language.
method_label: The intuition is that an encoder produces better representations if a decoder is capable of recognizing synonymous sentences in the same language even though the model is never trained for that task.
method_label: In our setup, we add 16 different auxiliary languages to a bidirectional bilingual baseline model (English-French) and test it with in-domain and out-of-domain paraphrases in English.
result_label: The results show that the perplexity is significantly reduced in each of the cases, indicating that meaning can be grounded in translation.
result_label: This is further supported by a study on paraphrase generation that we also include at the end of the paper.

===================================
paper_id: 2078255; YEAR: 2017
adju relevance: Related (+1)
difference: 2; annotator2: 0; annotator3: 2
sources: abs_cbow200 - specter
TITLE: From Characters to Words to in Between: Do We Capture Morphology?
ABSTRACT: background_label: Words can be represented by composing the representations of subword units such as word segments, characters, and/or character n-grams.
background_label: While such representations are effective and may capture the morphological regularities of words, they have not been systematically compared, and it is not understood how they interact with different morphological typologies.
method_label: On a language modeling task, we present experiments that systematically vary (1) the basic unit of representation, (2) the composition of these representations, and (3) the morphological typology of the language modeled.
result_label: Our results extend previous findings that character representations are effective across typologies, and we find that a previously unstudied combination of character trigram representations composed with bi-LSTMs outperforms most others.
result_label: But we also find room for improvement: none of the character-level models match the predictive accuracy of a model with access to true morphological analyses, even when learned from an order of magnitude more data.

===================================
paper_id: 17499628; YEAR: 2004
adju relevance: Related (+1)
difference: 0; annotator2: 1; annotator3: 1
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: Can CANISO activate CASINO? Transposed-letter similarity effects with nonadjacent letter positions q
ABSTRACT: background_label: Nonwords created by transposing two adjacent letters (i.e., transposed-letter (TL) nonwords like jugde) are very effective at activating the lexical representation of their base words.
background_label: This fact poses problems for most computational models of word recognition (e.g., the interactive-activation model and its extensions), which assume that exact letter positions are rapidly coded during the word recognition process.
method_label: To examine the scope of TL similarity effects further, we asked whether TL similarity effects occur for nonwords created by exchanging two nonadjacent letters (e.g., canisoCASINO) in three masked form priming experiments using the lexical decision task.
method_label: The two nonadjacent transposed letters were consonants in Experiment 1 (e.g., caniso-CASINO), vowels in Experiment 2 (anamil-ANIMAL) and both consonants and vowels in Experiment 3.
result_label: Results showed that nonadjacent TL primes produce priming effects (in comparison to orthographic controls, e.g., caviro-CASINO), however, only when the transposed letters are consonants.
result_label: In a final experiment we examined latencies for nonwords created by nonadjacent transpositions of consonants versus vowels in a lexical decision task.
result_label: Both types of nonwords produced longer latencies than matched controls, with consonant TL nonwords being more difficult than vowel TL nonwords.
result_label: The implications of these findings for models having ‘‘position-specific’’ coding schemes as well as for models proposing alternative coding schemes are discussed.

===================================
paper_id: 13860931; YEAR: 2003
adju relevance: Related (+1)
difference: 0; annotator2: 1; annotator3: 1
sources: abs_cbow200 - abs_tfidf - abs_tfidfcbow200
TITLE: Processing relative clauses in Chinese
ABSTRACT: background_label: This paper reports results from a self-paced reading study in Chinese that demonstrates that object-extracted relative clause structures are less complex than corresponding subject-extracted structures.
background_label: These results contrast with results from processing other Subject-Verb-Object languages like English, in which object-extracted structures are more complex than subject-extracted structures.
background_label: A key word-order difference between Chinese and other Subject-Verb-Object languages is that Chinese relative clauses precede their head nouns.
method_label: Because of this word order difference, the results follow from a resource-based theory of sentence complexity, according to which there is a storage cost associated with predicting syntactic heads in order to form a grammatical sentence.
result_label: The results are also consistent with a theory according to which people have less difficulty processing embedded clauses whose word order matches the word order in main clauses.
result_label: Some corpus analyses of Chinese texts provide results that constrain the classes of possible frequency-based theories.
result_label: Critically, these results demonstrate that there is nothing intrinsically easy about extracting from subject position: depending on the word order in the main clause and in a relative clause, extraction from object position can be easier to process in some circumstances.

===================================
paper_id: 9426092; YEAR: 2014
adju relevance: Related (+1)
difference: 1; annotator2: 1; annotator3: 0
sources: cited - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Language learners privilege structured meaning over surface frequency.
ABSTRACT: background_label: Although it is widely agreed that learning the syntax of natural languages involves acquiring structure-dependent rules, recent work on acquisition has nevertheless attempted to characterize the outcome of learning primarily in terms of statistical generalizations about surface distributional information.
objective_label: In this paper we investigate whether surface statistical knowledge or structural knowledge of English is used to infer properties of a novel language under conditions of impoverished input.
method_label: We expose learners to artificial-language patterns that are equally consistent with two possible underlying grammars--one more similar to English in terms of the linear ordering of words, the other more similar on abstract structural grounds.
method_label: We show that learners' grammatical inferences overwhelmingly favor structural similarity over preservation of superficial order.
method_label: Importantly, the relevant shared structure can be characterized in terms of a universal preference for isomorphism in the mapping from meanings to utterances.
result_label: Whereas previous empirical support for this universal has been based entirely on data from cross-linguistic language samples, our results suggest it may reflect a deep property of the human cognitive system--a property that, together with other structure-sensitive principles, constrains the acquisition of linguistic knowledge.

===================================
paper_id: 17822109; YEAR: 1990
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200 - abs_tfidf
TITLE: The case of the nonce loan in Tamil
ABSTRACT: background_label: Nonce borrowings in the speech of bilinguals differ from established loanwords in that they are not necessarily recurrent, widespread, or recognized by host language monolinguals.
background_label: With established loanwords, however, they share the characteristics of morphological and syntactic integration into the host language and consist of single content words or compounds.
background_label: Furthermore, both types of loanwords differ fro m intrasentential code-switching —alternate sentence fragments in the two languages, each of which is grammatical by monolingual standards from the standpoints of appropriate function words, morphology, and syntax.
background_label: In a large corpus of Tamil-English bilingual speech, many words of English origin are found in objects governed by Tamil verbs and vice versa.
background_label: The equivalence constraint on intrasentential code-switching predicts that no code-switch should occur between verb and object in an SOV/SVO bilingual situation, and hence that objects whose language differs from that of the verb must be borrowed, if only for the nonce.
result_label: To verify this prediction, we compare quantitatively the distribution across various syntactic contexts of both native Tamil and English-origin complements of Tamil verbs, and find them to be parallel.
result_label: But the strongest evidence in favor of the nonce borrowing hypothesis comes from an analysis of variable accusative and dative case marking in these complements, in which the English-origin material is shown, morphologically and syntactically, to be virtually indistinguishable from Tamil (nonpronominal) nouns.
result_label: In addition, we present supporting evidence from the genitive, locative, and other cases and from nonce borrowings from Tamil into these speakers' English.

===================================
paper_id: 52144417; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 1; annotator2: 0; annotator3: 1
sources: cited - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Do Language Models Understand Anything? On the Ability of LSTMs to Understand Negative Polarity Items
ABSTRACT: objective_label: In this paper, we attempt to link the inner workings of a neural language model to linguistic theory, focusing on a complex phenomenon well discussed in formal linguis- tics: (negative) polarity items.
method_label: We briefly discuss the leading hypotheses about the licensing contexts that allow negative polarity items and evaluate to what extent a neural language model has the ability to correctly process a subset of such constructions.
method_label: We show that the model finds a relation between the licensing context and the negative polarity item and appears to be aware of the scope of this context, which we extract from a parse tree of the sentence.
result_label: With this research, we hope to pave the way for other studies linking formal linguistics to deep learning.

===================================
paper_id: 15991124; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200
TITLE: Chinese Word Ordering Errors Detection and Correction for Non-Native Chinese Language Learners
ABSTRACT: background_label: AbstractWord Ordering Errors (WOEs) are the most frequent type of grammatical errors at sentence level for non-native Chinese language learners.
background_label: Learners taking Chinese as a foreign language often place character(s) in the wrong places in sentences, and that results in wrong word(s) or ungrammatical sentences.
background_label: Besides, there are no clear word boundaries in Chinese sentences.
background_label: That makes WOEs detection and correction more challenging.
objective_label: In this paper, we propose methods to detect and correct WOEs in Chinese sentences.
method_label: Conditional random fields (CRFs) based WOEs detection models identify the sentence segments containing WOEs.
background_label: Segment point-wise mutual information (PMI), inter-segment PMI difference, language model, tag of the previous segment, and CRF bigram template are explored.
background_label: Words in the segments containing WOEs are reordered to generate candidates that may have correct word orderings.
method_label: Ranking SVM based models rank the candidates and suggests the most proper corrections.
method_label: Training and testing sets are selected from HSK dynamic composition corpus created by Beijing Language and Culture University.
method_label: Besides the HSK WOE dataset, Google Chinese Web 5-gram corpus is used to learn features for WOEs detection and correction.
result_label: The best model achieves an accuracy of 0.834 for detecting WOEs in sentence segments.
result_label: On the average, the correct word orderings are ranked 4.8 among 184.48 candidates.

===================================
paper_id: 3829825; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Co-occurrence of the Benford-like and Zipf Laws Arising from the Texts Representing Human and Artificial Languages
ABSTRACT: background_label: We demonstrate that large texts, representing human (English, Russian, Ukrainian) and artificial (C++, Java) languages, display quantitative patterns characterized by the Benford-like and Zipf laws.
background_label: The frequency of a word following the Zipf law is inversely proportional to its rank, whereas the total numbers of a certain word appearing in the text generate the uneven Benford-like distribution of leading numbers.
method_label: Excluding the most popular words essentially improves the correlation of actual textual data with the Zipfian distribution, whereas the Benford distribution of leading numbers (arising from the overall amount of a certain word) is insensitive to the same elimination procedure.
result_label: The calculated values of the moduli of slopes of double logarithmical plots for artificial languages (C++, Java) are markedly larger than those for human ones.

===================================
paper_id: 3249779; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: title_tfidf
TITLE: A Survey of Inductive Biases for Factorial Representation-Learning
ABSTRACT: background_label: With the resurgence of interest in neural networks, representation learning has re-emerged as a central focus in artificial intelligence.
background_label: Representation learning refers to the discovery of useful encodings of data that make domain-relevant information explicit.
background_label: Factorial representations identify underlying independent causal factors of variation in data.
method_label: A factorial representation is compact and faithful, makes the causal factors explicit, and facilitates human interpretation of data.
method_label: Factorial representations support a variety of applications, including the generation of novel examples, indexing and search, novelty detection, and transfer learning.
result_label: This article surveys various constraints that encourage a learning algorithm to discover factorial representations.
background_label: I dichotomize the constraints in terms of unsupervised and supervised inductive bias.
background_label: Unsupervised inductive biases exploit assumptions about the environment, such as the statistical distribution of factor coefficients, assumptions about the perturbations a factor should be invariant to (e.g.
background_label: a representation of an object can be invariant to rotation, translation or scaling), and assumptions about how factors are combined to synthesize an observation.
background_label: Supervised inductive biases are constraints on the representations based on additional information connected to observations.
background_label: Supervisory labels come in variety of types, which vary in how strongly they constrain the representation, how many factors are labeled, how many observations are labeled, and whether or not we know the associations between the constraints and the factors they are related to.
result_label: This survey brings together a wide variety of models that all touch on the problem of learning factorial representations and lays out a framework for comparing these models based on the strengths of the underlying supervised and unsupervised inductive biases.

===================================
paper_id: 16760547; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 1; annotator2: 1; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: A Database of Paradigmatic Semantic Relation Pairs for German Nouns, Verbs, and Adjectives
ABSTRACT: background_label: AbstractA new collection of semantically related word pairs in German is presented, which was compiled via human judgement experiments and comprises (i) a representative selection of target lexical units balanced for semantic category, polysemy, and corpus frequency, (ii) a set of humangenerated semantically related word pairs based on the target units, and (iii) a subset of the generated word pairs rated for their relation strength, including positive and negative relation evidence.
method_label: We address the three paradigmatic relations antonymy, hypernymy and synonymy, and systematically work across the three word classes of adjectives, nouns, and verbs.
result_label: A series of quantitative and qualitative analyses demonstrates that (i) antonyms are more canonical than hypernyms and synonyms, (ii) relations are more or less natural with regard to the specific word classes, (iii) antonymy is clearly distinguishable from hypernymy and synonymy, but hypernymy and synonymy are often confused.
result_label: We anticipate that our new collection of semantic relation pairs will not only be of considerable use in computational areas in which semantic relations play a role, but also in studies in theoretical linguistics and psycholinguistics.

===================================
paper_id: 22611976; YEAR: 2005
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: title_tfidf
TITLE: A natural approach to studying vision
ABSTRACT: objective_label: An ultimate goal of systems neuroscience is to understand how sensory stimuli encountered in the natural environment are processed by neural circuits.
background_label: Achieving this goal requires knowledge of both the characteristics of natural stimuli and the response properties of sensory neurons under natural stimulation.
background_label: Most of our current notions of sensory processing have come from experiments using simple, parametric stimulus sets.
background_label: However, a growing number of researchers have begun to question whether this approach alone is sufficient for understanding the real-life sensory tasks performed by the organism.
result_label: Here, focusing on the early visual pathway, we argue that the use of natural stimuli is vital for advancing our understanding of sensory processing.

===================================
paper_id: 21700944; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: cited - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context
ABSTRACT: background_label: We know very little about how neural language models (LM) use prior linguistic context.
objective_label: In this paper, we investigate the role of context in an LSTM LM, through ablation studies.
method_label: Specifically, we analyze the increase in perplexity when prior context words are shuffled, replaced, or dropped.
method_label: On two standard datasets, Penn Treebank and WikiText-2, we find that the model is capable of using about 200 tokens of context on average, but sharply distinguishes nearby context (recent 50 tokens) from the distant history.
method_label: The model is highly sensitive to the order of words within the most recent sentence, but ignores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough semantic field or topic.
result_label: We further find that the neural caching model (Grave et al., 2017b) especially helps the LSTM to copy words from within this distant context.
result_label: Overall, our analysis not only provides a better understanding of how neural LMs use their context, but also sheds light on recent success from cache-based models.

===================================
paper_id: 51880056; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: specter - abs_tfidfcbow200
TITLE: An Evaluation of Two Vocabulary Reduction Methods for Neural Machine Translation
ABSTRACT: background_label: AbstractNeural machine translation (NMT) models are conventionally trained with fixed-size vocabularies to control the computational complexity and the quality of the learned word representations.
background_label: This, however, limits the accuracy and the generalization capability of the models, especially for morphologically-rich languages, which usually have very sparse vocabularies containing rare inflected or derivated word forms.
background_label: Some studies tried to overcome this problem by segmenting words into subword level representations and modeling translation at this level.
background_label: However, recent findings have shown that if these methods interrupt the word structure during segmentation, they might cause semantic or syntactic losses and lead to generating inaccurate translations.
method_label: In order to investigate this phenomenon, we present an extensive evaluation of two unsupervised vocabulary reduction methods in NMT.
method_label: The first is the wellknown byte-pair-encoding (BPE), a statistical subword segmentation method, whereas the second is linguistically-motivated vocabulary reduction (LMVR), a segmentation method which also considers morphological properties of subwords.
result_label: We compare both approaches on ten translation directions involving English and five other languages (Arabic, Czech, German, Italian and Turkish), each representing a distinct language family and morphological typology.
result_label: LMVR obtains significantly better performance in most languages, showing gains proportional to the sparseness of the vocabulary and the morphological complexity of the tested language.

===================================
paper_id: 9442505; YEAR: 2008
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: abs_cbow200
TITLE: Distributional Cues to Word Boundaries: Context is Important
ABSTRACT: background_label: Word segmentation, or identifying word boundaries in continuous speech, is one of the first problems that infants must solve as they are acquiring language.
background_label: A number of different weak cues to word boundaries are present in fluent speech, and there is evidence that infants are able to use many of these, including phonotactics (Mattys et al., 1999), allophonic variation (Jusczyk et al., 1999a), metrical (stress) patterns (Morgan et al., 1995; Jusczyk et al., 1999b), effects of coarticulation (Johnson and Jusczyk, 2001), and statistical regularities amongst sequences of syllables (Saffran et al., 1996a).
background_label: The kinds of statistical regularities studied by Saffran et al.
background_label: (1996a) allow for the possibility of language-independent word segmentation strategies, and seem to be used by infants earlier than other kinds of cues (Thiessen and Saffran, 2003).
background_label: These facts have led to the proposal that strategies exploiting the statistical patterns found in sound sequences are a crucial first step in bootstrapping word segmentation (Thiessen and Saffran, 2003), and have provoked a great deal of research into statistical word segmentation using both human subjects and computational models.
result_label: Most previous work on statistical word segmentation is based on the observation that transitions from one syllable or phoneme to the next tend to be less predictable at word boundaries than within words (Harris, 1955; Saffran et al., 1996a).
background_label: This observation has led to proposals that infants use statistics such as transitional probabilities or mutual information in order to segment words from speech.
background_label: A number of models have been developed in an attempt to explain how these kinds of statistics can be used procedurally to identify words or word boundaries.
objective_label: Here, we take a different approach: we seek to identify the assumptions the learner must make about the nature of language in order to correctly segment natural language input.
method_label: Observations about predictability at word boundaries are consistent with two different kinds of assumptions about what constitutes a word: either a word is a unit that is statistically independent of other units, or it is a unit that helps to predict other units (but to a lesser degree than the beginning of a word predicts its end).
method_label: In most artificial language experiments on word segmentation, the first assumption is adopted implicitly by creating stimuli through random concatenation of nonce words.
method_label: In this paper, we use simulations to examine learning from natural, rather than artificial, language input.
result_label: We ask what kinds of words are identified by a learner who assumes that words are statistically independent, or

===================================
paper_id: 8327599; YEAR: 2011
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: title_tfidfcbow200
TITLE: Eugene – A Domain Specific Language for Specifying and                     Constraining Synthetic Biological Parts, Devices, and Systems
ABSTRACT: background_label: BACKGROUND Synthetic biological systems are currently created by an ad-hoc, iterative process of specification, design, and assembly.
background_label: These systems would greatly benefit from a more formalized and rigorous specification of the desired system components as well as constraints on their composition.
background_label: Therefore, the creation of robust and efficient design flows and tools is imperative.
method_label: We present a human readable language (Eugene) that allows for the specification of synthetic biological designs based on biological parts, as well as provides a very expressive constraint system to drive the automatic creation of composite Parts (Devices) from a collection of individual Parts.
result_label: RESULTS We illustrate Eugene's capabilities in three different areas: Device specification, design space exploration, and assembly and simulation integration.
result_label: These results highlight Eugene's ability to create combinatorial design spaces and prune these spaces for simulation or physical assembly.
result_label: Eugene creates functional designs quickly and cost-effectively.
objective_label: CONCLUSIONS Eugene is intended for forward engineering of DNA-based devices, and through its data types and execution semantics, reflects the desired abstraction hierarchy in synthetic biology.
method_label: Eugene provides a powerful constraint system which can be used to drive the creation of new devices at runtime.
result_label: It accomplishes all of this while being part of a larger tool chain which includes support for design, simulation, and physical device assembly.

===================================
paper_id: 41924426; YEAR: 1967
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Simulation of biological evolution and machine learning. I. Selection of self-reproducing numeric patterns by data processing machines, effects of hereditary control, mutation type and crossing.
ABSTRACT: background_label: Abstract The effect of crossing and of different types of mutations (or genetic control) on the speed of selective adaptation has been investigated by data processing machines.
method_label: The procedure is based on the use of self-reproducing numeric patterns, or arrays of numbers and/or single bits of information.
method_label: Each number or bit was used to identify a property of the pattern, which could be subject to selection.
method_label: Some of the properties represented crossing, mutation or reproductive characteristics of the pattern.
method_label: Some other properties represented other characteristics essential for the pattern's ability to survive and reproduce, for example, by identifying the pattern's strategy in a game of poker used as a means to select the patterns to be reproduced.
result_label: A measure of the speed of improvement as a function of generation number has been defined and used to compare the improvement under various conditions.
background_label: The results obtained can be summarized as follows: under conditions simulating regular Mendelian (non-polygenic nor quantitative) inheritance, crossing greatly enhances the speed of selective adaptation, particularly if interaction between the expressions of different hereditary factors is avoided; under conditions simulating polygenic control of quantitative characters, crossing does not enhance the speed of selective adaptation.
method_label: Furthermore, in a period of rapid selective adaptation the ability to interbreed spreads rapidly in the population as a positively selected characteristic when conditions simulating regular Mendelian inheritance are used.
method_label: On the other hand, the breeding characteristic spreads less rapidly or not at all under conditions simulating quantitative characters under polygenic control.
method_label: In the simple game arrangements used in our adaptive selection experiments, the patterns were very often able to develop an optimum game strategy.
result_label: However, in some instances a “selective instability” characterized by statistical fluctuations preventing the achievement of an optimum game strategy was developed.
result_label: Selective instability, if not properly controlled, is likely to assume larger proportions in experiments of a more complicated nature, and may constitute a serious problem for the practical applications of adaptive selection methods by data processing machines.

===================================
paper_id: 114655108; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200 - title_tfidf
TITLE: Evaluation of Off-The-Shelf CNNs for the Representation of Natural Scenes with Large Seasonal Variations
ABSTRACT: objective_label: This paper focuses on the evaluation of deep convolutional neural networks for the analysis of images of natural scenes subjected to large seasonal variation as well as significant changes of lighting conditions.
objective_label: The context is the development of tools for long-term natural environment monitoring with an autonomous mobile robot.
method_label: We report various experiments conducted on a large dataset consisting of a weekly survey of the shore of a small lake over two years using an autonomous surface vessel.
method_label: This dataset is used first in a place recognition task framed as a classification problem, then in a pose regression task and finally the internal features learned by the network are evaluated for their representation power.
result_label: All our results are based on the Caffe library and default network structures where possible.

===================================
paper_id: 46761158; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: cited - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks
ABSTRACT: background_label: Humans can understand and produce new utterances effortlessly, thanks to their compositional skills.
background_label: Once a person learns the meaning of a new verb"dax,"he or she can immediately understand the meaning of"dax twice"or"sing and dax.
method_label: "In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences.
method_label: We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods.
method_label: We find that RNNs can make successful zero-shot generalizations when the differences between training and test commands are small, so that they can apply"mix-and-match"strategies to solve the task.
result_label: However, when generalization requires systematic compositional skills (as in the"dax"example above), RNNs fail spectacularly.
result_label: We conclude with a proof-of-concept experiment in neural machine translation, suggesting that lack of systematicity might be partially responsible for neural networks' notorious training data thirst.

===================================
paper_id: 18874161; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 1; annotator2: 0; annotator3: 1
sources: title_tfidfcbow200 - specter
TITLE: Syntactic Islands and Learning Biases: Combining Experimental Syntax and Computational Modeling to Investigate the Language Acquisition Problem
ABSTRACT: background_label: The induction problems facing language learners have played a central role in debates about the types of learning biases that exist in the human brain.
background_label: Many linguists have argued that some of the learning biases necessary to solve these language induction problems must be both innate and language-specific (i.e., the Universal Grammar (UG) hypothesis).
background_label: Though there have been several recent high-profile investigations of the necessary learning bias types for different linguistic phenomena, the UG hypothesis is still the dominant assumption for a large segment of linguists due to the lack of studies addressing central phenomena in generative linguistics.
objective_label: To address this, we focus on how to learn constraints on long-distance dependencies, also known as syntactic island constraints.
result_label: We use formal acceptability judgment data to identify the target state of learning for syntactic island constraints and conduct a corpus analysis of child-directed data to affirm that there does appear to be an induction problem wh...

===================================
paper_id: 5531014; YEAR: 2010
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Lemmatization and Lexicalized Statistical Parsing of Morphologically-Rich Languages: the Case of French
ABSTRACT: background_label: AbstractThis paper shows that training a lexicalized parser on a lemmatized morphologically-rich treebank such as the French Treebank slightly improves parsing results.
method_label: We also show that lemmatizing a similar in size subset of the English Penn Treebank has almost no effect on parsing performance with gold lemmas and leads to a small drop of performance when automatically assigned lemmas and POS tags are used.
result_label: This highlights two facts: (i) lemmatization helps to reduce lexicon data-sparseness issues for French, (ii) it also makes the parsing process sensitive to correct assignment of POS tags to unknown words.

===================================
paper_id: 2146123; YEAR: 2006
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: cited - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Learning phonology with substantive bias: An experimental and computational study of velar palatalization
ABSTRACT: background_label: There is an active debate within the field of phonology concerning the cognitive status of substantive phonetic factors such as ease of articulation and perceptual distinctiveness.
objective_label: A new framework is proposed in which substance acts as a bias, or prior, on phonological learning.
method_label: Two experiments tested this framework with a method in which participants are first provided highly impoverished evidence of a new phonological pattern, and then tested on how they extend this pattern to novel contexts and novel sounds.
method_label: Participants were found to generalize velar palatalization (e.g., the change from [k] as in keep to [t?∫S] as in cheap) in a way that accords with linguistic typology, and that is predicted by a cognitive bias in favor of changes that relate perceptually similar sounds.
method_label: Velar palatalization was extended from the mid front vowel context (i.e., before [e] as in cape) to the high front vowel context (i.e., before [i] as in keep), but not vice versa.
method_label: The key explanatory notion of perceptual similarity is quantified with a psychological model of categorization, and the substantively biased framework is formalized as a conditional random field.
result_label: Implications of these results for the debate on substance, theories of phonological generalization, and the formalization of similarity are discussed.

===================================
paper_id: 199661323; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 1; annotator2: 1; annotator3: 0
sources: abs_tfidf - specter
TITLE: Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing
ABSTRACT: objective_label: Linguistic typology aims to capture structural and semantic variation across the world’s languages.
background_label: A large-scale typology could provide excellent guidance for multilingual Natural Language Processing (NLP), particularly for languages that suffer from the lack of human labeled resources.
method_label: We present an extensive literature survey on the use of typological information in the development of NLP techniques.
method_label: Our survey demonstrates that to date, the use of information in existing typological databases has resulted in consistent but modest improvements in system performance.
method_label: We show that this is due to both intrinsic limitations of databases (in terms of coverage and feature granularity) and under-utilization of the typological features included in them.
method_label: We advocate for a new approach that adapts the broad and discrete nature of typological categories to the contextual and continuous nature of machine learning algorithms used in contemporary NLP.
result_label: In particular, we suggest that such an approach could be facilitated by recent developments in data-driven induction of typological knowledge.

===================================
paper_id: 115176295; YEAR: 2005
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: cited - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Order of subject, object, and verb
ABSTRACT: other_label: 1,159,485.
background_label: Fuel cell elements.
other_label: UNITED AIRCRAFT CORP. 29 Jan., 1968 [23 Feb., 1967], No.
other_label: 4455/68.
other_label: Heading H1B.
method_label: The electrodes 6, and electrolyte matrix 2 of a fuel cell are constructed as a single unit in which the matrix is surrounded by a frame 16 of the same thickness, each electrode is of substantially the same area dimensions as the matrix and is held against it by a frame 14 which overlaps the edges of the electrode and the frames are stuck together by impregnation with a dielectric binder.
other_label: The matrix may be asbestos and the electrodes nickel coated with a catalytic mixture of platinum and P.T.F.E.
background_label: or sintered nickel oxide.
background_label: The frames may be of a fibrous material, e.g.
background_label: paper or glass fibre.
background_label: The binder may be an epoxy-resin.
other_label: The frames may be joined at 121-177‹ C., 9A84-11A2 kg./cm.
background_label: 2 pressure.
method_label: In use the elements may be clamped together with double-sided separator units 22 comprising fuel and oxidant reservoirs and having cooling fins 38.
method_label: The reactants may be hydrogen and air and the electrolyte potassium hydroxide.

===================================
paper_id: 3238248; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: specter
TITLE: Combining Shallow and Linguistically Motivated Features in Native Language Identification
ABSTRACT: background_label: AbstractWe explore a range of features and ensembles for the task of Native Language Identification as part of the NLI Shared Task .
method_label: Starting with recurring word-based ngrams (Bykh and Meurers, 2012), we tested different linguistic abstractions such as partof-speech, dependencies, and syntactic trees as features for NLI.
method_label: We also experimented with features encoding morphological properties, the nature of the realizations of particular lemmas, and several measures of complexity developed for proficiency and readability classification (Vajjala and Meurers, 2012) .
method_label: Employing an ensemble classifier incorporating all of our features we achieved an accuracy of 82.2% (rank 5) in the closed task and 83.5% (rank 1) in the open-2 task.
result_label: In the open-1 task, the word-based recurring ngrams outperformed the ensemble, yielding 38.5% (rank 2).
result_label: Overall, across all three tasks, our best accuracy of 83.5% for the standard TOEFL11 test set came in second place.

===================================
paper_id: 1656805; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 1; annotator2: 1; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200
TITLE: A Rich Morphological Tagger for English: Exploring the Cross-Linguistic Tradeoff Between Morphology and Syntax
ABSTRACT: background_label: AbstractA traditional claim in linguistics is that all human languages are equally expressiveable to convey the same wide range of meanings.Morphologically rich languages, such as Czech, rely on overt inflectional and derivational morphology to convey many semantic distinctions.
background_label: Languages with comparatively limited morphology, such as English, should be able to accomplish the same using a combination of syntactic and contextual cues.
method_label: We capitalize on this idea by training a tagger for English that uses syntactic features obtained by automatic parsing to recover complex morphological tags projected from Czech.
result_label: The high accuracy of the resulting model provides quantitative confirmation of the underlying linguistic hypothesis of equal expressivity, and bodes well for future improvements in downstream HLT tasks including machine translation.

===================================
paper_id: 14236681; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: abs_cbow200 - specter
TITLE: Separated by an Un-common Language: Towards Judgment Language Informed Vector Space Modeling
ABSTRACT: background_label: A common evaluation practice in the vector space models (VSMs) literature is to measure the models' ability to predict human judgments about lexical semantic relations between word pairs.
background_label: Most existing evaluation sets, however, consist of scores collected for English word pairs only, ignoring the potential impact of the judgment language in which word pairs are presented on the human scores.
method_label: In this paper we translate two prominent evaluation sets, wordsim353 (association) and SimLex999 (similarity), from English to Italian, German and Russian and collect scores for each dataset from crowdworkers fluent in its language.
result_label: Our analysis reveals that human judgments are strongly impacted by the judgment language.
result_label: Moreover, we show that the predictions of monolingual VSMs do not necessarily best correlate with human judgments made with the language used for model training, suggesting that models and humans are affected differently by the language they use when making semantic judgments.
result_label: Finally, we show that in a large number of setups, multilingual VSM combination results in improved correlations with human judgments, suggesting that multilingualism may partially compensate for the judgment language effect on human judgments.

===================================
paper_id: 1356419; YEAR: 1998
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200 - abs_tfidf
TITLE: The Role of Verbs in Document Analysis
ABSTRACT: background_label: We present results of two methods for assessing the event profile of news articles as a function of verb type.
objective_label: The unique contribution of this research is the focus on the role of verbs, rather than nouns.
method_label: Two algorithms are presented and evaluated, one of which is shown to accurately discriminate documents by type and semantic properties, i.e.
method_label: the event profile.
method_label: The initial method, using WordNet (Miller et al.
method_label: 1990), produced multiple cross-classification of articles, primarily due to the bushy nature of the verb tree coupled with the sense disambiguation problem.
method_label: Our second approach using English Verb Classes and Alternations (EVCA) Levin (1993) showed that monosemous categorization of the frequent verbs in WSJ made it possible to usefully discriminate documents.
result_label: For example, our results show that articles in which communication verbs predominate tend to be opinion pieces, whereas articles with a high percentage of agreement verbs tend to be about mergers or legal cases.
result_label: An evaluation is performed on the results using Kendall's Tau.
result_label: We present convincing evidence for using verb semantic classes as a discriminant in document classification.

===================================
paper_id: 9530012; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 1; annotator2: 1; annotator3: 0
sources: cited - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Fine-Grained Prediction of Syntactic Typology: Discovering Latent Structure with Supervised Learning
ABSTRACT: background_label: We show how to predict the basic word-order facts of a novel language given only a corpus of part-of-speech (POS) sequences.
background_label: We predict how often direct objects follow their verbs, how often adjectives follow their nouns, and in general the directionalities of all dependency relations.
background_label: Such typological properties could be helpful in grammar induction.
objective_label: While such a problem is usually regarded as unsupervised learning, our innovation is to treat it as supervised learning, using a large collection of realistic synthetic languages as training data.
method_label: The supervised learner must identify surface features of a language's POS sequence (hand-engineered or neural features) that correlate with the language's deeper structure (latent trees).
method_label: In the experiment, we show: 1) Given a small set of real languages, it helps to add many synthetic languages to the training data.
result_label: 2) Our system is robust even when the POS sequences include noise.
result_label: 3) Our system on this task outperforms a grammar induction baseline by a large margin.

===================================
paper_id: 8364931; YEAR: 2010
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: NeuroML: A Language for Describing Data Driven Models of Neurons and Networks with a High Degree of Biological Detail
ABSTRACT: background_label: Biologically detailed single neuron and network models are important for understanding how ion channels, synapses and anatomical connectivity underlie the complex electrical behavior of the brain.
background_label: While neuronal simulators such as NEURON, GENESIS, MOOSE, NEST, and PSICS facilitate the development of these data-driven neuronal models, the specialized languages they employ are generally not interoperable, limiting model accessibility and preventing reuse of model components and cross-simulator validation.
method_label: To overcome these problems we have used an Open Source software approach to develop NeuroML, a neuronal model description language based on XML (Extensible Markup Language).
method_label: This enables these detailed models and their components to be defined in a standalone form, allowing them to be used across multiple simulators and archived in a standardized format.
method_label: Here we describe the structure of NeuroML and demonstrate its scope by converting into NeuroML models of a number of different voltage- and ligand-gated conductances, models of electrical coupling, synaptic transmission and short-term plasticity, together with morphologically detailed models of individual neurons.
method_label: We have also used these NeuroML-based components to develop an highly detailed cortical network model.
method_label: NeuroML-based model descriptions were validated by demonstrating similar model behavior across five independently developed simulators.
result_label: Although our results confirm that simulations run on different simulators converge, they reveal limits to model interoperability, by showing that for some models convergence only occurs at high levels of spatial and temporal discretisation, when the computational overhead is high.
result_label: Our development of NeuroML as a common description language for biophysically detailed neuronal and network models enables interoperability across multiple simulation environments, thereby improving model transparency, accessibility and reuse in computational neuroscience.

===================================
paper_id: 119989792; YEAR: 2010
adju relevance: Irrelevant (0)
difference: 1; annotator2: 1; annotator3: 0
sources: abs_tfidf
TITLE: Language clusters based on linguistic complex networks
ABSTRACT: objective_label: To investigate the feasibility of using complex networks in the study of linguistic typology, this paper builds and explores 15 linguistic complex networks based on the dependency syntactic treebanks of 15 languages.
method_label: The results show that it is possible to classify human languages by means of the following main parameters of complex networks: (a) average degree of the node, (b) cluster coefficients, (c) average path length, (d) network centralization, (e) diameter, (f) power exponent of degree distribution, and (g) the determination coefficient of power law distributions.
method_label: The precision of this method is similar to the results achieved by means of modern word order typology.
objective_label: This paper tries to solve two problems of current linguistic typology.
method_label: First, the language sample of a typological study is not real text; second, typological studies pay too much attention to local language structures in the course of choosing typological parameters.
result_label: This study performs better in global typological features of language and not only enhances typological methods, but it is also valuable for developing the applications of complex networks in the humanities, social, and life sciences.

===================================
paper_id: 119425731; YEAR: 1972
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: title_tfidf
TITLE: Unzerlegbare Darstellungen I
ABSTRACT: background_label: LetK be the structure got by forgetting the composition law of morphisms in a given category.
background_label: A linear representation ofK is given by a map V associating with any morphism ϕ: a→e ofK a linear vector space map V(ϕ): V(a)→V(e).
method_label: We classify thoseK having only finitely many isomorphy classes of indecomposable linear representations.
other_label: This classification is related to an old paper by Yoshii [3].

===================================
paper_id: 16874763; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Explaining the hierarchy of visual representational geometries by remixing of features from many computational vision models
ABSTRACT: background_label: AbstractVisual processing in cortex happens through a hierarchy of increasingly sophisticated representations.
background_label: Here we explore a very wide range of model representations (29 models), testing their categorization performance (animate/inanimate) and their ability to account for the representational geometry of brain regions along the visual hierarchy (V1, V2, V3, V4, and LO).
method_label: We also created new model instantiations (85 model instantiations in total) by reweighting and remixing of the model features.
method_label: Reweighting and remixing was based on brain responses to an independent training set of 1750 images.
method_label: We assessed the models with representational similarity analysis (RSA), which characterizes the geometry of a representation by a representational dissimilarity matrix (RDM).
method_label: In this study, the RDM is either computed on the basis of the model features or on the basis of predicted voxel responses.
result_label: Voxel responses are predicted by linear combinations of the model features.
background_label: The model features are linearly remixed so as to best explain the voxel responses (as in voxel/population receptive-field modelling).
background_label: This new approach of combining RSA with voxel receptive field modelling may help bridge the gap between the two methods.
method_label: We found that early visual areas are best accounted for by a Gabor wavelet pyramid (GWP) model.
method_label: The GWP implementations we used performed similarly with and without remixing, suggesting that the original features already approximate the representational space, obviating the need for remixing or reweighting.
method_label: The lateral occipital region (LO), a higher visual representation, was best explained by the higher layers of a deep convolutional network (Krizhevsky et al., 2012) .
method_label: However, this model could explain the LO representation only after appropriate remixing of its feature set.
method_label: Remixed RSA takes a step in an important direction, where each computational model representation is explored more broadly by considering not only its representational geometry, but the set of all geometries within reach of a linear transform.
result_label: The exploration of many models and many brain areas may lead to a better understanding of the processing stages in the visual hierarchy, from low-level image representations in V1 to visuo-semantic representations in higher-level visual areas.

===================================
paper_id: 198147626; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: specter - abs_tfidfcbow200
TITLE: Learning cross-lingual phonological and orthagraphic adaptations: a case study in improving neural machine translation between low-resource languages
ABSTRACT: background_label: Out-of-vocabulary (OOV) words can pose serious challenges for machine translation (MT) tasks, and in particular, for low-resource language (LRL) pairs, i.e., language pairs for which few or no parallel corpora exist.
objective_label: Our work adapts variants of seq2seq models to perform transduction of such words from Hindi to Bhojpuri (an LRL instance), learning from a set of cognate pairs built from a bilingual dictionary of Hindi--Bhojpuri words.
method_label: We demonstrate that our models can be effectively used for language pairs that have limited parallel corpora; our models work at the character level to grasp phonetic and orthographic similarities across multiple types of word adaptations, whether synchronic or diachronic, loan words or cognates.
method_label: We describe the training aspects of several character level NMT systems that we adapted to this task and characterize their typical errors.
method_label: Our method improves BLEU score by 6.3 on the Hindi-to-Bhojpuri translation task.
method_label: Further, we show that such transductions can generalize well to other languages by applying it successfully to Hindi -- Bangla cognate pairs.
result_label: Our work can be seen as an important step in the process of: (i) resolving the OOV words problem arising in MT tasks, (ii) creating effective parallel corpora for resource-constrained languages, and (iii) leveraging the enhanced semantic knowledge captured by word-level embeddings to perform character-level tasks.

===================================
paper_id: 2244960; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: cited - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Tree-structured composition in neural networks without tree-structured architectures
ABSTRACT: background_label: Tree-structured neural networks encode a particular tree geometry for a sentence in the network design.
background_label: However, these models have at best only slightly outperformed simpler sequence-based models.
objective_label: We hypothesize that neural sequence models like LSTMs are in fact able to discover and implicitly use recursive compositional structure, at least for tasks with clear cues to that structure in the data.
method_label: We demonstrate this possibility using an artificial data task for which recursive compositional structure is crucial, and find an LSTM-based sequence model can indeed learn to exploit the underlying tree structure.
result_label: However, its performance consistently lags behind that of tree models, even on large training sets, suggesting that tree-structured models are more effective at exploiting recursive structure.

===================================
paper_id: 56015860; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: Cross-linguistic analysis of discourse variation across registers
ABSTRACT: background_label: The present study deals with variation in discourse relations in different registers of English and German.
background_label: Our previous analyses have been concerned with the systemic contrasts between English and German, cf.
background_label: Kunz & Steiner (2013 a/b), Kunz & Lapshinova (to appear) and have addressed some cross-linguistic differences with regard to textual realizations of selected subtypes of cohesion.
objective_label: In our current work, our focus is on the empirical analysis of cross-linguistic variation between registers.
method_label: In order to obtain a more comprehensive picture, we investigate three main types of cohesion in combination: co-reference, substitution and conjunction and their subtypes, cf.
method_label: Halliday & Hasan (1976).
method_label: We extract instantiations of cohesive devices from an English-German corpus of spoken and written registers.
result_label: The data is analyzed with statistical procedures which show that subcorpora can be grouped along particular combinations of cohesive devices.

===================================
paper_id: 9558272; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 1; annotator2: 0; annotator3: 1
sources: abs_tfidf - abs_cbow200 - abs_tfidfcbow200
TITLE: Contrasting intrusion profiles for agreement and anaphora: Experimental and modeling evidence
ABSTRACT: background_label: We investigated the relationship between linguistic representation and memory access by comparing the processing of two linguistic dependencies that require comprehenders to check that the subject of the current clause has the correct morphological features: subject–verb agreement and reflexive anaphors in English.
method_label: In two eye-tracking experiments we examined the impact of structurally illicit noun phrases on the computation of reflexive and subject–verb agreement.
method_label: Experiment 1 directly compared the two dependencies within participants.
result_label: Results show a clear difference in the intrusion profile associated with each dependency: agreement resolution displays clear intrusion effects in comprehension (as found by Pearlmutter, Garnsey, & Bock, 1999; Wagers, Lau, & Phillips, 2009), but reflexives show no such intrusion effect from illicit antecedents (Sturt, 2003; Xiang, Dillon, & Phillips, 2009).
result_label: Experiment 2 replicated the lack of intrusion for reflexives, confirming the reliability of the pattern and examining a wider range of feature combinations.
result_label: In addition, we present modeling evidence that suggests that the reflexive results are best captured by a memory retrieval mechanism that uses primarily syntactic information to guide retrievals for the anaphor’s antecedent, in contrast to the mixed morphological and syntactic cues used resolve subject–verb agreement dependencies.
result_label: Despite the fact that agreement and reflexive dependencies are subject to a similar morphological agreement constraint, in online processing comprehenders appear to implement this constraint in distinct ways for the two dependencies.

===================================
paper_id: 82456167; YEAR: 2007
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: title_tfidf
TITLE: Janeway's Immunobiology
ABSTRACT: background_label: Part I An Introduction to Immunobiology and Innate Immunity 1.
background_label: Basic Concepts in Immunology 2.
background_label: Innate Immunity Part II The Recognition of Antigen 3.
background_label: Antigen Recognition by B-cell and T-cell Receptors 4.
method_label: The Generation of Lymphocyte Antigen Receptors 5.
method_label: Antigen Presentation to T Lymphocytes Part III The Development of Mature Lymphocyte Receptor Repertoires 6.
method_label: Signaling Through Immune System Receptors 7.
result_label: The Development and Survival of Lymphocytes Part IV The Adaptive Immune Response 8.
background_label: T Cell-Mediated Immunity 9.
background_label: The Humoral Immune Response 10.
background_label: Dynamics of Adaptive Immunity 11.
other_label: The Mucosal Immune System Part V The Immune System in Health and Disease 12.
background_label: Failures of Host Defense Mechanism 13.
other_label: Allergy and Hypersensitivity 14.
other_label: Autoimmunity and Transplantation 15.
other_label: Manipulation of the Immune Response Part VI The Origins of Immune Responses 16.
other_label: Evolution of the Immune System Appendix I Immunologists' Toolbox Appendix II CD Antigens Appendix III Cytokines and their Receptors Appendix IV Chemokines and their Receptors Appendix V Immunological Constants

===================================
paper_id: 17983355; YEAR: 2007
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200
TITLE: Gradient Grammar: An Effect of Animacy on the Syntax of give in New Zealand and American English
ABSTRACT: background_label: Abstract Bresnan et al.
background_label: (2007) show that a statistical model can predict United States (US) English speakers’ syntactic choices with ‘give’-type verbs extremely accurately.
background_label: They argue that these results are consistent with probabilistic models of grammar, which assume that grammar is quantitive, and learned from exposure to other speakers.
background_label: Such a model would also predict syntactic differences across time and space which are reflected not only in the use of clear dialectal features or clear-cut changes in progress, but also in subtle factors such as the relative importance of conditioning factors, and changes over time in speakers’ preferences between equally well-formed variants.
objective_label: This paper investigates these predictions by comparing the grammar of phrases involving ‘give’ in New Zealand (NZ) and US English.
method_label: We find that the grammar developed by Bresnan et al.
result_label: for US English generalizes remarkably well to NZ English.
result_label: NZ English is, however, subtly different, in that NZ English speakers appear to be more sensitive to the role of animacy.
result_label: Further, we investigate changes over time in NZ English and find that the overall behavior of ‘give’ phrases has subtly shifted.
result_label: We argue that these subtle differences in space and time provide support for the gradient nature of grammar, and are consistent with usage-based, probabilistic syntactic models.

===================================
paper_id: 122766670; YEAR: 2002
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: abs_cbow200 - specter
TITLE: The distributional structure of grammatical categories in speech to young children
ABSTRACT: background_label: We present a series of three analyses of young children’s linguistic input to determine the distributional information it could plausibly offer to the process of grammatical category learning.
background_label: Each analysis was conducted on four separate corpora from the CHILDES database (MacWhinney, 2000) of speech directed to children under 2;5.
method_label: We show that, in accord with other findings, a distributional analysis which categorizes words based on their co-occurrence patterns with surrounding words successfully categorizes the majority of nouns and verbs.
method_label: In Analyses 2 and 3, we attempt to make our analyses more closely relevant to natural language acquisition by adopting more realistic assumptions about how young children represent their input.
method_label: In Analysis 2, we limit the distributional context by imposing phrase structure boundaries, and find that categorization improves even beyond that obtained from less limited contexts.
result_label: In Analysis 3, we reduce the representation of input elements which young children might not fully process and we find that categorization is not adversely affected: Although noun categorization is worse than in Analyses 1 and 2, it is still good; and verb categorization actually improves.
result_label: Overall, successful categorization of nouns and verbs is maintained across all analyses.
result_label: These results provide promising support for theories of grammatical category formation involving distributional analysis, as long as these analyses are combined with appropriate assumptions about the child learner’s computational biases and capabilities.

===================================
paper_id: 13926782; YEAR: 2005
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: abs_cbow200
TITLE: Morphological features help POS tagging of unknown words across language varieties
ABSTRACT: background_label: AbstractPart-of-speech tagging, like any supervised statistical NLP task, is more difficult when test sets are very different from training sets, for example when tagging across genres or language varieties.
background_label: We examined the problem of POS tagging of different varieties of Mandarin Chinese (PRC-Mainland, PRCHong Kong, and Taiwan).
background_label: An analytic study first showed that unknown words were a major source of difficulty in cross-variety tagging.
method_label: Unknown words in English tend to be proper nouns.
method_label: By contrast, we found that Mandarin unknown words were mostly common nouns and verbs.
result_label: We showed these results are caused by the high frequency of morphological compounding in Mandarin; in this sense Mandarin is more like German than English.
method_label: Based on this analysis, we propose a variety of new morphological unknown-word features for POS tagging, extending earlier work by others on unknown-word tagging in English and German.
method_label: Our features were implemented in a maximum entropy Markov model.
result_label: Our system achieves state-of-the-art performance in Mandarin tagging, including improving unknown-word tagging performance on unseen varieties in Chinese Treebank 5.0 from 61% to 80% correct.

===================================
paper_id: 3638905; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 2; annotator2: 0; annotator3: 2
sources: title_tfidf
TITLE: Learning Inductive Biases with Simple Neural Networks
ABSTRACT: background_label: People use rich prior knowledge about the world in order to efficiently learn new concepts.
background_label: These priors - also known as"inductive biases"- pertain to the space of internal models considered by a learner, and they help the learner make inferences that go beyond the observed data.
background_label: A recent study found that deep neural networks optimized for object recognition develop the shape bias (Ritter et al., 2017), an inductive bias possessed by children that plays an important role in early word learning.
background_label: However, these networks use unrealistically large quantities of training data, and the conditions required for these biases to develop are not well understood.
background_label: Moreover, it is unclear how the learning dynamics of these networks relate to developmental processes in childhood.
method_label: We investigate the development and influence of the shape bias in neural networks using controlled datasets of abstract patterns and synthetic images, allowing us to systematically vary the quantity and form of the experience provided to the learning algorithms.
result_label: We find that simple neural networks develop a shape bias after seeing as few as 3 examples of 4 object categories.
result_label: The development of these biases predicts the onset of vocabulary acceleration in our networks, consistent with the developmental process in children.

===================================
paper_id: 86492460; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: title_tfidf
TITLE: On Inductive Biases in Deep Reinforcement Learning
ABSTRACT: background_label: Many deep reinforcement learning algorithms contain inductive biases that sculpt the agent's objective and its interface to the environment.
background_label: These inductive biases can take many forms, including domain knowledge and pretuned hyper-parameters.
background_label: In general, there is a trade-off between generality and performance when algorithms use such biases.
background_label: Stronger biases can lead to faster learning, but weaker biases can potentially lead to more general algorithms.
background_label: This trade-off is important because inductive biases are not free; substantial effort may be required to obtain relevant domain knowledge or to tune hyper-parameters effectively.
objective_label: In this paper, we re-examine several domain-specific components that bias the objective and the environmental interface of common deep reinforcement learning agents.
method_label: We investigated whether the performance deteriorates when these components are replaced with adaptive solutions from the literature.
result_label: In our experiments, performance sometimes decreased with the adaptive components, as one might expect when comparing to components crafted for the domain, but sometimes the adaptive components performed better.
result_label: We investigated the main benefit of having fewer domain-specific components, by comparing the learning performance of the two systems on a different set of continuous control problems, without additional tuning of either system.
result_label: As hypothesized, the system with adaptive components performed better on many of the new tasks.

===================================
paper_id: 52213711; YEAR: 2012
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200 - abs_tfidf
TITLE: Thai EFL Students' Writing Errors in Different Text Types: The Interference of the First Language
ABSTRACT: objective_label: This study aimed at analyzing writing errors caused by the interference of the Thai language, regarded as the first language (L1), in three writing genres, namely narration, description, and comparison/contrast.
method_label: 120 English paragraphs written by 40 second year English major students were analyzed by using Error Analysis (EA).The results revealed that the first language interference errors fell into 16 categories: verb tense, word choice, sentence structure, article, preposition, modal/auxiliary, singular/plural form, fragment, verb form, pronoun, run-on sentence, infinitive/gerund, transition, subject-verb agreement, parallel structure, and comparison structure, respectively, and the number of frequent errors made in each type of written tasks was apparently different.
result_label: In narration, the five most frequent errors found were verb tense, word choice, sentence structure, preposition, and modal/auxiliary, respectively, while the five most frequent errors in description and comparison/contrast were article, sentence structure, word choice, singular/plural form, and subject-verb agreement, respectively.
result_label: Interestingly, in the narrative and descriptive paragraphs, comparison structure was found to be the least frequent error, whereas it became the 10th frequent error in comparison/contrast writing.
result_label: It was apparent that a genre did affect writing errors as different text types required different structural features.
result_label: It could be concluded that to enhance students’ grammatical and lexical accuracy, a second language (L2) writing teacher should take into consideration L1 interference categories in different genres.

===================================
paper_id: 52909053; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: title_tfidf
TITLE: An Exploration of Dropout with RNNs for Natural Language Inference
ABSTRACT: background_label: Dropout is a crucial regularization technique for the Recurrent Neural Network (RNN) models of Natural Language Inference (NLI).
background_label: However, dropout has not been evaluated for the effectiveness at different layers and dropout rates in NLI models.
objective_label: In this paper, we propose a novel RNN model for NLI and empirically evaluate the effect of applying dropout at different layers in the model.
method_label: We also investigate the impact of varying dropout rates at these layers.
result_label: Our empirical evaluation on a large (Stanford Natural Language Inference (SNLI)) and a small (SciTail) dataset suggest that dropout at each feed-forward connection severely affects the model accuracy at increasing dropout rates.
result_label: We also show that regularizing the embedding layer is efficient for SNLI whereas regularizing the recurrent layer improves the accuracy for SciTail.
result_label: Our model achieved an accuracy 86.14% on the SNLI dataset and 77.05% on SciTail.

===================================
paper_id: 15625279; YEAR: 2009
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: abs_tfidf
TITLE: Agreement attraction in comprehension: Representations and processes
ABSTRACT: background_label: Much work has demonstrated so-called attraction errors in the production of subject–verb agreement (e.g., ‘The key to the cabinets are on the table’, [Bock, J. K., & Miller, C. A.
background_label: (1991).
background_label: Broken agreement.
background_label: Cognitive Psychology, 23, 45–93]), in which a verb erroneously agrees with an intervening noun.
result_label: Six self-paced reading experiments examined the online mechanisms underlying the analogous attraction effects that have been shown in comprehension; namely reduced disruption for subject–verb agreement violations when these ‘attractor’ nouns intervene.
background_label: One class of theories suggests that these effects are rooted in faulty representation of the number of the subject, while another class of theories suggests instead that such effects arise in the process of re-accessing subject number at the verb.
background_label: Two main findings provide evidence against the first class of theories.
method_label: First, attraction also occurs in relative clause configurations in which the attractor noun does not intervene between subject and verb and is not in a direct structural relationship with the subject head (e.g., ‘The drivers who the runner wave to each morning’).
method_label: Second, we observe a ‘grammatical asymmetry’: attraction effects are limited to ungrammatical sentences, which would be unexpected if the representation of subject number were inherently prone to error.
result_label: We argue that agreement attraction in comprehension instead reflects a cue-based retrieval mechanism that is subject to retrieval errors.
result_label: The grammatical asymmetry can be accounted for under one implementation that we propose, or if the mechanism is only called upon when the predicted agreement features fail to be instantiated on the verb.

===================================
paper_id: 202539170; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: specter
TITLE: Don't Forget the Long Tail! A Comprehensive Analysis of Morphological Generalization in Bilingual Lexicon Induction
ABSTRACT: background_label: Human translators routinely have to translate rare inflections of words - due to the Zipfian distribution of words in a language.
background_label: When translating from Spanish, a good translator would have no problem identifying the proper translation of a statistically rare inflection such as habl\'aramos.
background_label: Note the lexeme itself, hablar, is relatively common.
objective_label: In this work, we investigate whether state-of-the-art bilingual lexicon inducers are capable of learning this kind of generalization.
method_label: We introduce 40 morphologically complete dictionaries in 10 languages and evaluate three of the state-of-the-art models on the task of translation of less frequent morphological forms.
result_label: We demonstrate that the performance of state-of-the-art models drops considerably when evaluated on infrequent morphological inflections and then show that adding a simple morphological constraint at training time improves the performance, proving that the bilingual lexicon inducers can benefit from better encoding of morphology.

===================================
paper_id: 189927918; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: title_tfidf
TITLE: On the Computational Power of RNNs
ABSTRACT: background_label: Recent neural network architectures such as the basic recurrent neural network (RNN) and Gated Recurrent Unit (GRU) have gained prominence as end-to-end learning architectures for natural language processing tasks.
background_label: But what is the computational power of such systems?
method_label: We prove that finite precision RNNs with one hidden layer and ReLU activation and finite precision GRUs are exactly as computationally powerful as deterministic finite automata.
method_label: Allowing arbitrary precision, we prove that RNNs with one hidden layer and ReLU activation are at least as computationally powerful as pushdown automata.
method_label: If we also allow infinite precision, infinite edge weights, and nonlinear output activation functions, we prove that GRUs are at least as computationally powerful as pushdown automata.
result_label: All results are shown constructively.

===================================
paper_id: 44119185; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: specter
TITLE: Using Morphological Knowledge in Open-Vocabulary Neural Language Models
ABSTRACT: background_label: AbstractLanguages with productive morphology pose problems for language models that generate words from a fixed vocabulary.
background_label: Although character-based models allow any possible word type to be generated, they are linguistically naïve: they must discover that words exist and are delimited by spaces-basic linguistic facts that are built in to the structure of word-based models.
method_label: We introduce an openvocabulary language model that incorporates more sophisticated linguistic knowledge by predicting words using a mixture of three generative processes: (1) by generating words as a sequence of characters, (2) by directly generating full word forms, and (3) by generating words as a sequence of morphemes that are combined using a hand-written morphological analyzer.
result_label: Experiments on Finnish, Turkish, and Russian show that our model outperforms character sequence models and other strong baselines on intrinsic and extrinsic measures.
result_label: Furthermore, we show that our model learns to exploit morphological knowledge encoded in the analyzer, and, as a byproduct, it can perform effective unsupervised morphological disambiguation.

===================================
paper_id: 2255575; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: A data-based classification of Slavic languages: Indices of qualitative variation applied to grapheme frequencies
ABSTRACT: background_label: The Ord's graph is a simple graphical method for displaying frequency distributions of data or theoretical distributions in the two-dimensional plane.
background_label: Its coordinates are proportions of the first three moments, either empirical or theoretical ones.
method_label: A modification of the Ord's graph based on proportions of indices of qualitative variation is presented.
method_label: Such a modification makes the graph applicable also to data of categorical character.
method_label: In addition, the indices are normalized with values between 0 and 1, which enables comparing data files divided into different numbers of categories.
method_label: Both the original and the new graph are used to display grapheme frequencies in eleven Slavic languages.
method_label: As the original Ord's graph requires an assignment of numbers to the categories, graphemes were ordered decreasingly according to their frequencies.
method_label: Data were taken from parallel corpora, i.e., we work with grapheme frequencies from a Russian novel and its translations to ten other Slavic languages.
method_label: Then, cluster analysis is applied to the graph coordinates.
result_label: While the original graph yields results which are not linguistically interpretable, the modification reveals meaningful relations among the languages.

===================================
paper_id: 7677690; YEAR: 2011
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Malagasy Dialects and the Peopling of Madagascar
ABSTRACT: background_label: The origin of Malagasy DNA is half African and half Indonesian, nevertheless the Malagasy language, spoken by the entire population, belongs to the Austronesian family.
background_label: The language most closely related to Malagasy is Maanyan (Greater Barito East group of the Austronesian family), but related languages are also in Sulawesi, Malaysia and Sumatra.
background_label: For this reason, and because Maanyan is spoken by a population which lives along the Barito river in Kalimantan and which does not possess the necessary skill for long maritime navigation, the ethnic composition of the Indonesian colonizers is still unclear.
background_label: There is a general consensus that Indonesian sailors reached Madagascar by a maritime trek, but the time, the path and the landing area of the first colonization are all disputed.
other_label: In this research we try to answer these problems together with other ones, such as the historical configuration of Malagasy dialects, by types of analysis related to lexicostatistics and glottochronology which draw upon the automated method recently proposed by the authors \cite{Serva:2008, Holman:2008, Petroni:2008, Bakker:2009}.
result_label: The data were collected by the first author at the beginning of 2010 with the invaluable help of Joselin\`a Soafara N\'er\'e and consist of Swadesh lists of 200 items for 23 dialects covering all areas of the Island.

===================================
paper_id: 145704027; YEAR: 2001
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: title_cbow200
TITLE: The phonetics and phonology of Tashlhiyt Berber syllabic consonants
ABSTRACT: background_label: Tashlhiyt Berber has entered the folklore of phonological theory as an unusual language in which any consonant can be syllabic, many words consisting entirely of consonants.
background_label: I shall argue for an alternative analysis, according to which the epenthetic vowels which frequently accompany syllabic consonants are the phonetic realisations of syllable nuclei.
background_label: Where no vowel occurs, it can be regarded as hidden by the following consonant, according to a gestural overlap model.
result_label: On this view, Tashlhiyt syllable structure is a quite unmarked CV(C(C)), and syllabification is unproblematic.

===================================
paper_id: 184486914; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200 - abs_cbow200
TITLE: Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology
ABSTRACT: background_label: Gender stereotypes are manifest in most of the world's languages and are consequently propagated or amplified by NLP systems.
background_label: Although research has focused on mitigating gender stereotypes in English, the approaches that are commonly employed produce ungrammatical sentences in morphologically rich languages.
objective_label: We present a novel approach for converting between masculine-inflected and feminine-inflected sentences in such languages.
method_label: For Spanish and Hebrew, our approach achieves F1 scores of 82% and 73% at the level of tags and accuracies of 90% and 87% at the level of forms.
result_label: By evaluating our approach using four different languages, we show that, on average, it reduces gender stereotyping by a factor of 2.5 without any sacrifice to grammaticality.

===================================
paper_id: 202558708; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: specter - abs_tfidf
TITLE: From English to Code-Switching: Transfer Learning with Strong Morphological Clues
ABSTRACT: background_label: Code-switching is still an understudied phenomenon in natural language processing mainly because of two related challenges: it lacks annotated data, and it combines a vast diversity of low-resource languages.
background_label: Despite the language diversity, many code-switching scenarios occur in language pairs, and English is often a common factor among them.
method_label: In the first part of this paper, we use transfer learning from English to English-paired code-switched languages for the language identification (LID) task by applying two simple yet effective techniques: 1) a hierarchical attention mechanism that enhances morphological clues from character n-grams, and 2) a secondary loss that forces the model to learn n-gram representations that are particular to the languages involved.
method_label: We use the bottom layers of the ELMo architecture to learn these morphological clues by essentially recognizing what is and what is not English.
method_label: Our approach outperforms the previous state of the art on Nepali-English, Spanish-English, and Hindi-English datasets.
method_label: In the second part of the paper, we use our best LID models for the tasks of Spanish-English named entity recognition and Hindi-English part-of-speech tagging by replacing their inference layers and retraining them.
result_label: We show that our retrained models are capable of using the code-switching information on both tasks to outperform models that do not have such knowledge.

===================================
paper_id: 1070525; YEAR: 2002
adju relevance: Irrelevant (0)
difference: 1; annotator2: 1; annotator3: 0
sources: abs_tfidf
TITLE: A Multilingual Paradigm For Automatic Verb Classification
ABSTRACT: background_label: We demonstrate the benefits of a multilingual approach to automatic lexical semantic verb classification based on statistical analysis of corpora in multiple languages.
background_label: Our research incorporates two interrelated threads.
method_label: In one, we exploit the similarities in the crosslinguistic classification of verbs, to extend work on English verb classification to a new language (Italian), and to new classes within that language, achieving an accuracy of 86.4% (baseline 33.9%).
method_label: Our second strand of research exploits the differences across languages in the syntactic expression of semantic properties, to show that complementary information about English verbs can be extracted from their translations in a second language (Chinese).
result_label: The use of multilingual features improves classification performance of the English verbs, achieving an accuracy of 83.5% (baseline 33.3%).

===================================
paper_id: 192058044; YEAR: 1992
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: The alchemy of English : the spread, functions, and models of non-native Englishes
ABSTRACT: background_label: "What emerges from Kachru's fine work is the potential demarcation of an entire field, rather than merely the fruitful exploration of a topic.
background_label: .
other_label: .
background_label: .
other_label: [Kachru] is to be congratulated for having taken us as far as he already has and for doing so in so stimulating and so productive a fashion."
other_label: -World Englishes "A potent addition to theoretical, sociolinguistic, attitudinal and methodological explorations vis-a-vis the spread and functions of, and innovations in, English from the viewpoint of a non-Western scholar."
other_label: -The Language Teacher Winner of the Joint First Prize, Duke of Edinburgh English Language Book Competition of the EnglishSpeaking Union of the Commonwealth, 1987

===================================
paper_id: 8247293; YEAR: 2012
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200
TITLE: Towards a model of formal and informal address in English
ABSTRACT: background_label: AbstractInformal and formal ("T/V") address in dialogue is not distinguished overtly in modern English, e.g.
background_label: by pronoun choice like in many other languages such as French ("tu"/"vous").
objective_label: Our study investigates the status of the T/V distinction in English literary texts.
method_label: Our main findings are: (a) human raters can label monolingual English utterances as T or V fairly well, given sufficient context; (b), a bilingual corpus can be exploited to induce a supervised classifier for T/V without human annotation.
result_label: It assigns T/V at sentence level with up to 68% accuracy, relying mainly on lexical features; (c), there is a marked asymmetry between lexical features for formal speech (which are conventionalized and therefore general) and informal speech (which are text-specific).

===================================
paper_id: 3883482; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200 - specter
TITLE: Generating Bilingual Pragmatic Color References
ABSTRACT: background_label: Contextual influences on language often exhibit substantial cross-lingual regularities; for example, we are more verbose in situations that require finer distinctions.
background_label: However, these regularities are sometimes obscured by semantic and syntactic differences.
method_label: Using a newly-collected dataset of color reference games in Mandarin Chinese (which we release to the public), we confirm that a variety of constructions display the same sensitivity to contextual difficulty in Chinese and English.
method_label: We then show that a neural speaker agent trained on bilingual data with a simple multitask learning approach displays more human-like patterns of context dependence and is more pragmatically informative than its monolingual Chinese counterpart.
result_label: Moreover, this is not at the expense of language-specific semantic understanding: the resulting speaker model learns the different basic color term systems of English and Chinese (with noteworthy cross-lingual influences), and it can identify synonyms between the two languages using vector analogy operations on its output layer, despite having no exposure to parallel data.

===================================
paper_id: 1642440; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 1; annotator2: 0; annotator3: 1
sources: abs_tfidf - abs_cbow200
TITLE: Reconstructing Native Language Typology from Foreign Language Usage
ABSTRACT: background_label: Linguists and psychologists have long been studying cross-linguistic transfer, the influence of native language properties on linguistic performance in a foreign language.
objective_label: In this work we provide empirical evidence for this process in the form of a strong correlation between language similarities derived from structural features in English as Second Language (ESL) texts and equivalent similarities obtained from the typological features of the native languages.
method_label: We leverage this finding to recover native language typological similarity structure directly from ESL text, and perform prediction of typological features in an unsupervised fashion with respect to the target languages.
result_label: Our method achieves 72.2% accuracy on the typology prediction task, a result that is highly competitive with equivalent methods that rely on typological resources.

===================================
paper_id: 13848633; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: cited - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Are All Languages Equally Hard to Language-Model?
ABSTRACT: background_label: For general modeling methods applied to diverse languages, a natural question is: how well should we expect our models to work on languages with differing typological profiles?
method_label: In this work, we develop an evaluation framework for fair cross-linguistic comparison of language models, using translated text so that all models are asked to predict approximately the same information.
result_label: We then conduct a study on 21 languages, demonstrating that in some languages, the textual expression of the information is harder to predict with both $n$-gram and LSTM language models.
result_label: We show complex inflectional morphology to be a cause of performance differences among languages.

===================================
paper_id: 2675113; YEAR: 1963
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: cited - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Some Universals of Grammar with Particular Reference to the Order of Meaningful Elements
ABSTRACT: background_label: A log slasher includes an elongated log receiving bunk and a saw assembly movable longitudinally along the bunk to saw logs supported in the bunk into shortened lengths.
background_label: The saw is pivotally supported at one end on a traveling trolley for movement with the trolley along the bunk and in a vertical plane toward and away from the bunk.
method_label: A guide moves with the saw to hold the saw against whip as the saw is moving into, and away from, a cut.
method_label: A lift crane lifts logs into and out of the bunk and includes clam fingers having opposite angled surfaces adjacent the free ends thereof for engaging logs of different diameter.
result_label: The trolley drive and that for moving the saw toward and away from the bunk are carried on the trolley and an articulated arm assembly, extendible and contractible in response to trolley movement, supports the power supply conduits in a compact arrangement throughout the range of movement of the trolley and saw.

===================================
paper_id: 17111881; YEAR: 2008
adju relevance: Irrelevant (0)
difference: 1; annotator2: 0; annotator3: 1
sources: abs_tfidfcbow200
TITLE: AnCora-Verb: A Lexical Resource for the Semantic Annotation of Corpora
ABSTRACT: background_label: AbstractIn this paper we present two large-scale verbal lexicons, AnCora-Verb-Ca for Catalan and AnCora-Verb-Es for Spanish, which are the basis for the semantic annotation with arguments and thematic roles of AnCora corpora.
method_label: In AnCora-Verb lexicons, the mapping between syntactic functions, arguments and thematic roles of each verbal predicate it is established taking into account the verbal semantic class and the diatheses alternations in which the predicate can participate.
method_label: Each verbal predicate is related to one or more semantic classes basically differentiated according to the four event classes ─accomplishments, achievements, states and activities─, and on the diatheses alternations in which a verb can occur.
method_label: AnCora-Verb-Es contains a total of 1965 different verbs corresponding to 3671 senses and AnCora-Verb-Ca contains 2151 verbs and 4513 senses.
result_label: These figures correspond to the total of 500,000 words contained in each corpus, AnCora-Ca and AnCora-Es.
result_label: The lexicons and the annotated corpora constitute the richest linguistic resources of this kind freely available for Spanish and Catalan.
background_label: The big amount of linguistic information contained in both resources should be of great interest for computational applications and linguistic studies.
other_label: Currently, a consulting interface for these lexicons is available at (http://clic.ub.edu/ancora/).

===================================
paper_id: 49564714; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: abs_tfidf - specter
TITLE: Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing
ABSTRACT: objective_label: Linguistic typology aims to capture structural and semantic variation across the world's languages.
background_label: A large-scale typology could provide excellent guidance for multilingual Natural Language Processing (NLP), particularly for languages that suffer from the lack of human labeled resources.
method_label: We present an extensive literature survey on the use of typological information in the development of NLP techniques.
method_label: Our survey demonstrates that to date, the use of information in existing typological databases has resulted in consistent but modest improvements in system performance.
method_label: We show that this is due to both intrinsic limitations of databases (in terms of coverage and feature granularity) and under-employment of the typological features included in them.
method_label: We advocate for a new approach that adapts the broad and discrete nature of typological categories to the contextual and continuous nature of machine learning algorithms used in contemporary NLP.
result_label: In particular, we suggest that such approach could be facilitated by recent developments in data-driven induction of typological knowledge.

===================================
paper_id: 8665208; YEAR: 2012
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_tfidf
TITLE: A Class-Based Agreement Model for Generating Accurately Inflected Translations
ABSTRACT: background_label: AbstractWhen automatically translating from a weakly inflected source language like English to a target language with richer grammatical features such as gender and dual number, the output commonly contains morpho-syntactic agreement errors.
objective_label: To address this issue, we present a target-side, class-based agreement model.
method_label: Agreement is promoted by scoring a sequence of fine-grained morpho-syntactic classes that are predicted during decoding for each translation hypothesis.
result_label: For English-to-Arabic translation, our model yields a +1.04 BLEU average improvement over a state-of-the-art baseline.
result_label: The model does not require bitext or phrase table annotations and can be easily implemented as a feature in many phrase-based decoders.

===================================
paper_id: 7003847; YEAR: 1991
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: Co-Occurrences Of Antonymous Adjectives And Their Contexts
ABSTRACT: background_label: Antonymic AssociationMuch current research in linguistics is concerned with textual or discourse bases for linguistic structure; within lexical semantics, such research is directed at particular lexical relations and at correlations between syntax and semantics.
background_label: This paper addresses the textual underpinnings of antonymy between predicative adjectives, following up research reported by Charles and Miller (1989) .Antonymy is a special lexical association between word pairs.
background_label: That it is lexical and not simply semantic follows from the fact that different words for the same concept can have different antonyms; for example, big-little and large-small are good antonym pairs, but large-little is not.
method_label: The classic work on associations among adjectives, and between antonymous adjectives in particular, is by Deese (1964 Deese ( , 1965 , analyzing the results of stimulus-response word-association tests.
method_label: 1 Charles and Miller (1989) argue, contrary to more complex psycholinguistic theories, that the primary source of these associations is a tendency they hypothesize for antonyms to co-occur within the same sentences in discourse.
result_label: This paper supports and extends their hypothesis.Deese's work on word association for adjectives was based on a list of the 278 most frequent adjectives in the Thorndike-Lorge (1944) count, those having a frequency of 50 per million or more.
result_label: Thirty-four pairs of these words had a reciprocal property: each member of the pair was the most frequent response to the other on word-association• Natural

===================================
paper_id: 46935302; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: title_tfidf
TITLE: Relational inductive biases, deep learning, and graph networks
ABSTRACT: background_label: Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making.
background_label: This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning.
background_label: However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches.
background_label: In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI.
result_label: The following is part position paper, part review, and part unification.
background_label: We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective.
background_label: Just as biology uses nature and nurture cooperatively, we reject the false choice between"hand-engineering"and"end-to-end"learning, and instead advocate for an approach which benefits from their complementary strengths.
method_label: We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them.
method_label: We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors.
method_label: We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning.
result_label: As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.

===================================
paper_id: 56358815; YEAR: 2001
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Statistical self-similarity of spatial variations of snow cover: verification of the hypothesis and application in the snowmelt runoff generation models
ABSTRACT: background_label: An analysis of snow cover measurement data in a number of physiographic regions and landscapes has shown that fields of snow cover characteristics can be considered as random fields with homogeneous increments and that these fields exhibit statistical self-similarity.
method_label: A physically based distributed model of snowmelt runoff generation developed for the Upper Kolyma River basin (the catchment area is about 100 000 km2) has been used to estimate the sensitivity of snowmelt dynamics over the basin and flood hydrographs to the parameterization of subgrid effects based on the hypothesis of statistical self-similarity of the maximum snow water equivalent fields.
method_label: Such parameterization of subgrid effects enables us to improve the description of snowmelt dynamics both within subgrid areas and over the entire river basin.
result_label: The snowmelt flood hydrographs appear less sensitive to the self-similarity of snow cover over subgrid areas than to the dynamics of snowmelt because of a too large catchment area of the basin under consideration.
result_label: However, for certain hydrometeorological conditions and for small river basins this effect may lead to significant changes of the calculated hydrographs.
other_label: Copyright © 2001 John Wiley & Sons, Ltd.

===================================
paper_id: 67855753; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 1; annotator2: 0; annotator3: 1
sources: specter - abs_cbow200
TITLE: Structural Supervision Improves Learning of Non-Local Grammatical Dependencies
ABSTRACT: background_label: State-of-the-art LSTM language models trained on large corpora learn sequential contingencies in impressive detail and have been shown to acquire a number of non-local grammatical dependencies with some success.
objective_label: Here we investigate whether supervision with hierarchical structure enhances learning of a range of grammatical dependencies, a question that has previously been addressed only for subject-verb agreement.
method_label: Using controlled experimental methods from psycholinguistics, we compare the performance of word-based LSTM models versus two models that represent hierarchical structure and deploy it in left-to-right processing: Recurrent Neural Network Grammars (RNNGs) (Dyer et al., 2016) and a incrementalized version of the Parsing-as-Language-Modeling configuration from Chariak et al., (2016).
method_label: Models are tested on a diverse range of configurations for two classes of non-local grammatical dependencies in English---Negative Polarity licensing and Filler--Gap Dependencies.
result_label: Using the same training data across models, we find that structurally-supervised models outperform the LSTM, with the RNNG demonstrating best results on both types of grammatical dependencies and even learning many of the Island Constraints on the filler--gap dependency.
result_label: Structural supervision thus provides data efficiency advantages over purely string-based training of neural language models in acquiring human-like generalizations about non-local grammatical dependencies.

===================================
paper_id: 61829139; YEAR: 2008
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: abs_tfidf
TITLE: RE-THINKING SIGN LANGUAGE VERB CLASSES: THE BODY AS SUBJECT
ABSTRACT: background_label: This paper offers a new look at the traditional analysis of verb classes in sign languages.
background_label: According to this analysis (Padden 1988), verbs in many sign languages fall into one of three classes: plain verbs, spatial verbs and agreement verbs.
background_label: These classes differ from each other with respect to the properties of the arguments which they encode.
background_label: Agreement verbs, verbs denoting transfer, encode the syntactic role of the arguments, as well as their person and number features, by the direction of the movement of the hands and the facing of the palms.
method_label: In spatial verbs, the class of verbs denoting motion and location in space, the direction of movement encodes the locations of locative arguments, the source and the goal.
result_label: The shape of the path movement the hands are tracing often depicts the shape of the path that an object traverses in space.
result_label: Plain verbs, which constitute the default semantic class, do not encode any grammatical features of their arguments.
background_label: The above analysis focuses on the role of the hands in encoding the relevant grammatical features.
background_label: The hands are the active articulator in sign languages, and they carry most of the informational load of the sign.
objective_label: However, in this paper we would like to offer a novel look at verb classification in sign languages, by looking not at what the hands do, but rather at the role the body plays in different classes of verb.
objective_label: We argue that the basic function of the body in verb forms in a sign language is to represent the subject argument.
method_label: Other grammatical functions encoded by verbs, such as 1 st person, develop later, and are superimposed of the basic function of "body as subject", thus creating more grammatical complexity in the language.
method_label: This analysis has the following advantages: it explains a typological peculiarity of sign language verb agreement, namely the prominence of the object over subject in verb agreement forms.
method_label: It offers an explanation of why some verb forms are more complex than others, in terms of a competition between the different roles of the body in various sub-systems of the language.
result_label: Finally, it makes interesting predictions regarding sign language typology and diachronic developments within sign languages.

===================================
paper_id: 193493017; YEAR: 2011
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Germans, Queenslanders and Londoners: The Semantics of Demonyms
ABSTRACT: background_label: Demonyms are best defined as words that refer to people of a place, although very little research has actually touched upon the different types of demonyms (by types I do not mean endonyms or exonyms, but the subgrouping within the demonyms).
method_label: This paper will fill the gap by exploring the semantics of demonyms, as they are used in the English language, It focuses on three terms Germans, Queenslanders and Londoners and shows how the semantic molecule ‘country’ is embedded in these concepts and is essential to understanding their meanings.
method_label: It also shows how the use of semantic template within the NSM framework can help to identify the different types of demonyms and their relationship.
method_label: The analysis will draw from the British National Corpus, Corpus of Contemporary American English, and Collins Wordbanks Online and demonstrate that the different types of demonyms can be identified, at least to some extent, by observing their behaviour in natural language.
result_label: It will show the terms used refer to people from countries ( Germans, Australians , and Danes ) do not occur alongside terms that refer to people from cities ( Londoners, Melbournians , Parisians ), that each type of demonym seems to have its own set of restrictions.

===================================
paper_id: 14091946; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 1; annotator2: 1; annotator3: 0
sources: cited - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies
ABSTRACT: background_label: The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities.
background_label: Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations?
method_label: We begin addressing this question using number agreement in English subject-verb dependencies.
method_label: We probe the architecture's grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models.
method_label: In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1% errors), but errors increased when sequential and structural information conflicted.
method_label: The frequency of such errors rose sharply in the language-modeling setting.
result_label: We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured.

===================================
paper_id: 36117198; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: title_tfidf
TITLE: DeepMind_Commentary
ABSTRACT: background_label: We agree with Lake and colleagues on their list of key ingredients for building humanlike intelligence, including the idea that model-based reasoning is essential.
background_label: However, we favor an approach that centers on one additional ingredient: autonomy.
objective_label: In particular, we aim toward agents that can both build and exploit their own internal models, with minimal human hand-engineering.
method_label: We believe an approach centered on autonomous learning has the greatest chance of success as we scale toward real-world complexity, tackling domains for which ready-made formal models are not available.
result_label: Here we survey several important examples of the progress that has been made toward building autonomous agents with humanlike abilities, and highlight some outstanding challenges.

===================================
paper_id: 122349887; YEAR: 1988
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: abs_tfidf
TITLE: Computational Consequences of Agreement and Ambiguity in Natural Language
ABSTRACT: background_label: The computer science technique of computational complexity analysis can provide powerful insights into the algorithm-neutral analysis of information processing tasks.
background_label: Here we show that a simple, theory-neutral linguistic model of syntactic agreement and ambiguity demonstrates that natural language parsing may be computationally intractable.
background_label: Significantly, we show that it may be syntactic features rather than rules that can cause this difficulty.
other_label: Informally, human languages and the computationally intractable Satisfiability (SAT) problem share two costly computional mechanisms: both enforce agreement among symbols across unbounded distances (Subject-Verb agreement) and both allow ambiguity (is a word a Noun or a Verb?
result_label: ).

===================================
paper_id: 5243955; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: Melbourne at SemEval 2016 Task 11: Classifying Type-level Word Complexity using Random Forests with Corpus and Word List Features
ABSTRACT: background_label: SemEval 2016 task 11 involved determining whether words in a sentence were complex or simple for a cohort of people with English as a second language.
background_label: Training data consisted of 200 annotated sentences, representing the combined judgements of 20 human annota- tors, such that if any annotator of the group labelled a word as complex, then it was con- sidered to be complex.
background_label: Testing was based on single annotator judgements.
method_label: Our system used a random forest classifier with a variety of features, the most important of which were term frequency statistics garnered from four large corpora, and style lexicons built on two large corpora.
method_label: Minor features in the final system include the presence or absence of words in various readability word lists; many other features we tried were not successful.
result_label: Our rank- ing amongst submitted systems did not reflect the strength of our system, due to submitting a far from optimal weighting between complex and simple, but we show that when a more ap- propriate weighting is used, our system ranks amongst the best submitted systems.

===================================
paper_id: 22350021; YEAR: 1991
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Learning to express motion events in English and Korean: The influence of language-specific lexicalization patterns
ABSTRACT: background_label: English and Korean differ in how they lexicalize the components of motion events.
background_label: English characteristically conflates Motion with Manner, Cause, or Deixis, and expresses Path separately.
background_label: Korean, in contrast, conflates Motion with Path and elements of Figure and Ground in transitive clauses for caused Motion, but conflates motion with Deixis and spells out Path and Manner separately in intransitive clauses for spontaneous motion.
background_label: Children learning English and Korean show sensitivity to language-specific patterns in the way they talk about motion from as early as 17-20 months.
method_label: For example, learners of English quickly generalize their earliest spatial words--Path particles like up, down, and in--to both spontaneous and caused changes of location and, for up and down, to posture changes, while learners of Korean keep words for spontaneous and caused motion strictly separate and use different words for vertical changes of location and posture changes.
result_label: These findings challenge the widespread view that children initially map spatial words directly to nonlinguistic spatial concepts, and suggest that they are influenced by the semantic organization of their language virtually from the beginning.
result_label: We discuss how input and cognition may interact in the early phases of learning to talk about space.

===================================
paper_id: 13297601; YEAR: 2005
adju relevance: Irrelevant (0)
difference: 1; annotator2: 0; annotator3: 1
sources: abs_tfidf
TITLE: Making syntax of sense: number agreement in sentence production.
ABSTRACT: background_label: Grammatical agreement flags the parts of sentences that belong together regardless of whether the parts appear together.
background_label: In English, the major agreement controller is the sentence subject, the major agreement targets are verbs and pronouns, and the major agreement category is number.
method_label: The authors expand an account of number agreement whose tenets are that pronouns acquire number lexically, whereas verbs acquire it syntactically but with similar contributions from number meaning and from the number morphology of agreement controllers.
method_label: These tenets were instantiated in a model using existing verb agreement data.
method_label: The model was then fit to a new, more extensive set of verb data and tested with a parallel set of pronoun data.
result_label: The theory was supported by the model's outcomes.
result_label: The results have implications for the integration of words and structures, for the workings of agreement categories, and for the nature of the transition from thought to language.

===================================
paper_id: 18198045; YEAR: 2012
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Harnessing the CRF Complexity with Domain-Specific Constraints. The Case of Morphosyntactic Tagging of a Highly Inflected Language
ABSTRACT: background_label: ABSTRACTWe describe a domain-specific method of adapting conditional random fields (CRFs) to morphosyntactic tagging of highly-inflectional languages.
method_label: The solution involves extending CRFs with additional, position-wise restrictions on the output domain, which are used to impose consistency between the modeled label sequences and morphosyntactic analysis results both at the level of decoding and, more importantly, in parameters estimation process.
method_label: We decompose the problem of morphosyntactic disambiguation into two consecutive stages of the context-sensitive morphosyntactic guessing and the disambiguation proper.
method_label: The division helps in designing well-adjusted, CRF-based methods for both tasks, which in combination constitute Concraft, a highly accurate tagging system for the Polish language available under the 2-clause BSD license.
result_label: Evaluation on the National Corpus of Polish shows that our solution significantly outperforms other state-of-the-art taggers for Polish -Pantera, WMBT and WCRFT -especially in terms of the accuracy measured with respect to unknown words.

===================================
paper_id: 11620784; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200 - specter
TITLE: Measuring Gradience in Speakers' Grammaticality Judgements
ABSTRACT: background_label: The question of whether grammaticality is a binary categorical or a gradient property has been the subject of ongoing debate in linguistics and psychology for many years.
background_label: Linguists have tended to use constructed examples to test speakers’ judgements on specific sorts of constraint violation.
method_label: We applied machine translation to randomly selected subsets of the British National Corpus (BNC) to generate a large test set which contains well-formed English source sentences, and sentences that exhibit a wide variety of grammatical infelicities.
method_label: We tested a large number of speakers through (filtered) crowd sourcing, with three distinct modes of classification, one binary and two ordered scales.
method_label: We found a high degree of correlation in mean judgements for sentences across the three classification tasks.
method_label: We also did two visual image classification tasks to obtain benchmarks for binary and gradient judgement patterns, respectively.
result_label: Finally, we did a second crowd source experiment on 100 randomly selected linguistic textbook example sentences.
result_label: The sentence judgement distributions for individual speakers strongly resemble the gradience benchmark pattern.
result_label: This evidence suggests that speakers represent grammatical well-formedness as a gradient property.

===================================
paper_id: 3161350; YEAR: 2010
adju relevance: Irrelevant (0)
difference: 1; annotator2: 1; annotator3: 0
sources: abs_tfidfcbow200 - abs_tfidf - specter
TITLE: Experience and grammatical agreement: Statistical learning shapes number agreement production
ABSTRACT: background_label: A robust result in research on the production of grammatical agreement is that speakers are more likely to produce an erroneous verb with phrases such as the key to the cabinets, with a singular noun followed by a plural one, than with phrases such as the keys to the cabinet, where a plural noun is followed by a singular.
background_label: These asymmetries are thought to reflect core language production processes.
background_label: Previous accounts have attributed error patterns to a syntactic number feature present on plurals but not singulars.
method_label: An alternative approach is presented in which a process similar to structural priming contributes to the error asymmetry via speakers' past experiences with related agreement constructions.
method_label: A corpus analysis and two agreement production studies test this account.
result_label: The results suggest that agreement production is shaped by statistical learning from past language experience.
result_label: Implications for accounts of agreement are discussed.

===================================
paper_id: 53219915; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 2; annotator2: 0; annotator3: 2
sources: specter - abs_tfidfcbow200
TITLE: Do RNNs learn human-like abstract word order preferences?
ABSTRACT: background_label: RNN language models have achieved state-of-the-art results on various tasks, but what exactly they are representing about syntax is as yet unclear.
objective_label: Here we investigate whether RNN language models learn humanlike word order preferences in syntactic alternations.
method_label: We collect language model surprisal scores for controlled sentence stimuli exhibiting major syntactic alternations in English: heavy NP shift, particle shift, the dative alternation, and the genitive alternation.
method_label: We show that RNN language models reproduce human preferences in these alternations based on NP length, animacy, and definiteness.
method_label: We collect human acceptability ratings for our stimuli, in the first acceptability judgment experiment directly manipulating the predictors of syntactic alternations.
result_label: We show that the RNNs' performance is similar to the human acceptability ratings and is not matched by an n-gram baseline model.
result_label: Our results show that RNNs learn the abstract features of weight, animacy, and definiteness which underlie soft constraints on syntactic alternations.

===================================
paper_id: 52010508; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 1; annotator2: 1; annotator3: 0
sources: cited - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: RNN Simulations of Grammaticality Judgments on Long-distance Dependencies
ABSTRACT: background_label: AbstractThe paper explores the ability of LSTM networks trained on a language modeling task to detect linguistic structures which are ungrammatical due to extraction violations (extra arguments and subject-relative clause island violations), and considers its implications for the debate on language innatism.
method_label: The results show that the current RNN model can correctly classify (un)grammatical sentences, in certain conditions, but it is sensitive to linguistic processing factors and probably ultimately unable to induce a more abstract notion of grammaticality, at least in the domain we tested.
result_label: Title and Abstract in ItalianRNN Simulazioni di giudizi di grammaticità sulle dipendenze a distanza L'articolo studia la capacità delle reti neurali LSTM addestrate su un compito di modellazione linguistica di rilevare strutture linguistiche che sono agrammaticali a causa di violazioni nella estrazione di argomenti (dovute alla presenza di argomenti di troppo, o alla presenza di isole del soggetto e delle frasi relative), esplorando le implicazioni per il dibattito sull'innatismo linguistico.
result_label: I risultati mostrano che l'attuale modello RNN può classificare correttamente frasi grammaticali, in certe condizioni, maè eccessivamente sensibile a fattori di elaborazione linguistica e probabilmente non in grado di indurre una nozione più astratta di grammaticalità, almeno nel dominio da noi testato.

===================================
paper_id: 21855669; YEAR: 1999
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Quantifying complex patterns of bioacoustic variation: use of a neural network to compare killer whale (Orcinus orca) dialects.
ABSTRACT: background_label: A quantitative measure of acoustic similarity is crucial to any study comparing vocalizations of different species, social groups, or individuals.
objective_label: The goal of this study was to develop a method of extracting frequency contours from recordings of pulsed vocalizations and to test a nonlinear index of acoustic similarity based on the error of an artificial neural network at classifying them.
method_label: Since the performance of neural networks depends on the amount of consistent variation in the training data, this technique can be used to assess such variation from samples of acoustic signals.
method_label: The frequency contour extraction and the neural network index were tested on samples of one call type shared by nine social groups of killer whales.
method_label: For comparison, call similarity was judged by three human subjects in pairwise classification tasks.
result_label: The results showed a significant correlation between the neural network index and the similarity ratings by the subjects.
result_label: Both measures of acoustic similarity were significantly correlated with the groups' association patterns, indicating that both methods of quantifying acoustic similarity are biologically meaningful.
result_label: An index based on neural network analysis therefore represents an objective and repeatable means of measuring acoustic similarity, and allows comparison of results across studies, species and time.

===================================
paper_id: 24041868; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: title_cbow200
TITLE: The Controlled Natural Language of Randall Munroe's Thing Explainer
ABSTRACT: background_label: It is rare that texts or entire books written in a Controlled Natural Language (CNL) become very popular, but exactly this has happened with a book that has been published last year.
background_label: Randall Munroe's Thing Explainer uses only the 1'000 most often used words of the English language together with drawn pictures to explain complicated things such as nuclear reactors, jet engines, the solar system, and dishwashers.
background_label: This restricted language is a very interesting new case for the CNL community.
result_label: I describe here its place in the context of existing approaches on Controlled Natural Languages, and I provide a first analysis from a scientific perspective, covering the word production rules and word distributions.

===================================
paper_id: 16959835; YEAR: 2012
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200 - specter
TITLE: Revisiting the syntactic abilities of non-human animals: natural vocalizations and artificial grammar learning
ABSTRACT: background_label: The domain of syntax is seen as the core of the language faculty and as the most critical difference between animal vocalizations and language.
background_label: We review evidence from spontaneously produced vocalizations as well as from perceptual experiments using artificial grammars to analyse animal syntactic abilities, i.e.
objective_label: abilities to produce and perceive patterns following abstract rules.
method_label: Animal vocalizations consist of vocal units (elements) that are combined in a species-specific way to create higher order strings that in turn can be produced in different patterns.
method_label: While these patterns differ between species, they have in common that they are no more complex than a probabilistic finite-state grammar.
method_label: Experiments on the perception of artificial grammars confirm that animals can generalize and categorize vocal strings based on phonetic features.
result_label: They also demonstrate that animals can learn about the co-occurrence of elements or learn simple ‘rules’ like attending to reduplications of units.
result_label: However, these experiments do not provide strong evidence for an ability to detect abstract rules or rules beyond finite-state grammars.
result_label: Nevertheless, considering the rather limited number of experiments and the difficulty to design experiments that unequivocally demonstrate more complex rule learning, the question of what animals are able to do remains open.

===================================
paper_id: 57825754; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 1; annotator2: 1; annotator3: 0
sources: specter
TITLE: Linguistic Analysis of Pretrained Sentence Encoders with Acceptability Judgments
ABSTRACT: background_label: Recent work on evaluating grammatical knowledge in pretrained sentence encoders gives a fine-grained view of a small number of phenomena.
background_label: We introduce a new analysis dataset that also has broad coverage of linguistic phenomena.
method_label: We annotate the development set of the Corpus of Linguistic Acceptability (CoLA; Warstadt et al., 2018) for the presence of 13 classes of syntactic phenomena including various forms of argument alternations, movement, and modification.
method_label: We use this analysis set to investigate the grammatical knowledge of three pretrained encoders: BERT (Devlin et al., 2018), GPT (Radford et al., 2018), and the BiLSTM baseline from Warstadt et al.
method_label: We find that these models have a strong command of complex or non-canonical argument structures like ditransitives (Sue gave Dan a book) and passives (The book was read).
method_label: Sentences with long distance dependencies like questions (What do you think I ate?)
result_label: challenge all models, but for these, BERT and GPT have a distinct advantage over the baseline.
result_label: We conclude that recent sentence encoders, despite showing near-human performance on acceptability classification overall, still fail to make fine-grained grammaticality distinctions for many complex syntactic structures.

===================================
paper_id: 25446416; YEAR: 1998
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: GEOMETRIC MORPHOMETRICS OF DEVELOPMENTAL INSTABILITY: ANALYZING PATTERNS OF FLUCTUATING ASYMMETRY WITH PROCRUSTES METHODS.
ABSTRACT: background_label: Although fluctuating asymmetry has become popular as a measure of developmental instability, few studies have examined its developmental basis.
objective_label: We propose an approach to investigate the role of development for morphological asymmetry by means of morphometric methods.
method_label: Our approach combines geometric morphometrics with the two-way ANOVA customary for conventional analyses of fluctuating asymmetry and can discover localized features of shape variation by examining the patterns of covariance among landmarks.
method_label: This approach extends the notion of form used in studies of fluctuating asymmetry from collections of distances between morphological landmarks to an explicitly geometric concept of shape characterized by the configuration of landmarks.
method_label: We demonstrate this approach with a study of asymmetry in the wings of tsetse flies (Glossina palpalis gambiensis).
result_label: The analysis revealed significant fluctuating and directional asymmetry for shape as well as ample shape variation among individuals and between the offspring of young and old females.
result_label: The morphological landmarks differed markedly in their degree of variability but multivariate patterns of landmark covariation identified by principal component analysis were generally similar between fluctuating asymmetry (within-individual variability) and variation among individuals.
result_label: Therefore there is no evidence that special developmental processes control fluctuating asymmetry.
result_label: We relate some of the morphometric patterns to processes known to be involved in the development of fly wings.

===================================
paper_id: 10729723; YEAR: 2009
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: title_tfidfcbow200
TITLE: Neural network processing of natural language: II. Towards a unified model of corticostriatal function in learning sentence comprehension and non-linguistic sequencing.
ABSTRACT: background_label: A central issue in cognitive neuroscience today concerns how distributed neural networks in the brain that are used in language learning and processing can be involved in non-linguistic cognitive sequence learning.
background_label: This issue is informed by a wealth of functional neurophysiology studies of sentence comprehension, along with a number of recent studies that examined the brain processes involved in learning non-linguistic sequences, or artificial grammar learning (AGL).
objective_label: The current research attempts to reconcile these data with several current neurophysiologically based models of sentence processing, through the specification of a neural network model whose architecture is constrained by the known cortico-striato-thalamo-cortical (CSTC) neuroanatomy of the human language system.
objective_label: The challenge is to develop simulation models that take into account constraints both from neuranatomical connectivity, and from functional imaging data, and that can actually learn and perform the same kind of language and artificial syntax tasks.
method_label: In our proposed model, structural cues encoded in a recurrent cortical network in BA47 activate a CSTC circuit to modulate the flow of lexical semantic information from BA45 to an integrated representation of meaning at the sentence level in BA44/6.
method_label: During language acquisition, corticostriatal plasticity is employed to allow closed class structure to drive thematic role assignment.
method_label: From the AGL perspective, repetitive internal structure in the AGL strings is encoded in BA47, and activates the CSTC circuit to predict the next element in the sequence.
background_label: Simulation results from Caplan's [Caplan, D., Baker, C., & Dehaut, F. (1985).
background_label: Syntactic determinants of sentence comprehension in aphasia.
background_label: Cognition, 21, 117-175] test of syntactic comprehension, and from Gomez and Schvaneveldts' [Gomez, R. L., & Schvaneveldt, R. W. (1994).
method_label: What is learned from artificial grammars?.
method_label: Transfer tests of simple association.
method_label: Journal of Experimental Psychology: Learning, Memory and Cognition, 20, 396-410] artificial grammar learning experiments are presented.
result_label: These results are discussed in the context of a brain architecture for learning grammatical structure for multiple natural languages, and non-linguistic sequences.

===================================
paper_id: 118988729; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: title_tfidf
TITLE: A Microphotonic Astrocomb
ABSTRACT: background_label: One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers.
background_label: It has already been realized that the large number of exactly known optical frequencies provided by laser frequency combs ('astrocombs') can significantly surpass conventionally used hollow-cathode lamps as calibration light sources.
background_label: A remaining challenge, however, is generation of frequency combs with lines resolvable by astronomical spectrometers.
method_label: Here we demonstrate an astrocomb generated via soliton formation in an on-chip microphotonic resonator ('microresonator') with a resolvable line spacing of 23.7 GHz.
method_label: This comb is providing wavelength calibration on the 10 cm/s radial velocity level on the GIANO-B high-resolution near-infrared spectrometer.
result_label: As such, microresonator frequency combs have the potential of providing broadband wavelength calibration for the next-generation of astronomical instruments in planet-hunting and cosmological research.

===================================
paper_id: 60232836; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator2: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: The ICNALE and Sophisticated Contrastive Interlanguage Analysis of Asian Learners of English
ABSTRACT: background_label: The International Corpus Network of Asian Learners of English (ICNALE) is a new learner corpus designed for a reliable contrastive interlanguage analysis of varied English learners in Asia.
background_label: The ICNALE, in which writing conditions are controlled more strictly compared with other major learner corpora, allows researchers to examine the differences between writer groups in greater detail.
objective_label: The current paper outlines the features of the ICNALE and demonstrates how it can contribute to the sophistication of contrastive interlanguage analysis.

