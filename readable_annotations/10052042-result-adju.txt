======================================================================
paper_id: 10052042; YEAR: 2017
TITLE: Learning Topic-Sensitive Word Representations
ABSTRACT: background_label: Distributed word representations are widely used for modeling words in NLP tasks.
background_label: Most of the existing models generate one representation per word and do not consider different meanings of a word.
method_label: We present two approaches to learn multiple topic-sensitive representations per word by using Hierarchical Dirichlet Process.
method_label: We observe that by modeling topics and integrating topic distributions for each document we obtain representations that are able to distinguish between different meanings of a given word.
result_label: Our models yield statistically significant improvements for the lexical substitution task indicating that commonly used single word representations, even when combined with contextual information, are insufficient for this task.
===================================
paper_id: 1518169; YEAR: 2013
adju relevance: Identical (+3)
difference: 1; annotator4: 2; annotator3: 3
sources: cited - title_tfidf - abs_tfidf - title_tfidfcbow200 - title_cbow200 - abs_cbow200 - specter - abs_tfidfcbow200
TITLE: Learning to Rank Lexical Substitutions
ABSTRACT: background_label: AbstractThe problem to replace a word with a synonym that fits well in its sentential context is known as the lexical substitution task.
objective_label: In this paper, we tackle this task as a supervised ranking problem.
objective_label: Given a dataset of target words, their sentential contexts and the potential substitutions for the target words, the goal is to train a model that accurately ranks the candidate substitutions based on their contextual fitness.
method_label: As a key contribution, we customize and evaluate several learning-to-rank models to the lexical substitution task, including classification-based and regression-based approaches.
result_label: On two datasets widely used for lexical substitution, our best models significantly advance the state-of-the-art.

===================================
paper_id: 10076790; YEAR: 2015
adju relevance: Identical (+3)
difference: 1; annotator4: 2; annotator3: 3
sources: specter - abs_tfidf
TITLE: Modeling Word Meaning in Context with Substitute Vectors
ABSTRACT: background_label: Context representations are a key element in distributional models of word meaning.
background_label: In contrast to typical representations based on neighboring words, a recently proposed approach suggests to represent a context of a target word by a substitute vector, comprising the potential fillers for the target word slot in that context.
method_label: In this work we first propose a variant of substitute vectors, which we find particularly suitable for measuring context similarity.
method_label: Then, we propose a novel model for representing word meaning in context based on this context representation.
result_label: Our model outperforms state-of-the-art results on lexical substitution tasks in an unsupervised setting.

===================================
paper_id: 2156506; YEAR: 2010
adju relevance: Identical (+3)
difference: 2; annotator4: 1; annotator3: 3
sources: cited - title_tfidf - abs_tfidf - title_tfidfcbow200 - title_cbow200 - abs_cbow200 - specter - abs_tfidfcbow200
TITLE: Multi-Prototype Vector-Space Models of Word Meaning
ABSTRACT: background_label: AbstractCurrent vector-space models of lexical semantics create a single "prototype" vector to represent the meaning of a word.
background_label: However, due to lexical ambiguity, encoding word meaning with a single vector is problematic.
method_label: This paper presents a method that uses clustering to produce multiple "sense-specific" vectors for each word.
method_label: This approach provides a context-dependent vector representation of word meaning that naturally accommodates homonymy and polysemy.
result_label: Experimental comparisons to human judgements of semantic similarity for both isolated words as well as words in sentential contexts demonstrate the superiority of this approach over both prototype and exemplar based vector-space models.

===================================
paper_id: 11706860; YEAR: 2015
adju relevance: Similar (+2)
difference: 0; annotator4: 2; annotator3: 2
sources: abs_cbow200 - abs_tfidf - specter
TITLE: Learning to Represent Words in Context with Multilingual Supervision
ABSTRACT: background_label: We present a neural network architecture based on bidirectional LSTMs to compute representations of words in the sentential contexts.
background_label: These context-sensitive word representations are suitable for, e.g., distinguishing different word senses and other context-modulated variations in meaning.
method_label: To learn the parameters of our model, we use cross-lingual supervision, hypothesizing that a good representation of a word in context will be one that is sufficient for selecting the correct translation into a second language.
result_label: We evaluate the quality of our representations as features in three downstream tasks: prediction of semantic supersenses (which assign nouns and verbs into a few dozen semantic classes), low resource machine translation, and a lexical substitution task, and obtain state-of-the-art results on all of these.

===================================
paper_id: 49325336; YEAR: 2019
adju relevance: Similar (+2)
difference: 0; annotator4: 2; annotator3: 2
sources: specter
TITLE: Distributed representation of multi-sense words: A loss-driven approach
ABSTRACT: background_label: Word2Vec's Skip Gram model is the current state-of-the-art approach for estimating the distributed representation of words.
background_label: However, it assumes a single vector per word, which is not well-suited for representing words that have multiple senses.
objective_label: This work presents LDMI, a new model for estimating distributional representations of words.
method_label: LDMI relies on the idea that, if a word carries multiple senses, then having a different representation for each of its senses should lead to a lower loss associated with predicting its co-occurring words, as opposed to the case when a single vector representation is used for all the senses.
method_label: After identifying the multi-sense words, LDMI clusters the occurrences of these words to assign a sense to each occurrence.
result_label: Experiments on the contextual word similarity task show that LDMI leads to better performance than competing approaches.

===================================
paper_id: 17934269; YEAR: 2016
adju relevance: Similar (+2)
difference: 1; annotator4: 1; annotator3: 2
sources: abs_tfidf - specter
TITLE: Multi-phase Word Sense Embedding Learning Using a Corpus and a Lexical Ontology
ABSTRACT: background_label: AbstractWord embeddings play a significant role in many modern NLP systems.
background_label: However, most prevalent word embedding learning methods learn one representation per word which is problematic for polysemous words and homonymous words.
method_label: To address this problem, we propose a multi-phase word sense embedding learning method which utilizes both a corpus and a lexical ontology to learn one embedding per word sense.
method_label: We use word sense definitions and relations between word senses defined in a lexical ontology in a different way from existing systems.
result_label: Experimental results on word similarity task show that our approach produces word sense embeddings of high quality.

===================================
paper_id: 6222768; YEAR: 2015
adju relevance: Similar (+2)
difference: 0; annotator4: 2; annotator3: 2
sources: cited - title_tfidf - abs_tfidf - title_tfidfcbow200 - title_cbow200 - abs_cbow200 - specter - abs_tfidfcbow200
TITLE: Do Multi-Sense Embeddings Improve Natural Language Understanding?
ABSTRACT: background_label: Learning a distinct representation for each sense of an ambiguous word could lead to more powerful and fine-grained models of vector-space representations.
background_label: Yet while `multi-sense' methods have been proposed and tested on artificial word-similarity tasks, we don't know if they improve real natural language understanding tasks.
method_label: In this paper we introduce a multi-sense embedding model based on Chinese Restaurant Processes that achieves state of the art performance on matching human word similarity judgments, and propose a pipelined architecture for incorporating multi-sense embeddings into language understanding.
method_label: We then test the performance of our model on part-of-speech tagging, named entity recognition, sentiment analysis, semantic relation identification and semantic relatedness, controlling for embedding dimensionality.
result_label: We find that multi-sense embeddings do improve performance on some tasks (part-of-speech tagging, semantic relation identification, semantic relatedness) but not on others (named entity recognition, various forms of sentiment analysis).
result_label: We discuss how these differences may be caused by the different role of word sense information in each of the tasks.
result_label: The results highlight the importance of testing embedding models in real applications.

===================================
paper_id: 15963304; YEAR: 2013
adju relevance: Similar (+2)
difference: 2; annotator4: 2; annotator3: 0
sources: title_tfidf
TITLE: unimelb: Topic Modelling-based Word Sense Induction
ABSTRACT: background_label: AbstractThis paper describes our system for shared task 13 "Word Sense Induction for Graded and Non-Graded Senses" of SemEval-2013.
method_label: The task is on word sense induction (WSI), and builds on earlier SemEval WSI tasks in exploring the possibility of multiple senses being compatible to varying degrees with a single contextual instance: participants are asked to grade senses rather than selecting a single sense like most word sense disambiguation (WSD) settings.
method_label: The evaluation measures are designed to assess how well a system perceives the different senses in a contextual instance.
method_label: We adopt a previously-proposed WSI methodology for the task, which is based on a Hierarchical Dirichlet Process (HDP), a nonparametric topic model.
result_label: Our system requires no parameter tuning, uses the English ukWaC as an external resource, and achieves encouraging results over the shared task.

===================================
paper_id: 17262985; YEAR: 2016
adju relevance: Similar (+2)
difference: 1; annotator4: 2; annotator3: 1
sources: abs_tfidf - title_tfidf - abs_cbow200 - abs_tfidfcbow200
TITLE: Identity-sensitive Word Embedding through Heterogeneous Networks
ABSTRACT: background_label: Most existing word embedding approaches do not distinguish the same words in different contexts, therefore ignoring their contextual meanings.
background_label: As a result, the learned embeddings of these words are usually a mixture of multiple meanings.
background_label: In this paper, we acknowledge multiple identities of the same word in different contexts and learn the \textbf{identity-sensitive} word embeddings.
method_label: Based on an identity-labeled text corpora, a heterogeneous network of words and word identities is constructed to model different-levels of word co-occurrences.
method_label: The heterogeneous network is further embedded into a low-dimensional space through a principled network embedding approach, through which we are able to obtain the embeddings of words and the embeddings of word identities.
method_label: We study three different types of word identities including topics, sentiments and categories.
result_label: Experimental results on real-world data sets show that the identity-sensitive word embeddings learned by our approach indeed capture different meanings of words and outperforms competitive methods on tasks including text classification and word similarity computation.

===================================
paper_id: 2897037; YEAR: 2015
adju relevance: Similar (+2)
difference: 1; annotator4: 2; annotator3: 3
sources: cited - title_tfidf - abs_tfidf - title_tfidfcbow200 - title_cbow200 - abs_cbow200 - specter - abs_tfidfcbow200
TITLE: A Simple Word Embedding Model for Lexical Substitution
ABSTRACT: background_label: The lexical substitution task requires identifying meaning-preserving substitutes for a target word instance in a given sentential context.
background_label: Since its introduction in SemEval-2007, various models addressed this challenge, mostly in an unsupervised setting.
objective_label: In this work we propose a simple model for lexical substitution, which is based on the popular skip-gram word embedding model.
method_label: The novelty of our approach is in leveraging explicitly the context embeddings generated within the skip-gram model, which were so far considered only as an internal component of the learning process.
result_label: Our model is efficient, very simple to implement, and at the same time achieves state-ofthe-art results on lexical substitution tasks in an unsupervised setting.

===================================
paper_id: 16173223; YEAR: 2016
adju relevance: Similar (+2)
difference: 2; annotator4: 0; annotator3: 2
sources: abs_tfidf - abs_cbow200 - specter
TITLE: De-Conflated Semantic Representations
ABSTRACT: background_label: One major deficiency of most semantic representation techniques is that they usually model a word type as a single point in the semantic space, hence conflating all the meanings that the word can have.
background_label: Addressing this issue by learning distinct representations for individual meanings of words has been the subject of several research studies in the past few years.
background_label: However, the generated sense representations are either not linked to any sense inventory or are unreliable for infrequent word senses.
method_label: We propose a technique that tackles these problems by de-conflating the representations of words based on the deep knowledge it derives from a semantic network.
method_label: Our approach provides multiple advantages in comparison to the past work, including its high coverage and the ability to generate accurate representations even for infrequent word senses.
result_label: We carry out evaluations on six datasets across two semantic similarity tasks and report state-of-the-art results on most of them.

===================================
paper_id: 13064333; YEAR: 2016
adju relevance: Similar (+2)
difference: 1; annotator4: 1; annotator3: 2
sources: abs_tfidf - title_cbow200 - specter
TITLE: Learning Word Sense Embeddings from Word Sense Definitions
ABSTRACT: background_label: Word embeddings play a significant role in many modern NLP systems.
background_label: Since learning one representation per word is problematic for polysemous words and homonymous words, researchers propose to use one embedding per word sense.
background_label: Their approaches mainly train word sense embeddings on a corpus.
method_label: In this paper, we propose to use word sense definitions to learn one embedding per word sense.
result_label: Experimental results on word similarity tasks and a word sense disambiguation task show that word sense embeddings produced by our approach are of high quality.

===================================
paper_id: 9693038; YEAR: 2014
adju relevance: Similar (+2)
difference: 2; annotator4: 0; annotator3: 2
sources: abs_tfidf - abs_cbow200 - specter - abs_tfidfcbow200
TITLE: A Probabilistic Model for Learning Multi-Prototype Word Embeddings
ABSTRACT: background_label: AbstractDistributed word representations have been widely used and proven to be useful in quite a few natural language processing and text mining tasks.
background_label: Most of existing word embedding models aim at generating only one embedding vector for each individual word, which, however, limits their effectiveness because huge amounts of words are polysemous (such as bank and star).
background_label: To address this problem, it is necessary to build multi embedding vectors to represent different meanings of a word respectively.
method_label: Some recent studies attempted to train multi-prototype word embeddings through clustering context window features of the word.
method_label: However, due to a large number of parameters to train, these methods yield limited scalability and are inefficient to be trained with big data.
method_label: In this paper, we introduce a much more efficient method for learning multi embedding vectors for polysemous words.
method_label: In particular, we first propose to model word polysemy from a probabilistic perspective and integrate it with the highly efficient continuous Skip-Gram model.
method_label: Under this framework, we design an Expectation-Maximization algorithm to learn the word's multi embedding vectors.
result_label: With much less parameters to train, our model can achieve comparable or even better results on word-similarity tasks compared with conventional methods.

===================================
paper_id: 15251438; YEAR: 2015
adju relevance: Similar (+2)
difference: 1; annotator4: 2; annotator3: 3
sources: cited - title_tfidf - abs_tfidf - title_tfidfcbow200 - title_cbow200 - abs_cbow200 - specter - abs_tfidfcbow200
TITLE: Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space
ABSTRACT: background_label: There is rising interest in vector-space word embeddings and their use in NLP, especially given recent methods for their fast estimation at very large scale.
background_label: Nearly all this work, however, assumes a single vector per word type ignoring polysemy and thus jeopardizing their usefulness for downstream tasks.
objective_label: We present an extension to the Skip-gram model that efficiently learns multiple embeddings per word type.
method_label: It differs from recent related work by jointly performing word sense discrimination and embedding learning, by non-parametrically estimating the number of senses per word type, and by its efficiency and scalability.
result_label: We present new state-of-the-art results in the word similarity in context task and demonstrate its scalability by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours.

===================================
paper_id: 121125604; YEAR: 2019
adju relevance: Related (+1)
difference: 2; annotator4: 2; annotator3: 0
sources: abs_tfidfcbow200 - abs_tfidf
TITLE: Evaluating the Underlying Gender Bias in Contextualized Word Embeddings
ABSTRACT: background_label: Gender bias is highly impacting natural language processing applications.
background_label: Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources.
background_label: Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in.
objective_label: In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias.
method_label: Our analysis includes different measures previously applied in the literature to standard word embeddings.
result_label: Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.

===================================
paper_id: 14556042; YEAR: 2016
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: abs_tfidf - specter
TITLE: Multi-phase Word Sense Embedding Retrofitting with Lexical Ontology
ABSTRACT: background_label: Word embeddings play a significant role in many modern NLP systems.
background_label: However, most used word embedding learning methods learn one representation per word which is problematic for polysemous words and homonymous words.
method_label: To address this problem, we propose a multi-phase word sense embedding retrofitting method which utilizes a lexical ontology to learn one embedding per word sense.
method_label: We use word sense definitions and relations between word senses defined in a lexical ontology in a different way from existing systems.
result_label: Experimental results on word similarity task show that our approach remarkablely improves the quality of embeddings.

===================================
paper_id: 3411445; YEAR: 2018
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: specter - abs_tfidf - abs_tfidfcbow200
TITLE: Learning Word Vectors for 157 Languages
ABSTRACT: background_label: Distributed word representations, or word vectors, have recently been applied to many tasks in natural language processing, leading to state-of-the-art performance.
background_label: A key ingredient to the successful application of these representations is to train them on very large corpora, and use these pre-trained models in downstream tasks.
objective_label: In this paper, we describe how we trained such high quality word representations for 157 languages.
method_label: We used two sources of data to train these models: the free online encyclopedia Wikipedia and data from the common crawl project.
method_label: We also introduce three new word analogy datasets to evaluate these word vectors, for French, Hindi and Polish.
result_label: Finally, we evaluate our pre-trained word vectors on 10 languages for which evaluation datasets exists, showing very strong performance compared to previous models.

===================================
paper_id: 505323; YEAR: 2015
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: title_tfidfcbow200 - title_cbow200
TITLE: Learning Better Embeddings for Rare Words Using Distributional Representations
ABSTRACT: background_label: There are two main types of word representations: low-dimensional embeddings and high-dimensional distributional vectors, in which each dimension corresponds to a context word.
method_label: In this paper, we initialize an embedding-learning model with distributional vectors.
result_label: Evaluation on word similarity shows that this initialization significantly increases the quality of embeddings for rare words.

===================================
paper_id: 10171569; YEAR: 2009
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: cited - title_tfidf - abs_tfidf - title_tfidfcbow200 - title_cbow200 - abs_cbow200 - specter - abs_tfidfcbow200
TITLE: Bayesian Word Sense Induction
ABSTRACT: background_label: Sense induction seeks to automatically identify word senses directly from a corpus.
background_label: A key assumption underlying previous work is that the context surrounding an ambiguous word is indicative of its meaning.
objective_label: Sense induction is thus typically viewed as an unsupervised clustering problem where the aim is to partition a word's contexts into different classes, each representing a word sense.
method_label: Our work places sense induction in a Bayesian context by modeling the contexts of the ambiguous word as samples from a multinomial distribution over senses which are in turn characterized as distributions over words.
method_label: The Bayesian framework provides a principled way to incorporate a wide range of features beyond lexical co-occurrences and to systematically assess their utility on the sense induction task.
result_label: The proposed approach yields improvements over state-of-the-art systems on a benchmark dataset.

===================================
paper_id: 160009341; YEAR: 2019
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 2
sources: abs_tfidfcbow200 - abs_tfidf - specter
TITLE: Word Usage Similarity Estimation with Sentence Representations and Automatic Substitutes
ABSTRACT: background_label: Usage similarity estimation addresses the semantic proximity of word instances in different contexts.
method_label: We apply contextualized (ELMo and BERT) word and sentence embeddings to this task, and propose supervised models that leverage these representations for prediction.
method_label: Our models are further assisted by lexical substitute annotations automatically assigned to word instances by context2vec, a neural model that relies on a bidirectional LSTM.
method_label: We perform an extensive comparison of existing word and sentence representations on benchmark datasets addressing both graded and binary similarity.
result_label: The best performing models outperform previous methods in both settings.

===================================
paper_id: 1779089; YEAR: 2015
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_cbow200 - title_tfidf - abs_tfidfcbow200
TITLE: Embedding Semantic Relations into Word Representations
ABSTRACT: background_label: Learning representations for semantic relations is important for various tasks such as analogy detection, relational search, and relation classification.
background_label: Although there have been several proposals for learning representations for individual words, learning word representations that explicitly capture the semantic relations between words remains under developed.
method_label: We propose an unsupervised method for learning vector representations for words such that the learnt representations are sensitive to the semantic relations that exist between two words.
method_label: First, we extract lexical patterns from the co-occurrence contexts of two words in a corpus to represent the semantic relations that exist between those two words.
method_label: Second, we represent a lexical pattern as the weighted sum of the representations of the words that co-occur with that lexical pattern.
method_label: Third, we train a binary classifier to detect relationally similar vs. non-similar lexical pattern pairs.
method_label: The proposed method is unsupervised in the sense that the lexical pattern pairs we use as train data are automatically sampled from a corpus, without requiring any manual intervention.
result_label: Our proposed method statistically significantly outperforms the current state-of-the-art word representations on three benchmark datasets for proportional analogy detection, demonstrating its ability to accurately capture the semantic relations among words.

===================================
paper_id: 12730203; YEAR: 2014
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidfcbow200 - abs_tfidf - title_tfidfcbow200
TITLE: Linguistic Regularities in Sparse and Explicit Word Representations
ABSTRACT: background_label: Recent work has shown that neuralembedded word representations capture many relational similarities, which can be recovered by means of vector arithmetic in the embedded space.
method_label: We show that Mikolov et al.’s method of first adding and subtracting word vectors, and then searching for a word similar to the result, is equivalent to searching for a word that maximizes a linear combination of three pairwise word similarities.
method_label: Based on this observation, we suggest an improved method of recovering relational similarities, improving the state-of-the-art results on two recent word-analogy datasets.
result_label: Moreover, we demonstrate that analogy recovery is not restricted to neural word embeddings, and that a similar amount of relational similarities can be recovered from traditional distributional word representations.

===================================
paper_id: 52301591; YEAR: 2018
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: abs_cbow200 - abs_tfidf - abs_tfidfcbow200
TITLE: FRAGE: Frequency-Agnostic Word Representation
ABSTRACT: background_label: Continuous word representation (aka word embedding) is a basic building block in many neural network-based models used in natural language processing tasks.
background_label: Although it is widely accepted that words with similar semantics should be close to each other in the embedding space, we find that word embeddings learned in several tasks are biased towards word frequency: the embeddings of high-frequency and low-frequency words lie in different subregions of the embedding space, and the embedding of a rare word and a popular word can be far from each other even if they are semantically similar.
background_label: This makes learned word embeddings ineffective, especially for rare words, and consequently limits the performance of these neural network models.
objective_label: In this paper, we develop a neat, simple yet effective way to learn \emph{FRequency-AGnostic word Embedding} (FRAGE) using adversarial training.
method_label: We conducted comprehensive studies on ten datasets across four natural language processing tasks, including word similarity, language modeling, machine translation and text classification.
result_label: Results show that with FRAGE, we achieve higher performance than the baselines in all tasks.

===================================
paper_id: 162169044; YEAR: 2018
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidf - specter - abs_tfidfcbow200
TITLE: Incorporating Syntactic and Semantic Information in Word Embeddings using Graph Convolutional Networks
ABSTRACT: background_label: Word embeddings have been widely adopted across several NLP applications.
background_label: Most existing word embedding methods utilize sequential context of a word to learn its embedding.
background_label: While there have been some attempts at utilizing syntactic context of a word, such methods result in an explosion of the vocabulary size.
method_label: In this paper, we overcome this problem by proposing SynGCN, a flexible Graph Convolution based method for learning word embeddings.
method_label: SynGCN utilizes the dependency context of a word without increasing the vocabulary size.
method_label: Word embeddings learned by SynGCN outperform existing methods on various intrinsic and extrinsic tasks and provide an advantage when used with ELMo.
method_label: We also propose SemGCN, an effective framework for incorporating diverse semantic knowledge for further enhancing learned word representations.
result_label: We make the source code of both models available to encourage reproducible research.

===================================
paper_id: 8796808; YEAR: 2016
adju relevance: Related (+1)
difference: 1; annotator4: 0; annotator3: 1
sources: abs_cbow200
TITLE: Morphological Smoothing and Extrapolation of Word Embeddings
ABSTRACT: background_label: Languages with rich inflectional morphology exhibit lexical data sparsity, since the word used to express a given concept will vary with the syntactic context.
background_label: For instance, each count noun in Czech has 12 forms (where English uses only singular and plural).
background_label: Even in large corpora, we are unlikely to observe all inflections of a given lemma.
background_label: This reduces the vocabulary coverage of methods that induce continuous representations for words from distributional corpus information.
method_label: We solve this problem by exploiting existing morphological resources that can enumerate a word’s component morphemes.
method_label: We present a latentvariable Gaussian graphical model that allows us to extrapolate continuous representations for words not observed in the training corpus, as well as smoothing the representations provided for the observed words.
method_label: The latent variables represent embeddings of morphemes, which combine to create embeddings of words.
result_label: Over several languages and training sizes, our model improves the embeddings for words, when evaluated on an analogy task, skip-gram predictive accuracy, and word similarity.

===================================
paper_id: 28730125; YEAR: 2017
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: abs_tfidfcbow200
TITLE: Word Embeddings based on Fixed-Size Ordinally Forgetting Encoding
ABSTRACT: objective_label: AbstractIn this paper, we propose to learn word embeddings based on the recent fixedsize ordinally forgetting encoding (FOFE) method, which can almost uniquely encode any variable-length sequence into a fixed-size representation.
method_label: We use FOFE to fully encode the left and right context of each word in a corpus to construct a novel word-context matrix, which is further weighted and factorized using truncated SVD to generate low-dimension word embedding vectors.
method_label: We have evaluated this alternative method in encoding word-context statistics and show the new FOFE method has a notable effect on the resulting word embeddings.
result_label: Experimental results on several popular word similarity tasks have demonstrated that the proposed method outperforms many recently popular neural prediction methods as well as the conventional SVD models that use canonical count based techniques to generate word context matrices.

===================================
paper_id: 52000553; YEAR: 2018
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_cbow200 - title_tfidfcbow200 - title_cbow200
TITLE: Learning Word Meta-Embeddings by Autoencoding
ABSTRACT: background_label: AbstractDistributed word embeddings have shown superior performances in numerous Natural Language Processing (NLP) tasks.
background_label: However, their performances vary significantly across different tasks, implying that the word embeddings learnt by those methods capture complementary aspects of lexical semantics.
background_label: Therefore, we believe that it is important to combine the existing word embeddings to produce more accurate and complete meta-embeddings of words.
method_label: We model the meta-embedding learning problem as an autoencoding problem, where we would like to learn a meta-embedding space that can accurately reconstruct all source embeddings simultaneously.
method_label: Thereby, the meta-embedding space is enforced to capture complementary information in different source embeddings via a coherent common embedding space.
method_label: We propose three flavours of autoencoded meta-embeddings motivated by different requirements that must be satisfied by a meta-embedding.
result_label: Our experimental results on a series of benchmark evaluations show that the proposed autoencoded meta-embeddings outperform the existing state-of-the-art metaembeddings in multiple tasks.

===================================
paper_id: 5959482; YEAR: 2013
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: cited - title_tfidf - abs_tfidf - title_tfidfcbow200 - title_cbow200 - abs_cbow200 - specter - abs_tfidfcbow200
TITLE: Efficient Estimation of Word Representations in Vector Space
ABSTRACT: background_label: We propose two novel model architectures for computing continuous vector representations of words from very large data sets.
method_label: The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks.
background_label: We observe large improvements in accuracy at much lower computational cost, i.e.
method_label: it takes less than a day to learn high quality word vectors from a 1.6 billion words data set.
result_label: Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.

===================================
paper_id: 44134877; YEAR: 2018
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: abs_tfidfcbow200 - specter
TITLE: Querying Word Embeddings for Similarity and Relatedness
ABSTRACT: background_label: AbstractWord embeddings obtained from neural network models such as Word2Vec Skipgram have become popular representations of word meaning and have been evaluated on a variety of word similarity and relatedness norming data.
background_label: Skipgram generates a set of word and context embeddings, the latter typically discarded after training.
method_label: We demonstrate the usefulness of context embeddings in predicting asymmetric association between words from a recently published dataset of production norms (Jouravlev and McRae, 2016) .
result_label: Our findings suggest that humans respond with words closer to the cue within the context embedding space (rather than the word embedding space), when asked to generate thematically related words.

===================================
paper_id: 16863934; YEAR: 2015
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 2
sources: title_cbow200 - abs_tfidf - title_tfidfcbow200 - specter - abs_tfidfcbow200
TITLE: SensEmbed: Learning Sense Embeddings for Word and Relational Similarity
ABSTRACT: background_label: Word embeddings have recently gained considerable popularity for modeling words in different Natural Language Processing (NLP) tasks including semantic similarity measurement.
background_label: However, notwithstanding their success, word embeddings are by their very nature unable to capture polysemy, as different meanings of a word are conflated into a single representation.
background_label: In addition, their learning process usually relies on massive corpora only, preventing them from taking advantage of structured knowledge.
method_label: We address both issues by proposing a multifaceted approach that transforms word embeddings to the sense level and leverages knowledge from a large semantic network for effective semantic similarity measurement.
result_label: We evaluate our approach on word similarity and relational similarity frameworks, reporting state-of-the-art performance on multiple datasets.

===================================
paper_id: 14736559; YEAR: 2014
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: specter - title_tfidf - abs_tfidf - abs_cbow200 - abs_tfidfcbow200
TITLE: Rehabilitation of Count-based Models for Word Vector Representations
ABSTRACT: background_label: Recent works on word representations mostly rely on predictive models.
background_label: Distributed word representations (aka word embeddings) are trained to optimally predict the contexts in which the corresponding words tend to appear.
background_label: Such models have succeeded in capturing word similarties as well as semantic and syntactic regularities.
objective_label: Instead, we aim at reviving interest in a model based on counts.
method_label: We present a systematic study of the use of the Hellinger distance to extract semantic representations from the word co-occurence statistics of large text corpora.
method_label: We show that this distance gives good performance on word similarity and analogy tasks, with a proper type and size of context, and a dimensionality reduction based on a stochastic low-rank approximation.
method_label: Besides being both simple and intuitive, this method also provides an encoding function which can be used to infer unseen words or phrases.
result_label: This becomes a clear advantage compared to predictive models which must train these new words.

===================================
paper_id: 1957433; YEAR: 2014
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: cited - title_tfidf - abs_tfidf - title_tfidfcbow200 - title_cbow200 - abs_cbow200 - specter - abs_tfidfcbow200
TITLE: Glove: Global Vectors for Word Representation
ABSTRACT: background_label: Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque.
method_label: We analyze and make explicit the model properties needed for such regularities to emerge in word vectors.
method_label: The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods.
method_label: Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus.
method_label: The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task.
result_label: It also outperforms related models on similarity tasks and named entity recognition.

===================================
paper_id: 5244724; YEAR: 2014
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: abs_tfidf - title_tfidf - title_tfidfcbow200 - title_cbow200 - abs_tfidfcbow200
TITLE: Learning Effective Word Embedding using Morphological Word Similarity
ABSTRACT: other_label: Abstract.
background_label: Deep learning techniques aim at obtaining high-quality distributed representations of words, i.e., word embeddings, to address text mining and natural language processing tasks.
background_label: Recently, efficient methods have been proposed to learn word embeddings from context that captures both semantic and syntactic relationships between words.
background_label: However, it is challenging to handle unseen words or rare words with insufficient context.
objective_label: In this paper, inspired by the study on word recognition process in cognitive psychology, we propose to take advantage of seemingly less obvious but essentially important morphological word similarity to address these challenges.
method_label: In particular, we introduce a novel neural network architecture that leverages both contextual information and morphological word similarity to learn word embeddings.
method_label: Meanwhile, the learning architecture is also able to refine the pre-defined morphological knowledge and obtain more accurate word similarity.
result_label: Experiments on an analogical reasoning task and a word similarity task both demonstrate that the proposed method can greatly enhance the effectiveness of word embeddings.

===================================
paper_id: 14478375; YEAR: 2011
adju relevance: Related (+1)
difference: 1; annotator4: 2; annotator3: 1
sources: cited - title_tfidf - abs_tfidf - title_tfidfcbow200 - title_cbow200 - abs_cbow200 - specter - abs_tfidfcbow200
TITLE: Nonparametric Bayesian Word Sense Induction
ABSTRACT: objective_label: AbstractWe propose the use of a nonparametric Bayesian model, the Hierarchical Dirichlet Process (HDP), for the task of Word Sense Induction.
method_label: Results are shown through comparison against Latent Dirichlet Allocation (LDA), a parametric Bayesian model employed by Brody and Lapata (2009) for this task.
method_label: We find that the two models achieve similar levels of induction quality, while the HDP confers the advantage of automatically inducing a variable number of senses per word, as compared to manually fixing the number of senses a priori, as in LDA.
method_label: This flexibility allows for the model to adapt to terms with greater or lesser polysemy, when evidenced by corpus distributional statistics.
result_label: When trained on out-of-domain data, experimental results confirm the model's ability to make use of a restricted set of topically coherent induced senses, when then applied in a restricted domain.

===================================
paper_id: 20830942; YEAR: 2017
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: specter - abs_tfidf - abs_tfidfcbow200
TITLE: A Mixture Model for Learning Multi-Sense Word Embeddings
ABSTRACT: background_label: Word embeddings are now a standard technique for inducing meaning representations for words.
background_label: For getting good representations, it is important to take into account different senses of a word.
objective_label: In this paper, we propose a mixture model for learning multi-sense word embeddings.
method_label: Our model generalizes the previous works in that it allows to induce different weights of different senses of a word.
result_label: The experimental results show that our model outperforms previous models on standard evaluation tasks.

===================================
paper_id: 12643315; YEAR: 2014
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidfcbow200 - abs_tfidf
TITLE: KNET: A General Framework for Learning Word Embedding using Morphological Knowledge
ABSTRACT: background_label: Neural network techniques are widely applied to obtain high-quality distributed representations of words, i.e., word embeddings, to address text mining, information retrieval, and natural language processing tasks.
background_label: Recently, efficient methods have been proposed to learn word embeddings from context that captures both semantic and syntactic relationships between words.
background_label: However, it is challenging to handle unseen words or rare words with insufficient context.
objective_label: In this paper, inspired by the study on word recognition process in cognitive psychology, we propose to take advantage of seemingly less obvious but essentially important morphological knowledge to address these challenges.
method_label: In particular, we introduce a novel neural network architecture called KNET that leverages both contextual information and morphological word similarity built based on morphological knowledge to learn word embeddings.
method_label: Meanwhile, the learning architecture is also able to refine the pre-defined morphological knowledge and obtain more accurate word similarity.
result_label: Experiments on an analogical reasoning task and a word similarity task both demonstrate that the proposed KNET framework can greatly enhance the effectiveness of word embeddings.

===================================
paper_id: 15881253; YEAR: 2016
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: specter - abs_tfidf - abs_cbow200 - abs_tfidfcbow200
TITLE: Enriching Word Vectors with Subword Information
ABSTRACT: background_label: Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks.
background_label: Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word.
background_label: This is a limitation, especially for languages with large vocabularies and many rare words.
objective_label: In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character $n$-grams.
method_label: A vector representation is associated to each character $n$-gram; words being represented as the sum of these representations.
method_label: Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data.
result_label: We evaluate our word representations on nine different languages, both on word similarity and analogy tasks.
result_label: By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.

===================================
paper_id: 9471817; YEAR: 2017
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_cbow200 - abs_tfidf - abs_tfidfcbow200
TITLE: Improved Word Representation Learning with Sememes
ABSTRACT: background_label: AbstractSememes are minimum semantic units of word meanings, and the meaning of each word sense is typically composed by several sememes.
background_label: Since sememes are not explicit for each word, people manually annotate word sememes and form linguistic common-sense knowledge bases.
objective_label: In this paper, we present that, word sememe information can improve word representation learning (WRL), which maps words into a low-dimensional semantic space and serves as a fundamental step for many NLP tasks.
objective_label: The key idea is to utilize word sememes to capture exact meanings of a word within specific contexts accurately.
method_label: More specifically, we follow the framework of Skip-gram and present three sememe-encoded models to learn representations of sememes, senses and words, where we apply the attention scheme to detect word senses in various contexts.
method_label: We conduct experiments on two tasks including word similarity and word analogy, and our models significantly outperform baselines.
result_label: The results indicate that WRL can benefit from sememes via the attention scheme, and also confirm our models being capable of correctly modeling sememe information.

===================================
paper_id: 2570211; YEAR: 2015
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: abs_cbow200 - title_cbow200
TITLE: Learning Word Representations by Jointly Modeling Syntagmatic and Paradigmatic Relations
ABSTRACT: background_label: Vector space representation of words has been widely used to capture fine-grained linguistic regularities, and proven to be successful in various natural language processing tasks in recent years.
background_label: However, existing models for learning word representations focus on either syntagmatic or paradigmatic relations alone.
objective_label: In this paper, we argue that it is beneficial to jointly modeling both relations so that we can not only encode different types of linguistic properties in a unified way, but also boost the representation learning due to the mutual enhancement between these two types of relations.
method_label: We propose two novel distributional models for word representation using both syntagmatic and paradigmatic relations via a joint training objective.
method_label: The proposed models are trained on a public Wikipedia corpus, and the learned representations are evaluated on word analogy and word similarity tasks.
result_label: The results demonstrate that the proposed models can perform significantly better than all the state-of-the-art baseline methods on both tasks.

===================================
paper_id: 8506049; YEAR: 2015
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: WordRank: Learning Word Embeddings via Robust Ranking
ABSTRACT: background_label: Embedding words in a vector space has gained a lot of attention in recent years.
background_label: While state-of-the-art methods provide efficient computation of word similarities via a low-dimensional matrix embedding, their motivation is often left unclear.
objective_label: In this paper, we argue that word embedding can be naturally viewed as a ranking problem due to the ranking nature of the evaluation metrics.
method_label: Then, based on this insight, we propose a novel framework WordRank that efficiently estimates word representations via robust ranking, in which the attention mechanism and robustness to noise are readily achieved via the DCG-like ranking losses.
method_label: The performance of WordRank is measured in word similarity and word analogy benchmarks, and the results are compared to the state-of-the-art word embedding techniques.
method_label: Our algorithm is very competitive to the state-of-the- arts on large corpora, while outperforms them by a significant margin when the training set is limited (i.e., sparse and noisy).
result_label: With 17 million tokens, WordRank performs almost as well as existing methods using 7.2 billion tokens on a popular word similarity benchmark.
result_label: Our multi-node distributed implementation of WordRank is publicly available for general usage.

===================================
paper_id: 53080886; YEAR: 2018
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Word Relation Autoencoder for Unseen Hypernym Extraction Using Word Embeddings
ABSTRACT: background_label: AbstractLexicon relation extraction given distributional representation of words is an important topic in NLP.
background_label: We observe that the state-of-theart projection-based methods cannot be generalized to handle unseen hypernyms.
objective_label: We propose to analyze it in the perspective of pollution, that is, the predicted hypernyms are limited to those appeared in training set.
method_label: We propose a word relation autoencoder (WRAE) model to address the challenge and construct the corresponding indicator to measure the pollution.
result_label: Experiments on several hypernymlike lexicon datasets show that our model outperforms the competitors significantly.

===================================
paper_id: 20269438; YEAR: 2017
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: abs_tfidfcbow200 - abs_tfidf - specter
TITLE: Multimodal Word Distributions
ABSTRACT: background_label: Word embeddings provide point representations of words containing useful semantic information.
method_label: We introduce multimodal word distributions formed from Gaussian mixtures, for multiple word meanings, entailment, and rich uncertainty information.
method_label: To learn these distributions, we propose an energy-based max-margin objective.
result_label: We show that the resulting approach captures uniquely expressive semantic information, and outperforms alternatives, such as word2vec skip-grams, and Gaussian embeddings, on benchmark datasets such as word similarity and entailment.

===================================
paper_id: 13468104; YEAR: 2014
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: title_tfidf - specter
TITLE: Word Representations via Gaussian Embedding
ABSTRACT: background_label: Current work in lexical distributed representations maps each word to a point vector in low-dimensional space.
background_label: Mapping instead to a density provides many interesting advantages, including better capturing uncertainty about a representation and its relationships, expressing asymmetries more naturally than dot product or cosine similarity, and enabling more expressive parameterization of decision boundaries.
method_label: This paper advocates for density-based distributed embeddings and presents a method for learning representations in the space of Gaussian distributions.
result_label: We compare performance on various word embedding benchmarks, investigate the ability of these embeddings to model entailment and other asymmetric relationships, and explore novel properties of the representation.

===================================
paper_id: 6828753; YEAR: 2015
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: specter - abs_tfidf - title_tfidfcbow200 - abs_cbow200 - abs_tfidfcbow200
TITLE: Semantic Information Extraction for Improved Word Embeddings
ABSTRACT: background_label: Word embeddings have recently proven useful in a number of different applications that deal with natural language.
background_label: Such embeddings succinctly reflect semantic similarities between words based on their sentence-internal contexts in large corpora.
objective_label: In this paper, we show that information extraction techniques provide valuable additional evidence of semantic relationships that can be exploited when producing word embeddings.
method_label: We propose a joint model to train word embeddings both on regular context information and on more explicit semantic extractions.
result_label: The word vectors obtained from such an augmented joint training show improved results on word similarity tasks, suggesting that they can be useful in applications that involve word meanings.

===================================
paper_id: 14219425; YEAR: 2014
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: title_cbow200 - title_tfidf - abs_tfidf - title_tfidfcbow200 - abs_cbow200 - abs_tfidfcbow200
TITLE: Co-learning of Word Representations and Morpheme Representations
ABSTRACT: background_label: AbstractThe techniques of using neural networks to learn distributed word representations (i.e., word embeddings) have been used to solve a variety of natural language processing tasks.
background_label: The recently proposed methods, such as CBOW and Skip-gram, have demonstrated their effectiveness in learning word embeddings based on context information such that the obtained word embeddings can capture both semantic and syntactic relationships between words.
background_label: However, it is quite challenging to produce high-quality word representations for rare or unknown words due to their insufficient context information.
objective_label: In this paper, we propose to leverage morphological knowledge to address this problem.
method_label: Particularly, we introduce the morphological knowledge as both additional input representation and auxiliary supervision to the neural network framework.
method_label: As a result, beyond word representations, the proposed neural network model will produce morpheme representations, which can be further employed to infer the representations of rare or unknown words based on their morphological structure.
result_label: Experiments on an analogical reasoning task and several word similarity tasks have demonstrated the effectiveness of our method in producing high-quality words embeddings compared with the state-of-the-art methods.

===================================
paper_id: 14141143; YEAR: 2014
adju relevance: Related (+1)
difference: 1; annotator4: 2; annotator3: 1
sources: cited - title_tfidf - abs_tfidf - title_tfidfcbow200 - title_cbow200 - abs_cbow200 - specter - abs_tfidfcbow200
TITLE: What Substitutes Tell Us - Analysis of an "All-Words" Lexical Substitution Corpus
ABSTRACT: background_label: We present the first large-scale English “allwords lexical substitution” corpus.
background_label: The size of the corpus provides a rich resource for investigations into word meaning.
method_label: We investigate the nature of lexical substitute sets, comparing them to WordNet synsets.
method_label: We find them to be consistent with, but more fine-grained than, synsets.
result_label: We also identify significant differences to results for paraphrase ranking in context reported for the SEMEVAL lexical substitution data.
result_label: This highlights the influence of corpus construction approaches on evaluation results.

===================================
paper_id: 2034481; YEAR: 2016
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidf - title_tfidfcbow200 - title_cbow200 - specter - abs_tfidfcbow200
TITLE: Sparse Bilingual Word Representations for Cross-lingual Lexical Entailment
ABSTRACT: objective_label: We introduce the task of cross-lingual lexical entailment, which aims to detect whether the meaning of a word in one language can be inferred from the meaning of a word in another language.
method_label: We construct a gold standard for this task, and propose an unsupervised solution based on distributional word representations.
method_label: As commonly done in the monolingual setting, we assume a worde entails a wordf if the prominent context features of e are a subset of those of f .
method_label: To address the challenge of comparing contexts across languages, we propose a novel method for inducing sparse bilingual word representations from monolingual and parallel texts.
result_label: Our approach yields an Fscore of 70%, and significantly outperforms strong baselines based on translation and on existing word representations.

===================================
paper_id: 12909464; YEAR: 2015
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: abs_tfidf
TITLE: Breaking Sticks and Ambiguities with Adaptive Skip-gram
ABSTRACT: background_label: Recently proposed Skip-gram model is a powerful method for learning high-dimensional word representations that capture rich semantic relationships between words.
background_label: However, Skip-gram as well as most prior work on learning word representations does not take into account word ambiguity and maintain only single representation per word.
background_label: Although a number of Skip-gram modifications were proposed to overcome this limitation and learn multi-prototype word representations, they either require a known number of word meanings or learn them using greedy heuristic approaches.
method_label: In this paper we propose the Adaptive Skip-gram model which is a nonparametric Bayesian extension of Skip-gram capable to automatically learn the required number of representations for all words at desired semantic resolution.
result_label: We derive efficient online variational learning algorithm for the model and empirically demonstrate its efficiency on word-sense induction task.

===================================
paper_id: 372093; YEAR: 2012
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: cited - title_tfidf - abs_tfidf - title_tfidfcbow200 - title_cbow200 - abs_cbow200 - specter - abs_tfidfcbow200
TITLE: Improving Word Representations via Global Context and Multiple Word Prototypes
ABSTRACT: background_label: AbstractUnsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems.
background_label: However, most of these models are built with only local context and one representation per word.
background_label: This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings.
method_label: We present a new neural network architecture which 1) learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning multiple embeddings per word.
result_label: We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outperforms competitive baselines and other neural language models.

===================================
paper_id: 4078637; YEAR: 2018
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: title_tfidfcbow200
TITLE: Word sense induction using word embeddings and community detection in complex networks
ABSTRACT: background_label: Word Sense Induction (WSI) is the ability to automatically induce word senses from corpora.
background_label: The WSI task was first proposed to overcome the limitations of manually annotated corpus that are required in word sense disambiguation systems.
background_label: Even though several works have been proposed to induce word senses, existing systems are still very limited in the sense that they make use of structured, domain-specific knowledge sources.
method_label: In this paper, we devise a method that leverages recent findings in word embeddings research to generate context embeddings, which are embeddings containing information about the semantical context of a word.
method_label: In order to induce senses, we modeled the set of ambiguous words as a complex network.
method_label: In the generated network, two instances (nodes) are connected if the respective context embeddings are similar.
result_label: Upon using well-established community detection methods to cluster the obtained context embeddings, we found that the proposed method yields excellent performance for the WSI task.
result_label: Our method outperformed competing algorithms and baselines, in a completely unsupervised manner and without the need of any additional structured knowledge source.

===================================
paper_id: 4992154; YEAR: 2014
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: specter - title_tfidf - abs_tfidf - abs_tfidfcbow200
TITLE: Learning Word Representations with Hierarchical Sparse Coding
ABSTRACT: objective_label: We propose a new method for learning word representations using hierarchical regularization in sparse coding inspired by the linguistic study of word meanings.
method_label: We show an efficient learning algorithm based on stochastic proximal methods that is significantly faster than previous approaches, making it possible to perform hierarchical sparse coding on a corpus of billions of word tokens.
result_label: Experiments on various benchmark tasks---word similarity ranking, analogies, sentence completion, and sentiment analysis---demonstrate that the method outperforms or is competitive with state-of-the-art methods.
other_label: Our word representations are available at \url{http://www.ark.cs.cmu.edu/dyogatam/wordvecs/}.

===================================
paper_id: 3430183; YEAR: 2007
adju relevance: Related (+1)
difference: 1; annotator4: 0; annotator3: 1
sources: specter - title_tfidf - title_tfidfcbow200
TITLE: A Topic Model for Word Sense Disambiguation
ABSTRACT: background_label: AbstractWe develop latent Dirichlet allocation with WORDNET (LDAWN), an unsupervised probabilistic topic model that includes word sense as a hidden variable.
method_label: We develop a probabilistic posterior inference algorithm for simultaneously disambiguating a corpus and learning the domains in which to consider each word.
result_label: Using the WORDNET hierarchy, we embed the construction of Abney and Light (1999) in the topic model and show that automatically learned domains improve WSD accuracy compared to alternative contexts.

===================================
paper_id: 52289977; YEAR: 2018
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: abs_tfidfcbow200 - abs_tfidf
TITLE: Semi-Supervised Multi-Task Word Embeddings
ABSTRACT: background_label: Word embeddings have been shown to benefit from ensembling several word embedding sources, often carried out using straightforward mathematical operations over the set of vectors to produce a meta-embedding representation.
background_label: More recently, unsupervised learning has been used to find a lower-dimensional representation, similar in size to that of the word embeddings within the ensemble.
background_label: However, these methods do not use the available manual labeled datasets that are often used solely for the purpose of evaluation.
objective_label: We propose to improve word embeddings by simultaneously learning to reconstruct an ensemble of pretrained word embeddings with supervision from various labeled word similarity datasets.
method_label: This involves reconstructing word meta-embeddings while simultaneously using a Siamese Network to also learn word similarity where both processes share a hidden layer.
method_label: Experiments are carried out on 6 word similarity datasets and 3 analogy datasets.
result_label: We find that performance is improved for all word similarity datasets when compared to unsupervised learning methods with a mean increase of 11.33 in the Spearman Correlation coefficient.
result_label: Moreover, 4 of 6 of word similarity datasets from our approach show best performance when using of a cosine loss for reconstruction and Brier's loss for word similarity.

===================================
paper_id: 52198001; YEAR: 2018
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidf - abs_tfidfcbow200
TITLE: Graph Convolutional Networks based Word Embeddings
ABSTRACT: background_label: AbstractRecently, word embeddings have been widely adopted across several NLP applications.
background_label: However, most word embedding methods solely rely on linear context and do not provide a framework for incorporating word relationships like hypernym, nmod in a principled manner.
objective_label: In this paper, we propose WordGCN, a Graph Convolution based word representation learning approach which provides a framework for exploiting multiple types of word relationships.
method_label: WordGCN operates at sentence as well as corpus level and allows to incorporate dependency parse based context in an efficient manner without increasing the vocabulary size.
method_label: To the best of our knowledge, this is the first approach which effectively incorporates word relationships via Graph Convolutional Networks for learning word representations.
result_label: Through extensive experiments on various intrinsic and extrinsic tasks, we demonstrate WordGCN's effectiveness over existing word embedding approaches.
result_label: We make WordGCN's source code available to encourage reproducible research.

===================================
paper_id: 47012788; YEAR: 2018
adju relevance: Related (+1)
difference: 1; annotator4: 0; annotator3: 1
sources: abs_cbow200 - abs_tfidf - specter - abs_tfidfcbow200
TITLE: Probabilistic FastText for Multi-Sense Word Embeddings
ABSTRACT: background_label: We introduce Probabilistic FastText, a new model for word embeddings that can capture multiple word senses, sub-word structure, and uncertainty information.
method_label: In particular, we represent each word with a Gaussian mixture density, where the mean of a mixture component is given by the sum of n-grams.
background_label: This representation allows the model to share statistical strength across sub-word structures (e.g.
method_label: Latin roots), producing accurate representations of rare, misspelt, or even unseen words.
method_label: Moreover, each component of the mixture can capture a different word sense.
result_label: Probabilistic FastText outperforms both FastText, which has no probabilistic model, and dictionary-level probabilistic embeddings, which do not incorporate subword structures, on several word-similarity benchmarks, including English RareWord and foreign language datasets.
result_label: We also achieve state-of-art performance on benchmarks that measure ability to discern different meanings.
result_label: Thus, the proposed model is the first to achieve multi-sense representations while having enriched semantics on rare words.

===================================
paper_id: 18079144; YEAR: 2015
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidf - abs_cbow200 - abs_tfidfcbow200
TITLE: Evaluating distributed word representations for capturing semantics of biomedical concepts
ABSTRACT: background_label: AbstractRecently there is a surge in interest in learning vector representations of words using huge corpus in unsupervised manner.
background_label: Such word vector representations, also known as word embedding, have been shown to improve the performance of machine learning models in several NLP tasks.
background_label: However efficiency of such representation has not been systematically evaluated in biomedical domain.
objective_label: In this work our aim is to compare the performance of two state-of-the-art word embedding methods, namely word2vec and GloVe on a basic task of reflecting semantic similarity and relatedness of biomedical concepts.
method_label: For this, vector representations of all unique words in the corpus of more than 1 million full-length research articles in biomedical domain are obtained from the two methods.
other_label: These word vectors are evaluated for their ability to reflect semantic similarity and semantic relatedness of word-pairs in a benchmark data set of manually curated semantic similar and related words available at http:// rxinformatics.umn.edu.
result_label: We observe that parameters of these models do affect their ability to capture lexicosemantic properties and word2vec with particular language modeling seems to perform better than others.

===================================
paper_id: 753195; YEAR: 2014
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: specter - title_tfidf - abs_tfidf - abs_tfidfcbow200
TITLE: A Simple and Efficient Method To Generate Word Sense Representations
ABSTRACT: background_label: Distributed representations of words have boosted the performance of many Natural Language Processing tasks.
background_label: However, usually only one representation per word is obtained, not acknowledging the fact that some words have multiple meanings.
background_label: This has a negative effect on the individual word representations and the language model as a whole.
objective_label: In this paper we present a simple model that enables recent techniques for building word vectors to represent distinct senses of polysemic words.
result_label: In our assessment of this model we show that it is able to effectively discriminate between words' senses and to do so in a computationally efficient manner.

===================================
paper_id: 3792324; YEAR: 2014
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: specter - title_tfidf
TITLE: Improving Vector Space Word Representations Using Multilingual Correlation
ABSTRACT: background_label: The distributional hypothesis of Harris (1954), according to which the meaning of words is evidenced by the contexts they occur in, has motivated several effective techniques for obtaining vector space semantic representations of words using unannotated text corpora.
objective_label: This paper argues that lexico-semantic content should additionally be invariant across languages and proposes a simple technique based on canonical correlation analysis (CCA) for incorporating multilingual evidence into vectors generated monolingually.
result_label: We evaluate the resulting word representations on standard lexical semantic evaluation tasks and show that our method produces substantially better semantic representations than monolingual techniques.

===================================
paper_id: 3076894; YEAR: 2015
adju relevance: Related (+1)
difference: 1; annotator4: 0; annotator3: 1
sources: abs_tfidf - title_tfidfcbow200 - title_cbow200 - abs_cbow200 - specter - abs_tfidfcbow200
TITLE: Joint Word Representation Learning using a Corpus and a Semantic Lexicon
ABSTRACT: background_label: Methods for learning word representations using large text corpora have received much attention lately due to their impressive performance in numerous natural language processing (NLP) tasks such as, semantic similarity measurement, and word analogy detection.
background_label: Despite their success, these data-driven word representation learning methods do not consider the rich semantic relational structure between words in a co-occurring context.
background_label: On the other hand, already much manual effort has gone into the construction of semantic lexicons such as the WordNet that represent the meanings of words by defining the various relationships that exist among the words in a language.
method_label: We consider the question, can we improve the word representations learnt using a corpora by integrating the knowledge from semantic lexicons?.
method_label: For this purpose, we propose a joint word representation learning method that simultaneously predicts the co-occurrences of two words in a sentence subject to the relational constrains given by the semantic lexicon.
method_label: We use relations that exist between words in the lexicon to regularize the word representations learnt from the corpus.
method_label: Our proposed method statistically significantly outperforms previously proposed methods for incorporating semantic lexicons into word representations on several benchmark datasets for semantic similarity and word analogy.

===================================
paper_id: 17770881; YEAR: 2015
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidf - abs_cbow200 - specter - abs_tfidfcbow200
TITLE: "The Sum of Its Parts": Joint Learning of Word and Phrase Representations with Autoencoders
ABSTRACT: background_label: Recently, there has been a lot of effort to represent words in continuous vector spaces.
background_label: Those representations have been shown to capture both semantic and syntactic information about words.
background_label: However, distributed representations of phrases remain a challenge.
method_label: We introduce a novel model that jointly learns word vector representations and their summation.
method_label: Word representations are learnt using the word co-occurrence statistical information.
background_label: To embed sequences of words (i.e.
objective_label: phrases) with different sizes into a common semantic space, we propose to average word vector representations.
method_label: In contrast with previous methods which reported a posteriori some compositionality aspects by simple summation, we simultaneously train words to sum, while keeping the maximum information from the original vectors.
method_label: We evaluate the quality of the word representations on several classical word evaluation tasks, and we introduce a novel task to evaluate the quality of the phrase representations.
result_label: While our distributed representations compete with other methods of learning word representations on word evaluations, we show that they give better performance on the phrase evaluation.
result_label: Such representations of phrases could be interesting for many tasks in natural language processing.

===================================
paper_id: 3334696; YEAR: 2018
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidfcbow200 - abs_tfidf - abs_cbow200
TITLE: Can Network Embedding of Distributional Thesaurus be Combined with Word Vectors for Better Representation?
ABSTRACT: background_label: Distributed representations of words learned from text have proved to be successful in various natural language processing tasks in recent times.
background_label: While some methods represent words as vectors computed from text using predictive model (Word2vec) or dense count based model (GloVe), others attempt to represent these in a distributional thesaurus network structure where the neighborhood of a word is a set of words having adequate context overlap.
method_label: Being motivated by recent surge of research in network embedding techniques (DeepWalk, LINE, node2vec etc.
method_label: ), we turn a distributional thesaurus network into dense word vectors and investigate the usefulness of distributional thesaurus embedding in improving overall word representation.
method_label: This is the first attempt where we show that combining the proposed word representation obtained by distributional thesaurus embedding with the state-of-the-art word representations helps in improving the performance by a significant margin when evaluated against NLP tasks like word similarity and relatedness, synonym detection, analogy detection.
result_label: Additionally, we show that even without using any handcrafted lexical resources we can come up with representations having comparable performance in the word similarity and relatedness tasks compared to the representations where a lexical resource has been used.

===================================
paper_id: 9098191; YEAR: 2015
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: specter - abs_tfidf - abs_cbow200 - abs_tfidfcbow200
TITLE: Category Enhanced Word Embedding
ABSTRACT: background_label: Distributed word representations have been demonstrated to be effective in capturing semantic and syntactic regularities.
background_label: Unsupervised representation learning from large unlabeled corpora can learn similar representations for those words that present similar co-occurrence statistics.
background_label: Besides local occurrence statistics, global topical information is also important knowledge that may help discriminate a word from another.
method_label: In this paper, we incorporate category information of documents in the learning of word representations and to learn the proposed models in a document-wise manner.
method_label: Our models outperform several state-of-the-art models in word analogy and word similarity tasks.
result_label: Moreover, we evaluate the learned word vectors on sentiment analysis and text classification tasks, which shows the superiority of our learned word vectors.
result_label: We also learn high-quality category embeddings that reflect topical meanings.

===================================
paper_id: 7567901; YEAR: 2015
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: abs_tfidfcbow200 - specter
TITLE: Improving Distributed Representation of Word Sense via WordNet Gloss Composition and Context Clustering
ABSTRACT: background_label: In recent years, there has been an increas-ing interest in learning a distributed rep-resentation of word sense.
background_label: Traditional context clustering based models usually require careful tuning of model parame-ters, and typically perform worse on infre-quent word senses.
method_label: This paper presents a novel approach which addresses these lim-itations by first initializing the word sense embeddings through learning sentence-level embeddings from WordNet glosses using a convolutional neural networks.
method_label: The initialized word sense embeddings are used by a context clustering based model to generate the distributed representations of word senses.
result_label: Our learned represen-tations outperform the publicly available embeddings on 2 out of 4 metrics in the word similarity task, and 6 out of 13 sub tasks in the analogical reasoning task.

===================================
paper_id: 9199643; YEAR: 2015
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: specter - title_tfidf
TITLE: Non-distributional Word Vector Representations
ABSTRACT: background_label: Data-driven representation learning for words is a technique of central importance in NLP.
background_label: While indisputably useful as a source of features in downstream tasks, such vectors tend to consist of uninterpretable components whose relationship to the categories of traditional lexical semantic theories is tenuous at best.
method_label: We present a method for constructing interpretable word vectors from hand-crafted linguistic resources like WordNet, FrameNet etc.
method_label: These vectors are binary (i.e, contain only 0 and 1) and are 99.9% sparse.
result_label: We analyze their performance on state-of-the-art evaluation methods for distributional models of word vectors and find they are competitive to standard distributional approaches.

===================================
paper_id: 119117163; YEAR: 2019
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: abs_tfidfcbow200 - title_tfidf - abs_tfidf - specter
TITLE: Gating Mechanisms for Combining Character and Word-level Word Representations: An Empirical Study
ABSTRACT: background_label: In this paper we study how different ways of combining character and word-level representations affect the quality of both final word and sentence representations.
method_label: We provide strong empirical evidence that modeling characters improves the learned representations at the word and sentence levels, and that doing so is particularly useful when representing less frequent words.
method_label: We further show that a feature-wise sigmoid gating mechanism is a robust method for creating representations that encode semantic similarity, as it performed reasonably well in several word similarity datasets.
result_label: Finally, our findings suggest that properly capturing semantic similarity at the word level does not consistently yield improved performance in downstream sentence-level tasks.
other_label: Our code is available at https://github.com/jabalazs/gating

===================================
paper_id: 1605984; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf - abs_cbow200 - specter
TITLE: Reading Between the Lines: Overcoming Data Sparsity for Accurate Classification of Lexical Relationships
ABSTRACT: background_label: The lexical semantic relationships between word pairs are key features for many NLP tasks.
background_label: Most approaches for automatically classifying related word pairs are hindered by data sparsity because of their need to observe two words co-occurring in order to detect the lexical relation holding between them.
background_label: Even when mining very large corpora, not every related word pair co-occurs.
method_label: Using novel representations based on graphs and word embeddings, we present two systems that are able to predict relations between words, even when these are never found in the same sentence in a given corpus.
result_label: In two experiments, we demonstrate superior performance of both approaches over the state of the art, achieving significant gains in recall.

===================================
paper_id: 1157497; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf - title_tfidfcbow200 - abs_tfidfcbow200
TITLE: Semantic word cloud generation based on word embeddings
ABSTRACT: background_label: Word clouds have been widely used to present the contents and themes in the text for summary and visualization.
objective_label: In this paper, we propose a new semantic word cloud taking into account the word semantic meanings.
method_label: Distributed word representation is applied to accurately describe the semantic meaning of words, and a word similarity graph is constructed based on the semantic distance between words to lay out words in a more compact and aesthetic manner.
method_label: Word-related interactions are introduced to guide users fast read and understand the text.
method_label: We apply the proposed word cloud to user generated reviews in different fields to demonstrate the effectiveness of our method.

===================================
paper_id: 11075130; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Deep Learning Paradigm with Transformed Monolingual Word Embeddings for Multilingual Sentiment Analysis
ABSTRACT: background_label: The surge of social media use brings huge demand of multilingual sentiment analysis (MSA) for unveiling cultural difference.
background_label: So far, traditional methods resorted to machine translation---translating texts in other languages to English, and then adopt the methods once worked in English.
background_label: However, this paradigm is conditioned by the quality of machine translation.
objective_label: In this paper, we propose a new deep learning paradigm to assimilate the differences between languages for MSA.
method_label: We first pre-train monolingual word embeddings separately, then map word embeddings in different spaces into a shared embedding space, and then finally train a parameter-sharing deep neural network for MSA.
result_label: The experimental results show that our paradigm is effective.
result_label: Especially, our CNN model outperforms a state-of-the-art baseline by around 2.1% in terms of classification accuracy.

===================================
paper_id: 3866444; YEAR: 2010
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf
TITLE: Niche as a determinant of word fate in online groups
ABSTRACT: background_label: Patterns of word use both reflect and influence a myriad of human activities and interactions.
background_label: Like other entities that are reproduced and evolve, words rise or decline depending upon a complex interplay between {their intrinsic properties and the environments in which they function}.
method_label: Using Internet discussion communities as model systems, we define the concept of a word niche as the relationship between the word and the characteristic features of the environments in which it is used.
method_label: We develop a method to quantify two important aspects of the size of the word niche: the range of individuals using the word and the range of topics it is used to discuss.
method_label: Controlling for word frequency, we show that these aspects of the word niche are strong determinants of changes in word frequency.
method_label: Previous studies have already indicated that word frequency itself is a correlate of word success at historical time scales.
result_label: Our analysis of changes in word frequencies over time reveals that the relative sizes of word niches are far more important than word frequencies in the dynamics of the entire vocabulary at shorter time scales, as the language adapts to new concepts and social groupings.
method_label: We also distinguish endogenous versus exogenous factors as additional contributors to the fates of words, and demonstrate the force of this distinction in the rise of novel words.
result_label: Our results indicate that short-term nonstationarity in word statistics is strongly driven by individual proclivities, including inclinations to provide novel information and to project a distinctive social identity.

===================================
paper_id: 58981651; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf - title_tfidf - title_tfidfcbow200 - title_cbow200 - specter - abs_tfidfcbow200
TITLE: Enhancing Semantic Word Representations by Embedding Deeper Word Relationships
ABSTRACT: background_label: Word representations are created using analogy context-based statistics and lexical relations on words.
background_label: Word representations are inputs for the learning models in Natural Language Understanding (NLU) tasks.
background_label: However, to understand language, knowing only the context is not sufficient.
background_label: Reading between the lines is a key component of NLU.
method_label: Embedding deeper word relationships which are not represented in the context enhances the word representation.
method_label: This paper presents a word embedding which combines an analogy, context-based statistics using Word2Vec, and deeper word relationships using Conceptnet, to create an expanded word representation.
method_label: In order to fine-tune the word representation, Self-Organizing Map is used to optimize it.
method_label: The proposed word representation is compared with semantic word representations using Simlex 999.
method_label: Furthermore, the use of 3D visual representations has shown to be capable of representing the similarity and association between words.
result_label: The proposed word representation shows a Spearman correlation score of 0.886 and provided the best results when compared to the current state-of-the-art methods, and exceed the human performance of 0.78.

===================================
paper_id: 7143588; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Obtaining referential word meanings from visual and distributional information: Experiments on object naming
ABSTRACT: background_label: AbstractWe investigate object naming, which is an important sub-task of referring expression generation on real-world images.
background_label: As opposed to mutually exclusive labels used in object recognition, object names are more flexible, subject to communicative preferences and semantically related to each other.
objective_label: Therefore, we investigate models of referential word meaning that link visual to lexical information which we assume to be given through distributional word embeddings.
method_label: We present a model that learns individual predictors for object names that link visual and distributional aspects of word meaning during training.
method_label: We show that this is particularly beneficial for zero-shot learning, as compared to projecting visual objects directly into the distributional space.
result_label: In a standard object naming task, we find that different ways of combining lexical and visual information achieve very similar performance, though experiments on model combination suggest that they capture complementary aspects of referential meaning.

===================================
paper_id: 174801339; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Learning Word Embeddings with Domain Awareness
ABSTRACT: background_label: Word embeddings are traditionally trained on a large corpus in an unsupervised setting, with no specific design for incorporating domain knowledge.
background_label: This can lead to unsatisfactory performances when training data originate from heterogeneous domains.
objective_label: In this paper, we propose two novel mechanisms for domain-aware word embedding training, namely domain indicator and domain attention, which integrate domain-specific knowledge into the widely used SG and CBOW models, respectively.
method_label: The two methods are based on a joint learning paradigm and ensure that words in a target domain are intensively focused when trained on a source domain corpus.
result_label: Qualitative and quantitative evaluation confirm the validity and effectiveness of our models.
result_label: Compared to baseline methods, our method is particularly effective in near-cold-start scenarios.

===================================
paper_id: 19191009; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200
TITLE: Convolutional Neural Network with Word Embeddings for Chinese Word Segmentation
ABSTRACT: background_label: Character-based sequence labeling framework is flexible and efficient for Chinese word segmentation (CWS).
background_label: Recently, many character-based neural models have been applied to CWS.
background_label: While they obtain good performance, they have two obvious weaknesses.
background_label: The first is that they heavily rely on manually designed bigram feature, i.e.
result_label: they are not good at capturing n-gram features automatically.
background_label: The second is that they make no use of full word information.
background_label: For the first weakness, we propose a convolutional neural model, which is able to capture rich n-gram features without any feature engineering.
objective_label: For the second one, we propose an effective approach to integrate the proposed model with word embeddings.
method_label: We evaluate the model on two benchmark datasets: PKU and MSR.
result_label: Without any feature engineering, the model obtains competitive performance -- 95.7% on PKU and 97.3% on MSR.
result_label: Armed with word embeddings, the model achieves state-of-the-art performance on both datasets -- 96.5% on PKU and 98.0% on MSR, without using any external labeled resource.

===================================
paper_id: 5481961; YEAR: 1969
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200
TITLE: IX. Word-Word Associations in Document Retrieval Systems
ABSTRACT: background_label: The SMART automatic document retrieval system is used to study association procedures for automatic content analysis.
background_label: The effect of word frequency and other parameters on the association process is investigated through examination of related pairs and through retrieval experiments.
background_label: Associated pairs of words usually reflect localized word meanings, and true synonyms cannot readily be found from first or second order relationships in our document collections.
background_label: There is little overlap between word relationships found through associations and those used in thesaurus construction, and the effects of word associations and a thesaurus in retrieval are independent.
method_label: The use of associations in retrieval experiments improves not only recall, by permitting new matches between requests and documents, but also precision, by reinforcing existing matches.
result_label: In our experiments, the precision effect is responsible for most of the improvement possible with associations.
result_label: A properly constructed thesaurus, however, offers better performance than statistical association methods.

===================================
paper_id: 890889; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter - abs_tfidf - abs_tfidfcbow200
TITLE: A Multiplicative Model for Learning Distributed Text-Based Attribute Representations
ABSTRACT: background_label: In this paper we propose a general framework for learning distributed representations of attributes: characteristics of text whose representations can be jointly learned with word embeddings.
background_label: Attributes can correspond to document indicators (to learn sentence vectors), language indicators (to learn distributed language representations), meta-data and side information (such as the age, gender and industry of a blogger) or representations of authors.
method_label: We describe a third-order model where word context and attribute vectors interact multiplicatively to predict the next word in a sequence.
method_label: This leads to the notion of conditional word similarity: how meanings of words change when conditioned on different attributes.
method_label: We perform several experimental tasks including sentiment classification, cross-lingual document classification, and blog authorship attribution.
result_label: We also qualitatively evaluate conditional word neighbours and attribute-conditioned text generation.

===================================
paper_id: 7335121; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Learning Multi-Modal Word Representation Grounded in Visual Context
ABSTRACT: background_label: Representing the semantics of words is a long-standing problem for the natural language processing community.
background_label: Most methods compute word semantics given their textual context in large corpora.
background_label: More recently, researchers attempted to integrate perceptual and visual features.
background_label: Most of these works consider the visual appearance of objects to enhance word representations but they ignore the visual environment and context in which objects appear.
method_label: We propose to unify text-based techniques with vision-based techniques by simultaneously leveraging textual and visual context to learn multimodal word embeddings.
method_label: We explore various choices for what can serve as a visual context and present an end-to-end method to integrate visual context elements in a multimodal skip-gram model.
result_label: We provide experiments and extensive analysis of the obtained results.

===================================
paper_id: 10533925; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: Ordering-sensitive and Semantic-aware Topic Modeling
ABSTRACT: background_label: Topic modeling of textual corpora is an important and challenging problem.
background_label: In most previous work, the"bag-of-words"assumption is usually made which ignores the ordering of words.
background_label: This assumption simplifies the computation, but it unrealistically loses the ordering information and the semantic of words in the context.
objective_label: In this paper, we present a Gaussian Mixture Neural Topic Model (GMNTM) which incorporates both the ordering of words and the semantic meaning of sentences into topic modeling.
method_label: Specifically, we represent each topic as a cluster of multi-dimensional vectors and embed the corpus into a collection of vectors generated by the Gaussian mixture model.
method_label: Each word is affected not only by its topic, but also by the embedding vector of its surrounding words and the context.
method_label: The Gaussian mixture components and the topic of documents, sentences and words can be learnt jointly.
result_label: Extensive experiments show that our model can learn better topics and more accurate word distributions for each topic.
result_label: Quantitatively, comparing to state-of-the-art topic modeling approaches, GMNTM obtains significantly better performance in terms of perplexity, retrieval accuracy and classification accuracy.

===================================
paper_id: 886027; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited - title_tfidf - abs_tfidf - title_tfidfcbow200 - title_cbow200 - abs_cbow200 - specter - abs_tfidfcbow200
TITLE: Learning Sentiment-Specific Word Embedding for Twitter Sentiment Classification
ABSTRACT: background_label: We present a method that learns word embedding for Twitter sentiment classification in this paper.
background_label: Most existing algorithms for learning continuous word representations typically only model the syntactic context of words but ignore the sentiment of text.
background_label: This is problematic for sentiment analysis as they usually map words with similar syntactic context but opposite sentiment polarity, such as good and bad, to neighboring word vectors.
objective_label: We address this issue by learning sentimentspecific word embedding (SSWE), which encodes sentiment information in the continuous representation of words.
method_label: Specifically, we develop three neural networks to effectively incorporate the supervision from sentiment polarity of text (e.g.
method_label: sentences or tweets) in their loss functions.
method_label: To obtain large scale training corpora, we learn the sentiment-specific word embedding from massive distant-supervised tweets collected by positive and negative emoticons.
result_label: Experiments on applying SSWE to a benchmark Twitter sentiment classification dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the performance is further improved by concatenating SSWE with existing feature set.

===================================
paper_id: 202558572; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: Neural Embedding Allocation: Distributed Representations of Topic Models
ABSTRACT: background_label: Word embedding models such as the skip-gram learn vector representations of words' semantic relationships, and document embedding models learn similar representations for documents.
background_label: On the other hand, topic models provide latent representations of the documents' topical themes.
method_label: To get the benefits of these representations simultaneously, we propose a unifying algorithm, called neural embedding allocation (NEA), which deconstructs topic models into interpretable vector-space embeddings of words, topics, documents, authors, and so on, by learning neural embeddings to mimic the topic models.
method_label: We showcase NEA's effectiveness and generality on LDA, author-topic models and the recently proposed mixed membership skip gram topic model and achieve better performance with the embeddings compared to several state-of-the-art models.
result_label: Furthermore, we demonstrate that using NEA to smooth out the topics improves coherence scores over the original topic models when the number of topics is large.

===================================
paper_id: 15632387; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_tfidf - title_cbow200
TITLE: Parsimonious Topic Models with Salient Word Discovery
ABSTRACT: background_label: We propose a parsimonious topic model for text corpora.
background_label: In related models such as Latent Dirichlet Allocation (LDA), all words are modeled topic-specifically, even though many words occur with similar frequencies across different topics.
method_label: Our modeling determines salient words for each topic, which have topic-specific probabilities, with the rest explained by a universal shared model.
method_label: Further, in LDA all topics are in principle present in every document.
method_label: By contrast our model gives sparse topic representation, determining the (small) subset of relevant topics for each document.
method_label: We derive a Bayesian Information Criterion (BIC), balancing model complexity and goodness of fit.
method_label: Here, interestingly, we identify an effective sample size and corresponding penalty specific to each parameter type in our model.
method_label: We minimize BIC to jointly determine our entire model -- the topic-specific words, document-specific topics, all model parameter values, {\it and} the total number of topics -- in a wholly unsupervised fashion.
result_label: Results on three text corpora and an image dataset show that our model achieves higher test set likelihood and better agreement with ground-truth class labels, compared to LDA and to a model designed to incorporate sparsity.

===================================
paper_id: 19835713; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: Hash Embeddings for Efficient Word Representations
ABSTRACT: background_label: We present hash embeddings, an efficient method for representing words in a continuous vector form.
background_label: A hash embedding may be seen as an interpolation between a standard word embedding and a word embedding created using a random hash function (the hashing trick).
method_label: In hash embeddings each token is represented by $k$ $d$-dimensional embeddings vectors and one $k$ dimensional weight vector.
method_label: The final $d$ dimensional representation of the token is the product of the two.
method_label: Rather than fitting the embedding vectors for each token these are selected by the hashing trick from a shared pool of $B$ embedding vectors.
method_label: Our experiments show that hash embeddings can easily deal with huge vocabularies consisting of millions of tokens.
method_label: When using a hash embedding there is no need to create a dictionary before training nor to perform any kind of vocabulary pruning after training.
result_label: We show that models trained using hash embeddings exhibit at least the same level of performance as models trained using regular embeddings across a wide range of tasks.
result_label: Furthermore, the number of parameters needed by such an embedding is only a fraction of what is required by a regular embedding.
result_label: Since standard embeddings and embeddings constructed using the hashing trick are actually just special cases of a hash embedding, hash embeddings can be considered an extension and improvement over the existing regular embedding types.

===================================
paper_id: 173990823; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter - abs_tfidf - abs_tfidfcbow200
TITLE: Contextually Propagated Term Weights for Document Representation
ABSTRACT: background_label: Word embeddings predict a word from its neighbours by learning small, dense embedding vectors.
background_label: In practice, this prediction corresponds to a semantic score given to the predicted word (or term weight).
objective_label: We present a novel model that, given a target word, redistributes part of that word's weight (that has been computed with word embeddings) across words occurring in similar contexts as the target word.
method_label: Thus, our model aims to simulate how semantic meaning is shared by words occurring in similar contexts, which is incorporated into bag-of-words document representations.
result_label: Experimental evaluation in an unsupervised setting against 8 state of the art baselines shows that our model yields the best micro and macro F1 scores across datasets of increasing difficulty.

===================================
paper_id: 8279277; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200 - abs_tfidf - title_tfidfcbow200 - abs_tfidfcbow200
TITLE: Discriminative Acoustic Word Embeddings: Recurrent Neural Network-Based Approaches
ABSTRACT: background_label: Acoustic word embeddings --- fixed-dimensional vector representations of variable-length spoken word segments --- have begun to be considered for tasks such as speech recognition and query-by-example search.
background_label: Such embeddings can be learned discriminatively so that they are similar for speech segments corresponding to the same word, while being dissimilar for segments corresponding to different words.
background_label: Recent work has found that acoustic word embeddings can outperform dynamic time warping on query-by-example search and related word discrimination tasks.
background_label: However, the space of embedding models and training approaches is still relatively unexplored.
objective_label: In this paper we present new discriminative embedding models based on recurrent neural networks (RNNs).
method_label: We consider training losses that have been successful in prior work, in particular a cross entropy loss for word classification and a contrastive loss that explicitly aims to separate same-word and different-word pairs in a"Siamese network"training setting.
result_label: We find that both classifier-based and Siamese RNN embeddings improve over previously reported results on a word discrimination task, with Siamese RNNs outperforming classification models.
result_label: In addition, we present analyses of the learned embeddings and the effects of variables such as dimensionality and network structure.

===================================
paper_id: 7300843; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf - abs_cbow200 - specter - abs_tfidfcbow200
TITLE: Comparative study of word embedding methods in topic segmentation
ABSTRACT: background_label: Abstract The vector representations of words are very useful in different natural language processing tasks in order to capture the semantic meaning of words.
background_label: In this context, the three known methods are: LSA, Word2Vec and GloVe.
method_label: In this paper, these methods will be investigated in the field of topic segmentation for both languages Arabic and English.
method_label: Moreover, Word2Vec is studied in depth by using different models and approximation algorithms.
result_label: As results, we found out that LSA, Word2Vec and GloVe depend on the used language.
result_label: However, Word2Vec presents the best word vector representation yet it depends on the choice of model.

===================================
paper_id: 186206964; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: A Systematic Comparison of English Noun Compound Representations
ABSTRACT: background_label: Building meaningful representations of noun compounds is not trivial since many of them scarcely appear in the corpus.
background_label: To that end, composition functions approximate the distributional representation of a noun compound by combining its constituent distributional vectors.
background_label: In the more general case, phrase embeddings have been trained by minimizing the distance between the vectors representing paraphrases.
method_label: We compare various types of noun compound representations, including distributional, compositional, and paraphrase-based representations, through a series of tasks and analyses, and with an extensive number of underlying word embeddings.
result_label: We find that indeed, in most cases, composition functions produce higher quality representations than distributional ones, and they improve with computational power.
result_label: No single function performs best in all scenarios, suggesting that a joint training objective may produce improved representations.

===================================
paper_id: 195767626; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Few-Shot Representation Learning for Out-Of-Vocabulary Words
ABSTRACT: background_label: Existing approaches for learning word embeddings often assume there are sufficient occurrences for each word in the corpus, such that the representation of words can be accurately estimated from their contexts.
background_label: However, in real-world scenarios, out-of-vocabulary (a.k.a.
background_label: OOV) words that do not appear in training corpus emerge frequently.
background_label: It is challenging to learn accurate representations of these words with only a few observations.
objective_label: In this paper, we formulate the learning of OOV embeddings as a few-shot regression problem, and address it by training a representation function to predict the oracle embedding vector (defined as embedding trained with abundant observations) based on limited observations.
method_label: Specifically, we propose a novel hierarchical attention-based architecture to serve as the neural regression function, with which the context information of a word is encoded and aggregated from K observations.
method_label: Furthermore, our approach can leverage Model-Agnostic Meta-Learning (MAML) for adapting the learned model to the new corpus fast and robustly.
result_label: Experiments show that the proposed approach significantly outperforms existing methods in constructing accurate embeddings for OOV words, and improves downstream tasks where these embeddings are utilized.

===================================
paper_id: 10980859; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - abs_tfidf - title_tfidfcbow200 - abs_cbow200 - specter - abs_tfidfcbow200
TITLE: Jointly Learning Word Embeddings and Latent Topics
ABSTRACT: background_label: Word embedding models such as Skip-gram learn a vector-space representation for each word, based on the local word collocation patterns that are observed in a text corpus.
background_label: Latent topic models, on the other hand, take a more global view, looking at the word distributions across the corpus to assign a topic to each word occurrence.
background_label: These two paradigms are complementary in how they represent the meaning of word occurrences.
method_label: While some previous works have already looked at using word embeddings for improving the quality of latent topics, and conversely, at using latent topics for improving word embeddings, such"two-step"methods cannot capture the mutual interaction between the two paradigms.
method_label: In this paper, we propose STE, a framework which can learn word embeddings and latent topics in a unified manner.
method_label: STE naturally obtains topic-specific word embeddings, and thus addresses the issue of polysemy.
method_label: At the same time, it also learns the term distributions of the topics, and the topic distributions of the documents.
result_label: Our experimental results demonstrate that the STE model can indeed generate useful topic-specific word embeddings and coherent latent topics in an effective and efficient way.

===================================
paper_id: 5344828; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidf - title_tfidfcbow200 - specter
TITLE: Cross-lingual Learning of Semantic Textual Similarity with Multilingual Word Representations
ABSTRACT: background_label: AbstractAssessing the semantic similarity between sentences in different languages is challenging.We approach this problem by leveraging multilingual distributional word representations, where similar words in different languages are close to each other.
background_label: The availability of parallel data allows us to train such representations on a large amount of languages.
method_label: This allows us to leverage semantic similarity data for languages for which no such data exists.
method_label: We train and evaluate on five language pairs, including English, Spanish, and Arabic.
result_label: We are able to train wellperforming systems for several language pairs, without any labelled data for that language pair.

===================================
paper_id: 2776354; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Learning Domain-Specific Word Embeddings from Sparse Cybersecurity Texts
ABSTRACT: background_label: Word embedding is a Natural Language Processing (NLP) technique that automatically maps words from a vocabulary to vectors of real numbers in an embedding space.
background_label: It has been widely used in recent years to boost the performance of a vari-ety of NLP tasks such as Named Entity Recognition, Syntac-tic Parsing and Sentiment Analysis.
background_label: Classic word embedding methods such as Word2Vec and GloVe work well when they are given a large text corpus.
background_label: When the input texts are sparse as in many specialized domains (e.g., cybersecurity), these methods often fail to produce high-quality vectors.
method_label: In this pa-per, we describe a novel method to train domain-specificword embeddings from sparse texts.
method_label: In addition to domain texts, our method also leverages diverse types of domain knowledge such as domain vocabulary and semantic relations.
method_label: Specifi-cally, we first propose a general framework to encode diverse types of domain knowledge as text annotations.
method_label: Then we de-velop a novel Word Annotation Embedding (WAE) algorithm to incorporate diverse types of text annotations in word em-bedding.
result_label: We have evaluated our method on two cybersecurity text corpora: a malware description corpus and a Common Vulnerability and Exposure (CVE) corpus.
result_label: Our evaluation re-sults have demonstrated the effectiveness of our method in learning domain-specific word embeddings.

===================================
paper_id: 19558838; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: specter - abs_tfidf - abs_cbow200 - abs_tfidfcbow200
TITLE: Modeling Context Words as Regions: An Ordinal Regression Approach to Word Embedding
ABSTRACT: background_label: AbstractVector representations of word meaning have found many applications in the field of natural language processing.
background_label: Word vectors intuitively represent the average context in which a given word tends to occur, but they cannot explicitly model the diversity of these contexts.
background_label: Although region representations of word meaning offer a natural alternative to word vectors, only few methods have been proposed that can effectively learn word regions.
method_label: In this paper, we propose a new word embedding model which is based on SVM regression.
method_label: We show that the underlying ranking interpretation of word contexts is sufficient to match, and sometimes outperform, the performance of popular methods such as Skip-gram.
result_label: Furthermore, we show that by using a quadratic kernel, we can effectively learn word regions, which outperform existing unsupervised models for the task of hypernym detection.

===================================
paper_id: 168170148; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: title_tfidfcbow200
TITLE: Learning Multilingual Word Embeddings Using Image-Text Data
ABSTRACT: background_label: There has been significant interest recently in learning multilingual word embeddings -- in which semantically similar words across languages have similar embeddings.
background_label: State-of-the-art approaches have relied on expensive labeled data, which is unavailable for low-resource languages, or have involved post-hoc unification of monolingual embeddings.
objective_label: In the present paper, we investigate the efficacy of multilingual embeddings learned from weakly-supervised image-text data.
method_label: In particular, we propose methods for learning multilingual embeddings using image-text data, by enforcing similarity between the representations of the image and that of the text.
result_label: Our experiments reveal that even without using any expensive labeled data, a bag-of-words-based embedding model trained on image-text data achieves performance comparable to the state-of-the-art on crosslingual semantic similarity tasks.

===================================
paper_id: 7102885; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200 - specter
TITLE: Multivariate Gaussian Document Representation from Word Embeddings for Text Categorization
ABSTRACT: background_label: AbstractRecently, there has been a lot of activity in learning distributed representations of words in vector spaces.
background_label: Although there are models capable of learning high-quality distributed representations of words, how to generate vector representations of the same quality for phrases or documents still remains a challenge.
objective_label: In this paper, we propose to model each document as a multivariate Gaussian distribution based on the distributed representations of its words.
method_label: We then measure the similarity between two documents based on the similarity of their distributions.
result_label: Experiments on eight standard text categorization datasets demonstrate the effectiveness of the proposed approach in comparison with stateof-the-art methods.

===================================
paper_id: 3249710; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Learning Task-specific Bilexical Embeddings
ABSTRACT: method_label: AbstractWe present a method that learns bilexical operators over distributional representations of words and leverages supervised data for a linguistic relation.
method_label: The learning algorithm exploits lowrank bilinear forms and induces low-dimensional embeddings of the lexical space tailored for the target linguistic relation.
method_label: An advantage of imposing low-rank constraints is that prediction is expressed as the inner-product between low-dimensional embeddings, which can have great computational benefits.
result_label: In experiments with multiple linguistic bilexical relations we show that our method effectively learns using embeddings of a few dimensions.

===================================
paper_id: 184487455; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf
TITLE: Word-level Speech Recognition with a Dynamic Lexicon
ABSTRACT: background_label: We propose a direct-to-word sequence model with a dynamic lexicon.
background_label: Our word network constructs word embeddings dynamically from the character level tokens.
method_label: The word network can be integrated seamlessly with arbitrary sequence models including Connectionist Temporal Classification and encoder-decoder models with attention.
method_label: Sub-word units are commonly used in speech recognition yet are generated without the use of acoustic context.
method_label: We show our direct-to-word model can achieve word error rate gains over sub-word level models for speech recognition.
result_label: Furthermore, we empirically validate that the word-level embeddings we learn contain significant acoustic information, making them more suitable for use in speech recognition.
result_label: We also show that our direct-to-word approach retains the ability to predict words not seen at training time without any retraining.

===================================
paper_id: 14631029; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200 - title_cbow200
TITLE: Learning Compositionality Functions on Word Embeddings for Modelling Attribute Meaning in Adjective-Noun Phrases
ABSTRACT: background_label: Word embeddings have been shown to be highly effective in a variety of lexical semantic tasks.
background_label: They tend to capture meaningful relational similarities between individual words, at the expense of lacking the capabilty of making the underlying semantic relation explicit.
objective_label: In this paper, we investigate the attribute relation that often holds between the constituents of adjective-noun phrases.
method_label: We use CBOW word embeddings to represent word meaning and learn a compositionality function that combines the individual constituents into a phrase representation, thus capturing the compositional attribute meaning.
method_label: The resulting embedding model, while being fully interpretable, outperforms count-  based distributional vector space models that are tailored to attribute meaning in the two tasks of attribute selection and phrase similarity prediction.
result_label: Moreover, as the model captures a generalized layer of attribute meaning, it bears the potential to be used for predictions over various attribute inventories without re-training.

===================================
paper_id: 31708368; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_tfidf - title_tfidfcbow200 - title_cbow200 - abs_cbow200
TITLE: Sentiment Analysis by Joint Learning of Word Embeddings and Classifier
ABSTRACT: background_label: Word embeddings are representations of individual words of a text document in a vector space and they are often use- ful for performing natural language pro- cessing tasks.
background_label: Current state of the art al- gorithms for learning word embeddings learn vector representations from large corpora of text documents in an unsu- pervised fashion.
objective_label: This paper introduces SWESA (Supervised Word Embeddings for Sentiment Analysis), an algorithm for sentiment analysis via word embeddings.
method_label: SWESA leverages document label infor- mation to learn vector representations of words from a modest corpus of text doc- uments by solving an optimization prob- lem that minimizes a cost function with respect to both word embeddings as well as classification accuracy.
method_label: Analysis re- veals that SWESA provides an efficient way of estimating the dimension of the word embeddings that are to be learned.
result_label: Experiments on several real world data sets show that SWESA has superior per- formance when compared to previously suggested approaches to word embeddings and sentiment analysis tasks.

===================================
paper_id: 3445212; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200 - abs_cbow200 - abs_tfidfcbow200
TITLE: Refining Word Embeddings for Sentiment Analysis
ABSTRACT: background_label: AbstractWord embeddings that can capture semantic and syntactic information from contexts have been extensively used for various natural language processing tasks.
background_label: However, existing methods for learning contextbased word embeddings typically fail to capture sufficient sentiment information.
background_label: This may result in words with similar vector representations having an opposite sentiment polarity (e.g., good and bad), thus degrading sentiment analysis performance.
objective_label: Therefore, this study proposes a word vector refinement model that can be applied to any pre-trained word vectors (e.g., Word2vec and GloVe).
method_label: The refinement model is based on adjusting the vector representations of words such that they can be closer to both semantically and sentimentally similar words and further away from sentimentally dissimilar words.
result_label: Experimental results show that the proposed method can improve conventional word embeddings and outperform previously proposed sentiment embeddings for both binary and fine-grained classification on Stanford Sentiment Treebank (SST).

===================================
paper_id: 17313435; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf - title_tfidfcbow200 - title_cbow200
TITLE: TweetSift: Tweet Topic Classification Based on Entity Knowledge Base and Topic Enhanced Word Embedding
ABSTRACT: background_label: Classifying tweets into topic categories is necessary and important for many applications, since tweets are about a variety of topics and users are only interested in certain topical areas.
background_label: Many tweet classification approaches fail to achieve high accuracy due to data sparseness issue.
background_label: Tweet, as a special type of short text, in additional to its text, also has other metadata that can be used to enrich its context, such as user name, mention, hashtag and embedded link.
method_label: In this demonstration, we present TweetSift, an efficient and effective real time tweet topic classifier.
method_label: TweetSift exploits external tweet-specific entity knowledge to provide more topical context for a tweet, and integrates them with topic enhanced word embeddings for topic classification.
result_label: The demonstration will show how TweetSift works and how it is incorporated with our social media event detection system.

===================================
paper_id: 1919756; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Learning Semantic Hierarchies via Word Embeddings
ABSTRACT: objective_label: Semantic hierarchy construction aims to build structures of concepts linked by hypernym‐hyponym (“is-a”) relations.
background_label: A major challenge for this task is the automatic discovery of such relations.
method_label: This paper proposes a novel and effective method for the construction of semantic hierarchies based on word embeddings, which can be used to measure the semantic relationship between words.
method_label: We identify whether a candidate word pair has hypernym‐hyponym relation by using the word-embedding-based semantic projections between words and their hypernyms.
result_label: Our result, an F-score of 73.74%, outperforms the state-of-theart methods on a manually labeled test dataset.
result_label: Moreover, combining our method with a previous manually-built hierarchy extension method can further improve Fscore to 80.29%.

===================================
paper_id: 17751128; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Short Text Similarity with Word Embeddings
ABSTRACT: background_label: Determining semantic similarity between texts is important in many tasks in information retrieval such as search, query suggestion, automatic summarization and image finding.
background_label: Many approaches have been suggested, based on lexical matching, handcrafted patterns, syntactic parse trees, external sources of structured semantic knowledge and distributional semantics.
background_label: However, lexical features, like string matching, do not capture semantic similarity beyond a trivial level.
background_label: Furthermore, handcrafted patterns and external sources of structured semantic knowledge cannot be assumed to be available in all circumstances and for all domains.
method_label: Lastly, approaches depending on parse trees are restricted to syntactically well-formed texts, typically of one sentence in length.
result_label: We investigate whether determining short text similarity is possible using only semantic features---where by semantic we mean, pertaining to a representation of meaning---rather than relying on similarity in lexical or syntactic representations.
background_label: We use word embeddings, vector representations of terms, computed from unlabelled data, that represent terms in a semantic space in which proximity of vectors can be interpreted as semantic similarity.
objective_label: We propose to go from word-level to text-level semantics by combining insights from methods based on external sources of semantic knowledge with word embeddings.
method_label: A novel feature of our approach is that an arbitrary number of word embedding sets can be incorporated.
method_label: We derive multiple types of meta-features from the comparison of the word vectors for short text pairs, and from the vector means of their respective word embeddings.
method_label: The features representing labelled short text pairs are used to train a supervised learning algorithm.
result_label: We use the trained model at testing time to predict the semantic similarity of new, unlabelled pairs of short texts   We show on a publicly available evaluation set commonly used for the task of semantic similarity that our method outperforms baseline methods that work under the same conditions.

===================================
paper_id: 35684985; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 1; annotator4: 0; annotator3: 1
sources: abs_tfidfcbow200 - title_tfidf - abs_tfidf - title_tfidfcbow200
TITLE: A Correlated Topic Model Using Word Embeddings
ABSTRACT: background_label: AbstractConventional correlated topic models are able to capture correlation structure among latent topics by replacing the Dirichlet prior with the logistic normal distribution.
background_label: Word embeddings have been proven to be able to capture semantic regularities in language.
background_label: Therefore, the semantic relatedness and correlations between words can be directly calculated in the word embedding space, for example, via cosine values.
objective_label: In this paper, we propose a novel correlated topic model using word embeddings.
method_label: The proposed model enables us to exploit the additional word-level correlation information in word embeddings and directly model topic correlation in the continuous word embedding space.
method_label: In the model, words in documents are replaced with meaningful word embeddings, topics are modeled as multivariate Gaussian distributions over the word embeddings and topic correlations are learned among the continuous Gaussian topics.
method_label: A Gibbs sampling solution with data augmentation is given to perform inference.
result_label: We evaluate our model on the 20 Newsgroups dataset and the Reuters-21578 dataset qualitatively and quantitatively.
result_label: The experimental results show the effectiveness of our proposed model.

===================================
paper_id: 13706128; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidf - title_tfidfcbow200
TITLE: Learning Domain-Sensitive and Sentiment-Aware Word Embeddings
ABSTRACT: background_label: Word embeddings have been widely used in sentiment classification because of their efficacy for semantic representations of words.
background_label: Given reviews from different domains, some existing methods for word embeddings exploit sentiment information, but they cannot produce domain-sensitive embeddings.
background_label: On the other hand, some other existing methods can generate domain-sensitive word embeddings, but they cannot distinguish words with similar contexts but opposite sentiment polarity.
method_label: We propose a new method for learning domain-sensitive and sentiment-aware embeddings that simultaneously capture the information of sentiment semantics and domain sensitivity of individual words.
method_label: Our method can automatically determine and produce domain-common embeddings and domain-specific embeddings.
method_label: The differentiation of domain-common and domain-specific words enables the advantage of data augmentation of common semantics from multiple domains and capture the varied semantics of specific words from different domains at the same time.
result_label: Experimental results show that our model provides an effective way to learn domain-sensitive and sentiment-aware word embeddings which benefit sentiment classification at both sentence level and lexicon term level.

===================================
paper_id: 44106594; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 1; annotator4: 0; annotator3: 1
sources: abs_tfidf - title_tfidf - abs_cbow200 - abs_tfidfcbow200
TITLE: WELDA: Enhancing Topic Models by Incorporating Local Word Context
ABSTRACT: background_label: The distributional hypothesis states that similar words tend to have similar contexts in which they occur.
background_label: Word embedding models exploit this hypothesis by learning word vectors based on the local context of words.
background_label: Probabilistic topic models on the other hand utilize word co-occurrences across documents to identify topically related words.
method_label: Due to their complementary nature, these models define different notions of word similarity, which, when combined, can produce better topical representations.
method_label: In this paper we propose WELDA, a new type of topic model, which combines word embeddings (WE) with latent Dirichlet allocation (LDA) to improve topic quality.
method_label: We achieve this by estimating topic distributions in the word embedding space and exchanging selected topic words via Gibbs sampling from this space.
result_label: We present an extensive evaluation showing that WELDA cuts runtime by at least 30% while outperforming other combined approaches with respect to topic coherence and for solving word intrusion tasks.

===================================
paper_id: 747342; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidfcbow200 - abs_tfidf - abs_cbow200 - specter
TITLE: Do Supervised Distributional Methods Really Learn Lexical Inference Relations?
ABSTRACT: background_label: Distributional representations of words have been recently used in supervised settings for recognizing lexical inference relations between word pairs, such as hypernymy and entailment.
method_label: We investigate a collection of these state-of-the-art methods, and show that they do not actually learn a relation between two words.
result_label: Instead, they learn an independent property of a single word in the pair: whether that word is a “prototypical hypernym”.

===================================
paper_id: 41260659; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: Representations of Sparse Distributed Networks: A Locality-Sensitive Approach
ABSTRACT: background_label: In 1999, Brodal and Fagerberg (BF) gave an algorithm for maintaining a low outdegree orientation of a dynamic uniformly sparse graph.
background_label: Specifically, for a dynamic graph on $n$-vertices, with arboricity bounded by $\alpha$ at all times, the BF algorithm supports edge updates in $O(\log n)$ amortized update time, while keeping the maximum outdegree in the graph bounded by $O(\alpha)$.
background_label: Such an orientation provides a basic data structure for uniformly sparse graphs, which found applications to a plethora of dynamic graph algorithms.
background_label: A significant weakness of the BF algorithm is the possible \emph{temporary} blowup of the maximum outdegree, following edge insertions.
background_label: Although BF eventually reduces all outdegrees to $O(\alpha)$, local memory usage at the vertices, which is an important quality measure in distributed systems, cannot be bounded.
method_label: We show how to modify the BF algorithm to guarantee that the outdegrees of all vertices are bounded by $O(\alpha)$ at all times, without hurting any of its other properties, and present an efficient distributed implementation of the modified algorithm.
method_label: This provides the \emph{first} representation of distributed networks in which the local memory usage at all vertices is bounded by the arboricity (which is essentially the average degree of the densest subgraph) rather than the maximum degree.
method_label: For settings where there are no local memory constraints, we take the temporary outdegree blowup to the extreme and allow a permanent outdegree blowup.
method_label: This allows us to address the second significant weakness of the BF algorithm -- its inherently \emph{global} nature: An insertion of an edge $(u,v)$ may trigger changes in the orientations of edges that are far away from $u$ and $v$.
result_label: We suggest an alternative \emph{local} scheme, which does not guarantee any outdegree bound on the vertices, yet is just as efficient as the BF scheme for various applications.

===================================
paper_id: 119425731; YEAR: 1972
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: Unzerlegbare Darstellungen I
ABSTRACT: background_label: LetK be the structure got by forgetting the composition law of morphisms in a given category.
background_label: A linear representation ofK is given by a map V associating with any morphism ϕ: a→e ofK a linear vector space map V(ϕ): V(a)→V(e).
method_label: We classify thoseK having only finitely many isomorphy classes of indecomposable linear representations.
other_label: This classification is related to an old paper by Yoshii [3].

===================================
paper_id: 58014233; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf - title_tfidfcbow200 - title_cbow200
TITLE: Robust Chinese Word Segmentation with Contextualized Word Representations
ABSTRACT: background_label: In recent years, after the neural-network-based method was proposed, the accuracy of the Chinese word segmentation task has made great progress.
background_label: However, when dealing with out-of-vocabulary words, there is still a large error rate.
method_label: We used a simple bidirectional LSTM architecture and a large-scale pretrained language model to generate high-quality contextualize character representations, which successfully reduced the weakness of the ambiguous meanings of each Chinese character that widely appears in Chinese characters, and hence effectively reduced OOV error rate.
result_label: State-of-the-art performance is achieved on many datasets.

===================================
paper_id: 15814448; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200 - specter
TITLE: Semantic word embedding neural network language models for automatic speech recognition
ABSTRACT: background_label: Semantic word embeddings have become increasingly important in natural language processing tasks over the last few years.
background_label: This popularity is due to their ability to easily capture rich semantic information through a distributed representation and the availability of fast and scalable algorithms for learning them from large text corpora.
background_label: State-of-the-art neural network language models (NNLMs) used in automatic speech recognition (ASR) and natural language processing also learn word embeddings optimized to model local N-gram dependencies given training text but are not optimized to capture semantic information.
method_label: We hypothesize that semantic word embeddings provide diverse information compared to the word embeddings learned by NNLMs.
method_label: We propose novel feedforward NNLM architectures that incorporate semantic word embeddings.
method_label: We apply the resulting NNLMs to ASR on broadcast news and show improvements in both perplexity and word error rate.

===================================
paper_id: 53730858; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200 - specter
TITLE: Correcting the Common Discourse Bias in Linear Representation of Sentences using Conceptors
ABSTRACT: background_label: Distributed representations of words, better known as word embeddings, have become important building blocks for natural language processing tasks.
background_label: Numerous studies are devoted to transferring the success of unsupervised word embeddings to sentence embeddings.
method_label: In this paper, we introduce a simple representation of sentences in which a sentence embedding is represented as a weighted average of word vectors followed by a soft projection.
other_label: We demonstrate the effectiveness of this proposed method on the clinical semantic textual similarity task of the BioCreative/OHNLP Challenge 2018.

===================================
paper_id: 5791183; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Unsupervised Learning of Discriminative Attributes and Visual Representations
ABSTRACT: background_label: Attributes offer useful mid-level features to interpret visual data.
background_label: While most attribute learning methods are supervised by costly human-generated labels, we introduce a simple yet powerful unsupervised approach to learn and predict visual attributes directly from data.
method_label: Given a large unlabeled image collection as input, we train deep Convolutional Neural Networks (CNNs) to output a set of discriminative, binary attributes often with semantic meanings.
method_label: Specifically, we first train a CNN coupled with unsupervised discriminative clustering, and then use the cluster membership as a soft supervision to discover shared attributes from the clusters while maximizing their separability.
method_label: The learned attributes are shown to be capable of encoding rich imagery properties from both natural images and contour patches.
result_label: The visual representations learned in this way are also transferrable to other tasks such as object detection.
result_label: We show other convincing results on the related tasks of image retrieval and classification, and contour detection.

===================================
paper_id: 67751231; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200 - title_tfidfcbow200
TITLE: Learned In Speech Recognition: Contextual Acoustic Word Embeddings
ABSTRACT: background_label: End-to-end acoustic-to-word speech recognition models have recently gained popularity because they are easy to train, scale well to large amounts of training data, and do not require a lexicon.
background_label: In addition, word models may also be easier to integrate with downstream tasks such as spoken language understanding, because inference (search) is much simplified compared to phoneme, character or any other sort of sub-word units.
method_label: In this paper, we describe methods to construct contextual acoustic word embeddings directly from a supervised sequence-to-sequence acoustic-to-word speech recognition model using the learned attention distribution.
method_label: On a suite of 16 standard sentence evaluation tasks, our embeddings show competitive performance against a word2vec model trained on the speech transcriptions.
result_label: In addition, we evaluate these embeddings on a spoken language understanding task, and observe that our embeddings match the performance of text-based embeddings in a pipeline of first performing speech recognition and then constructing word embeddings from transcriptions.

===================================
paper_id: 53216389; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Unsupervised Hyperalignment for Multilingual Word Embeddings
ABSTRACT: background_label: We consider the problem of aligning continuous word representations, learned in multiple languages, to a common space.
background_label: It was recently shown that, in the case of two languages, it is possible to learn such a mapping without supervision.
objective_label: This paper extends this line of work to the problem of aligning multiple languages to a common space.
method_label: A solution is to independently map all languages to a pivot language.
method_label: Unfortunately, this degrades the quality of indirect word translation.
method_label: We thus propose a novel formulation that ensures composable mappings, leading to better alignments.
result_label: We evaluate our method by jointly aligning word vectors in eleven languages, showing consistent improvement with indirect mappings while maintaining competitive performance on direct word translation.

===================================
paper_id: 3405188; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: title_tfidf
TITLE: Evaluating Visual Representations for Topic Understanding and Their Effects on Manually Generated Topic Labels
ABSTRACT: background_label: Probabilistic topic models are important tools for indexing, summarizing, and analyzing large document collections by their themes.
background_label: However, promoting end-user understanding of topics remains an open research problem.
method_label: We compare labels generated by users given four topic visualization techniques—word lists, word lists with bars, word clouds, and network graphs—against each other and against automatically generated labels.
method_label: Our basis of comparison is participant ratings of how well labels describe documents from the topic.
method_label: Our study has two phases: a labeling phase where participants label visualized topics and a validation phase where different participants select which labels best describe the topics’ documents.
method_label: Although all visualizations produce similar quality labels, simple visualizations such as word lists allow participants to quickly understand topics, while complex visualizations take longer but expose multi-word expressions that simpler visualizations obscure.
result_label: Automatic labels lag behind user-created labels, but our dataset of manually labeled topics highlights linguistic patterns (e.g., hypernyms, phrases) that can be used to improve automatic topic labeling algorithms.

===================================
paper_id: 16251657; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features
ABSTRACT: background_label: The recent tremendous success of unsupervised word embeddings in a multitude of applications raises the obvious question if similar methods could be derived to improve embeddings (i.e.
background_label: semantic representations) of word sequences as well.
method_label: We present a simple but efficient unsupervised objective to train distributed representations of sentences.
method_label: Our method outperforms the state-of-the-art unsupervised models on most benchmark tasks, highlighting the robustness of the produced general-purpose sentence embeddings.

===================================
paper_id: 52161864; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Learning Gender-Neutral Word Embeddings
ABSTRACT: background_label: Word embedding models have become a fundamental component in a wide range of Natural Language Processing (NLP) applications.
background_label: However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs.
objective_label: To address this concern, in this paper, we propose a novel training procedure for learning gender-neutral word embeddings.
objective_label: Our approach aims to preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence.
method_label: Based on the proposed method, we generate a Gender-Neutral variant of GloVe (GN-GloVe).
result_label: Quantitative and qualitative experiments demonstrate that GN-GloVe successfully isolates gender information without sacrificing the functionality of the embedding model.

===================================
paper_id: 3084740; YEAR: 2010
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf - abs_tfidf
TITLE: Best Topic Word Selection for Topic Labelling
ABSTRACT: objective_label: AbstractThis paper presents the novel task of best topic word selection, that is the selection of the topic word that is the best label for a given topic, as a means of enhancing the interpretation and visualisation of topic models.
method_label: We propose a number of features intended to capture the best topic word, and show that, in combination as inputs to a reranking model, we are able to consistently achieve results above the baseline of simply selecting the highest-ranked topic word.
result_label: This is the case both when training in-domain over other labelled topics for that topic model, and cross-domain, using only labellings from independent topic models learned over document collections from different domains and genres.

===================================
paper_id: 52886585; YEAR: 1994
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited - title_tfidf - abs_tfidf - title_tfidfcbow200 - title_cbow200 - abs_cbow200 - specter - abs_tfidfcbow200
TITLE: WORDNET: A Lexical Database For English
ABSTRACT: objective_label: The goal of this project is to provide lexical resources for natural language research.
objective_label: The primary emphases are on the further development and dissemination of the on-line lexical database, WordNet.
objective_label: A secondary goal is to learn how to develop contextual representations for different senses of a polysemous word, where a contextual representation is comprised of topical and local context for each sense.

===================================
paper_id: 8088725; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - specter
TITLE: Improving Word Alignment of Rare Words with Word Embeddings
ABSTRACT: objective_label: AbstractWe address the problem of inducing word alignment for language pairs by developing an unsupervised model with the capability of getting applied to other generative alignment models.
method_label: We approach the task by: i) proposing a new alignment model based on the IBM alignment model 1 that uses vector representation of words, and ii) examining the use of similar source words to overcome the problem of rare source words and improving the alignments.
method_label: We apply our method to English-French corpora and run the experiments with different sizes of sentence pairs.
result_label: Our results show competitive performance against the baseline and in some cases improve the results up to 6.9% in terms of precision.

===================================
paper_id: 15240372; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter - title_tfidf - title_tfidfcbow200 - title_cbow200
TITLE: Topic Modeling Using Distributed Word Embeddings
ABSTRACT: objective_label: We propose a new algorithm for topic modeling, Vec2Topic, that identifies the main topics in a corpus using semantic information captured via high-dimensional distributed word embeddings.
method_label: Our technique is unsupervised and generates a list of topics ranked with respect to importance.
method_label: We find that it works better than existing topic modeling techniques such as Latent Dirichlet Allocation for identifying key topics in user-generated content, such as emails, chats, etc., where topics are diffused across the corpus.
result_label: We also find that Vec2Topic works equally well for non-user generated content, such as papers, reports, etc., and for small corpora such as a single-document.

===================================
paper_id: 4316311; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter - title_tfidf - title_tfidfcbow200
TITLE: Extended Topic Model for Word Dependency
ABSTRACT: background_label: Topic Model such as Latent Dirichlet Allocation(LDA) makes assumption that topic assignment of different words are conditionally independent.
objective_label: In this paper, we propose a new model Extended Global Topic Random Field (EGTRF) to model non-linear dependencies between words.
method_label: Specifically, we parse sentences into dependency trees and represent them as a graph, and assume the topic assignment of a word is influenced by its adjacent words and distance-2 words.
method_label: Word similarity information learned from large corpus is incorporated to enhance word topic assignment.
result_label: Parameters are estimated efficiently by variational inference and experimental results on two datasets show EGTRF achieves lower perplexity and higher log predictive probability.

===================================
paper_id: 3003897; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidfcbow200 - abs_tfidf - abs_cbow200 - specter
TITLE: Embedding Words and Senses Together via Joint Knowledge-Enhanced Training
ABSTRACT: background_label: Word embeddings are widely used in Natural Language Processing, mainly due to their success in capturing semantic information from massive corpora.
background_label: However, their creation process does not allow the different meanings of a word to be automatically separated, as it conflates them into a single vector.
objective_label: We address this issue by proposing a new model which learns word and sense embeddings jointly.
method_label: Our model exploits large corpora and knowledge from semantic networks in order to produce a unified vector space of word and sense embeddings.
result_label: We evaluate the main features of our approach both qualitatively and quantitatively in a variety of tasks, highlighting the advantages of the proposed method in comparison to state-of-the-art word- and sense-based models.

===================================
paper_id: 16583101; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: CSE: Conceptual Sentence Embeddings based on Attention Model
ABSTRACT: background_label: Most sentence embedding models typically represent each sentence only using word surface, which makes these models indiscriminative for ubiquitous homonymy and polysemy.
method_label: In order to enhance representation capability of sentence, we employ conceptualization model to assign associated concepts for each sentence in the text corpus, and then learn conceptual sentence embedding (CSE).
method_label: Hence, this semantic representation is more expressive than some widely-used text representation models such as latent topic model, especially for short-text.
method_label: Moreover, we further extend CSE models by utilizing a local attention-based model that select relevant words within the context to make more efficient prediction.
result_label: In the experiments, we evaluate the CSE models on two tasks, text classification and information retrieval.
result_label: The experimental results show that the proposed models outperform typical sentence embed-ding models.

===================================
paper_id: 5851907; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: Learning Distributed Word Representations for Natural Logic Reasoning
ABSTRACT: background_label: Natural logic offers a powerful relational conception of meaning that is a natural counterpart to distributed semantic representations, which have proven valuable in a wide range of sophisticated language tasks.
background_label: However, it remains an open question whether it is possible to train distributed representations to support the rich, diverse logical reasoning captured by natural logic.
method_label: We address this question using two neural network-based models for learning embeddings: plain neural networks and neural tensor networks.
result_label: Our experiments evaluate the models' ability to learn the basic algebra of natural logic relations from simulated data and from the WordNet noun graph.
result_label: The overall positive results are promising for the future of learned distributed representations in the applied modeling of logical semantics.

===================================
paper_id: 16243967; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited - title_tfidf - abs_tfidf - title_tfidfcbow200 - title_cbow200 - abs_cbow200 - specter - abs_tfidfcbow200
TITLE: Learning Word Sense Distributions, Detecting Unattested Senses and Identifying Novel Senses Using Topic Models
ABSTRACT: background_label: Unsupervised word sense disambiguation (WSD) methods are an attractive approach to all-words WSD due to their non-reliance on expensive annotated data.
background_label: Unsupervised estimates of sense frequency have been shown to be very useful for WSD due to the skewed nature of word sense distributions.
method_label: This paper presents a fully unsupervised topic modelling-based approach to sense frequency estimation, which is highly portable to different corpora and sense inventories, in being applicable to any part of speech, and not requiring a hierarchical sense inventory, parsing or parallel text.
result_label: We demonstrate the effectiveness of the method over the tasks of predominant sense learning and sense distribution acquisition, and also the novel tasks of detecting senses which aren’t attested in the corpus, and identifying novel senses in the corpus which aren’t captured in the sense inventory.

===================================
paper_id: 8406927; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidf - title_tfidfcbow200
TITLE: Learning Robust Representations of Text
ABSTRACT: background_label: Deep neural networks have achieved remarkable results across many language processing tasks, however these methods are highly sensitive to noise and adversarial attacks.
method_label: We present a regularization based method for limiting network sensitivity to its inputs, inspired by ideas from computer vision, thus learning models that are more robust.
result_label: Empirical evaluation over a range of sentiment datasets with a convolutional neural network shows that, compared to a baseline model and the dropout method, our method achieves superior performance over noisy inputs and out-of-domain data.

===================================
paper_id: 717237; YEAR: 2010
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200
TITLE: Learning to use words: Event-related potentials index single-shot contextual word learning
ABSTRACT: background_label: Humans have the remarkable capacity to learn words from a single instance.
objective_label: The goal of this study was to examine the impact of initial learning context on the understanding of novel word usage using event-related brain potentials.
method_label: Participants saw known and unknown words in strongly or weakly constraining sentence contexts.
method_label: After each sentence context, word usage knowledge was assessed via plausibility ratings of these words as the objects of transitive verbs.
method_label: Plausibility effects were observed in the N400 component to the verb only when the upcoming novel word object had initially appeared in a strongly constraining context.
result_label: These results demonstrate that rapid word learning is modulated by contextual constraint and reveal a rapid mental process that is sensitive to novel word usage.

===================================
paper_id: 85543290; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Mining Discourse Markers for Unsupervised Sentence Representation Learning
ABSTRACT: background_label: Current state of the art systems in NLP heavily rely on manually annotated datasets, which are expensive to construct.
background_label: Very little work adequately exploits unannotated data -- such as discourse markers between sentences -- mainly because of data sparseness and ineffective extraction methods.
method_label: In the present work, we propose a method to automatically discover sentence pairs with relevant discourse markers, and apply it to massive amounts of data.
method_label: Our resulting dataset contains 174 discourse markers with at least 10k examples each, even for rare markers such as coincidentally or amazingly We use the resulting data as supervision for learning transferable sentence embeddings.
result_label: In addition, we show that even though sentence representation learning through prediction of discourse markers yields state of the art results across different transfer tasks, it is not clear that our models made use of the semantic relation between sentences, thus leaving room for further improvements.
other_label: Our datasets are publicly available (https://github.com/synapse-developpement/Discovery)

===================================
paper_id: 14502960; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_tfidf
TITLE: Gaussian LDA for Topic Models with Word Embeddings
ABSTRACT: background_label: Continuous space word embeddings learned from large, unstructured corpora have been shown to be effective at capturing semantic regularities in language.
background_label: In this paper we replace LDA’s parameterization of “topics” as categorical distributions over opaque word types with multivariate Gaussian distributions on the embedding space.
method_label: This encourages the model to group words that are a priori known to be semantically related into topics.
method_label: To perform inference, we introduce a fast collapsed Gibbs sampling algorithm based on Cholesky decompositions of covariance matrices of the posterior predictive distributions.
method_label: We further derive a scalable algorithm that draws samples from stale posterior predictive distributions and corrects them with a Metropolis–Hastings step.
result_label: Using vectors learned from a domain-general corpus (English Wikipedia), we report results on two document collections (20-newsgroups and NIPS).
result_label: Qualitatively, Gaussian LDA infers different (but still very sensible) topics relative to standard LDA.
result_label: Quantitatively, our technique outperforms existing models at dealing with OOV words in held-out documents.

===================================
paper_id: 5087912; YEAR: 2000
adju relevance: Irrelevant (0)
difference: 1; annotator4: 0; annotator3: 1
sources: abs_tfidf
TITLE: A Classification Approach to Word Prediction
ABSTRACT: objective_label: The eventual goal of a language model is to accurately predict the value of a missing word given its context.
objective_label: We present an approach to word prediction that is based on learning a representation for each word as a function of words and linguistics predicates in its context.
objective_label: This approach raises a few new questions that we address.
method_label: First, in order to learn good word representations it is necessary to use an expressive representation of the context.
method_label: We present a way that uses external knowledge to generate expressive context representations, along with a learning method capable of handling the large number of features generated this way that can, potentially, contribute to each prediction.
method_label: Second, since the number of words ``competing'' for each prediction is large, there is a need to ``focus the attention'' on a smaller subset of these.
method_label: We exhibit the contribution of a ``focus of attention'' mechanism to the performance of the word predictor.
result_label: Finally, we describe a large scale experimental study in which the approach presented is shown to yield significant improvements in word prediction tasks.

===================================
paper_id: 653078; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Unsupervised word segmentation and lexicon discovery using acoustic word embeddings
ABSTRACT: background_label: In settings where only unlabelled speech data is available, speech technology needs to be developed without transcriptions, pronunciation dictionaries, or language modelling text.
background_label: A similar problem is faced when modelling infant language acquisition.
background_label: In these cases, categorical linguistic structure needs to be discovered directly from speech audio.
method_label: We present a novel unsupervised Bayesian model that segments unlabelled speech and clusters the segments into hypothesized word groupings.
method_label: The result is a complete unsupervised tokenization of the input speech in terms of discovered word types.
method_label: In our approach, a potential word segment (of arbitrary length) is embedded in a fixed-dimensional acoustic vector space.
method_label: The model, implemented as a Gibbs sampler, then builds a whole-word acoustic model in this space while jointly performing segmentation.
method_label: We report word error rates in a small-vocabulary connected digit recognition task by mapping the unsupervised decoded output to ground truth transcriptions.
result_label: The model achieves around 20% error rate, outperforming a previous HMM-based system by about 10% absolute.
result_label: Moreover, in contrast to the baseline, our model does not require a pre-specified vocabulary size.

===================================
paper_id: 1428702; YEAR: 2011
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Learning Word Vectors for Sentiment Analysis
ABSTRACT: background_label: AbstractUnsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks.
method_label: We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term-document information as well as rich sentiment content.
method_label: The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations.
method_label: We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g.
method_label: star ratings).
result_label: We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification.
result_label: We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.

===================================
paper_id: 3265361; YEAR: 1997
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: cited - title_tfidf - abs_tfidf - title_tfidfcbow200 - title_cbow200 - abs_cbow200 - specter - abs_tfidfcbow200
TITLE: "I don't believe in word senses"
ABSTRACT: background_label: Word sense disambiguation assumes word senses.
background_label: Within the lexicography and linguistics literature, they are known to be very slippery entities.
objective_label: The paper looks at problems with existing accounts of `word sense' and describes the various kinds of ways in which a word's meaning can deviate from its core meaning.
method_label: An analysis is presented in which word senses are abstractions from clusters of corpus citations, in accordance with current lexicographic practice.
method_label: The corpus citations, not the word senses, are the basic objects in the ontology.
method_label: The corpus citations will be clustered into senses according to the purposes of whoever or whatever does the clustering.
method_label: In the absence of such purposes, word senses do not exist.
method_label: Word sense disambiguation also needs a set of word senses to disambiguate between.
method_label: In most recent work, the set has been taken from a general-purpose lexical resource, with the assumption that the lexical resource describes the word senses of English/French/..., between which NLP applications will need to disambiguate.
result_label: The implication of the paper is, by contrast, that word senses exist only relative to a task.

===================================
paper_id: 12087925; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidf - abs_cbow200 - abs_tfidfcbow200
TITLE: Big Data Small Data, In Domain Out-of Domain, Known Word Unknown Word: The Impact of Word Representation on Sequence Labelling Tasks
ABSTRACT: background_label: Word embeddings -- distributed word representations that can be learned from unlabelled data -- have been shown to have high utility in many natural language processing applications.
objective_label: In this paper, we perform an extrinsic evaluation of five popular word embedding methods in the context of four sequence labelling tasks: POS-tagging, syntactic chunking, NER and MWE identification.
objective_label: A particular focus of the paper is analysing the effects of task-based updating of word representations.
method_label: We show that when using word embeddings as features, as few as several hundred training instances are sufficient to achieve competitive results, and that word embeddings lead to improvements over OOV words and out of domain.
result_label: Perhaps more surprisingly, our results indicate there is little difference between the different word embedding methods, and that simple Brown clusters are often competitive with word embeddings across all tasks we consider.

===================================
paper_id: 28121799; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter - abs_tfidfcbow200
TITLE: A Simple Regularization-based Algorithm for Learning Cross-Domain Word Embeddings
ABSTRACT: background_label: Learning word embeddings has received a significant amount of attention recently.
background_label: Often, word embeddings are learned in an unsupervised manner from a large collection of text.
background_label: The genre of the text typically plays an important role in the effectiveness of the resulting embeddings.
background_label: How to effectively train word embedding models using data from different domains remains a problem that is underexplored.
method_label: In this paper, we present a simple yet effective method for learning word embeddings based on text from different domains.
result_label: We demonstrate the effectiveness of our approach through extensive experiments on various down-stream NLP tasks.

===================================
paper_id: 15797562; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited - title_tfidf - abs_tfidf - title_tfidfcbow200 - title_cbow200 - abs_cbow200 - specter - abs_tfidfcbow200
TITLE: A Word Embedding Approach to Identifying Verb-Noun Idiomatic Combinations
ABSTRACT: background_label: AbstractVerb-noun idiomatic combinations (VNICs) are idioms consisting of a verb with a noun in its direct object position.
background_label: Usages of these expressions can be ambiguous between an idiomatic usage and a literal combination.
objective_label: In this paper we propose supervised and unsupervised approaches, based on word embeddings, to identifying token instances of VNICs.
method_label: Our proposed supervised and unsupervised approaches perform better than the supervised and unsupervised approaches of Fazly et al.
result_label: (2009) , respectively.

===================================
paper_id: 217774; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf - abs_tfidf - title_tfidfcbow200 - title_cbow200
TITLE: An Autoencoder Approach to Learning Bilingual Word Representations
ABSTRACT: background_label: Cross-language learning allows us to use training data from one language to build models for a different language.
background_label: Many approaches to bilingual learning require that we have word-level alignment of sentences from parallel corpora.
method_label: In this work we explore the use of autoencoder-based methods for cross-language learning of vectorial word representations that are aligned between two languages, while not relying on word-level alignments.
method_label: We show that by simply learning to reconstruct the bag-of-words representations of aligned sentences, within and between languages, we can in fact learn high-quality representations and do without word alignments.
method_label: Since training autoencoders on word observations presents certain computational issues, we propose and compare different variations adapted to this setting.
method_label: We also propose an explicit correlation maximizing regularizer that leads to significant improvement in the performance.
method_label: We empirically investigate the success of our approach on the problem of cross-language test classification, where a classifier trained on a given language (e.g., English) must learn to generalize to a different language (e.g., German).
result_label: These experiments demonstrate that our approaches are competitive with the state-of-the-art, achieving up to 10-14 percentage point improvements over the best reported results on this task.

===================================
paper_id: 2857704; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_tfidf
TITLE: Topic Discovery for Short Texts Using Word Embeddings
ABSTRACT: background_label: Discovering topics in short texts, such as news titles and tweets, has become an important task for many content analysis applications.
background_label: However, due to the lack of rich context information in short texts, the performance of conventional topic models on short texts is usually unsatisfying.
objective_label: In this paper, we propose a novel topic model for short text corpus using word embeddings.
method_label: Continuous space word embeddings, which is proven effective at capturing regularities in language, is incorporated into our model to provide additional semantics.
method_label: Thus we model each short document as a Gaussian topic over word embeddings in the vector space.
method_label: In addition, considering that background words in a short text are usually not semantically related, we introduce a discrete background mode over word types to complement the continuous Gaussian topics.
result_label: We evaluate our model on news titles from data sources like abcnews, showing that our model is able to extract more coherent topics from short texts compared with the baseline methods and learn better topic representation for each short document.

===================================
paper_id: 23039976; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200 - specter - abs_tfidfcbow200
TITLE: Utility of General and Specific Word Embeddings for Classifying Translational Stages of Research
ABSTRACT: background_label: Conventional text classification models make a bag-of-words assumption reducing text into word occurrence counts per document.
background_label: Recent algorithms such as word2vec are capable of learning semantic meaning and similarity between words in an entirely unsupervised manner using a contextual window and doing so much faster than previous methods.
background_label: Each word is projected into vector space such that similar meaning words such as"strong"and"powerful"are projected into the same general Euclidean space.
method_label: Open questions about these embeddings include their utility across classification tasks and the optimal properties and source of documents to construct broadly functional embeddings.
result_label: In this work, we demonstrate the usefulness of pre-trained embeddings for classification in our task and demonstrate that custom word embeddings, built in the domain and for the tasks, can improve performance over word embeddings learnt on more general data including news articles or Wikipedia.

===================================
paper_id: 47021019; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Entropy-Based Subword Mining with an Application to Word Embeddings
ABSTRACT: background_label: AbstractRecent literature has shown a wide variety of benefits to mapping traditional onehot representations of words and phrases to lower-dimensional real-valued vectors known as word embeddings.
background_label: Traditionally, most word embedding algorithms treat each word as the finest meaningful semantic granularity and perform embedding by learning distinct embedding vectors for each word.
background_label: Contrary to this line of thought, technical domains such as scientific and medical literature compose words from subword structures such as prefixes, suffixes, and root-words as well as compound words.
background_label: Treating individual words as the finest-granularity unit discards meaningful shared semantic structure between words sharing substructures.
method_label: This not only leads to poor embeddings for text corpora that have longtail distributions, but also heuristic methods for handling out-of-vocabulary words.
method_label: In this paper we propose SubwordMine, an entropybased subword mining algorithm that is fast, unsupervised, and fully data-driven.
method_label: We show that this allows for great cross-domain performance in identifying semantically meaningful subwords.
result_label: We then investigate utilizing the mined subwords within the FastText embedding model and compare performance of the learned representations in a downstream language modeling task.

===================================
paper_id: 16086818; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Learning Discriminative Representations for Semantic Cross Media Retrieval
ABSTRACT: background_label: Heterogeneous gap among different modalities emerges as one of the critical issues in modern AI problems.
background_label: Unlike traditional uni-modal cases, where raw features are extracted and directly measured, the heterogeneous nature of cross modal tasks requires the intrinsic semantic representation to be compared in a unified framework.
objective_label: This paper studies the learning of different representations that can be retrieved across different modality contents.
method_label: A novel approach for mining cross-modal representations is proposed by incorporating explicit linear semantic projecting in Hilbert space.
method_label: The insight is that the discriminative structures of different modality data can be linearly represented in appropriate high dimension Hilbert spaces, where linear operations can be used to approximate nonlinear decisions in the original spaces.
method_label: As a result, an efficient linear semantic down mapping is jointly learned for multimodal data, leading to a common space where they can be compared.
method_label: The mechanism of"feature up-lifting and down-projecting"works seamlessly as a whole, which accomplishes crossmodal retrieval tasks very well.
method_label: The proposed method, named as shared discriminative semantic representation learning (\textbf{SDSRL}), is tested on two public multimodal dataset for both within- and inter- modal retrieval.
result_label: The experiments demonstrate that it outperforms several state-of-the-art methods in most scenarios.

===================================
paper_id: 53079155; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: specter - abs_tfidf - abs_tfidfcbow200
TITLE: Quantifying Context Overlap for Training Word Embeddings
ABSTRACT: background_label: AbstractMost models for learning word embeddings are trained based on the context information of words, more precisely first order cooccurrence relations.
method_label: In this paper, a metric is designed to estimate second order cooccurrence relations based on context overlap.
method_label: The estimated values are further used as the augmented data to enhance the learning of word embeddings by joint training with existing neural word embedding models.
result_label: Experimental results show that better word vectors can be obtained for word similarity tasks and some downstream NLP tasks by the enhanced approach.

===================================
paper_id: 201124468; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_cbow200
TITLE: Parsimonious Morpheme Segmentation with an Application to Enriching Word Embeddings
ABSTRACT: background_label: Traditionally, many text-mining tasks treat individual word-tokens as the finest meaningful semantic granularity.
background_label: However, in many languages and specialized corpora, words are composed by concatenating semantically meaningful subword structures.
background_label: Word-level analysis cannot leverage the semantic information present in such subword structures.
background_label: With regard to word embedding techniques, this leads to not only poor embeddings for infrequent words in long-tailed text corpora but also weak capabilities for handling out-of-vocabulary words.
objective_label: In this paper we propose MorphMine for unsupervised morpheme segmentation.
method_label: MorphMine applies a parsimony criterion to hierarchically segment words into the fewest number of morphemes at each level of the hierarchy.
method_label: This leads to longer shared morphemes at each level of segmentation.
method_label: Experiments show that MorphMine segments words in a variety of languages into human-verified morphemes.
result_label: Additionally, we experimentally demonstrate that utilizing MorphMine morphemes to enrich word embeddings consistently improves embedding quality on a variety of of embedding evaluations and a downstream language modeling task.

===================================
paper_id: 7934949; YEAR: 2004
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited - title_tfidf - abs_tfidf - title_tfidfcbow200 - title_cbow200 - abs_cbow200 - specter - abs_tfidfcbow200
TITLE: Hierarchical Dirichlet processes
ABSTRACT: background_label: We consider problems involving groups of data where each observation within a group is a draw from a mixture model and where it is desirable to share mixture components between groups.
background_label: We assume that the number of mixture components is unknown a priori and is to be inferred from the data.
background_label: In this setting it is natural to consider sets of Dirichlet processes, one for each group, where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of mixture components within each group.
method_label: Given our desire to tie the mixture models in the various groups, we consider a hierarchical model, specifically one in which the base measure for the child Dirichlet processes is itself distributed according to a Dirichlet process.
method_label: Such a base measure being discrete, the child Dirichlet processes necessarily share atoms.
result_label: Thus, as desired, the mixture models in the different groups necessarily share mixture components.
result_label: We discuss representations of hierarchical Dirichlet processes ...

===================================
paper_id: 18471199; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf - title_tfidfcbow200 - title_cbow200
TITLE: Tweet Topic Classification Using Distributed Language Representations
ABSTRACT: background_label: Many classification tasks on short text, such as tweet, fail to achieve high accuracy due to data sparseness.
objective_label: One approach to solving this problem is to enrich the context of data by using external data sources, or distributed language representations trained on huge amount of data.
method_label: In this paper, we present several tweet topic classification methods by exploiting different types of data: tweet text, tweet text plus entity knowledge base, word embeddings derived from tweet text, distributed representations of tweets, and topical word embeddings.
method_label: The word embedding, topical word embedding and sentence representation models are generated from billions of words from tweets without supervision.
result_label: To the best of our knowledge, this is the first study of applying distributed language representations to tweet topic classification task.

===================================
paper_id: 6618571; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Combining Language and Vision with a Multimodal Skip-gram Model
ABSTRACT: background_label: We extend the SKIP-GRAM model of Mikolov et al.
background_label: (2013a) by taking visual information into account.
background_label: Like SKIP-GRAM, our multimodal models (MMSKIP-GRAM) build vector-based word representations by learning to predict linguistic contexts in text corpora.
method_label: However, for a restricted set of words, the models are also exposed to visual representations of the objects they denote (extracted from natural images), and must predict linguistic and visual features jointly.
method_label: The MMSKIP-GRAM models achieve good performance on a variety of semantic benchmarks.
method_label: Moreover, since they propagate visual information to all words, we use them to improve image labeling and retrieval in the zero-shot setup, where the test concepts are never seen during model training.
result_label: Finally, the MMSKIP-GRAM models discover intriguing visual properties of abstract words, paving the way to realistic implementations of embodied theories of meaning.

===================================
paper_id: 8961026; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter - abs_tfidf - abs_tfidfcbow200
TITLE: Comparing Word Representations for Implicit Discourse Relation Classification
ABSTRACT: background_label: This paper presents a detailed comparative framework for assessing the usefulness of unsupervised word representations for identifying so-called implicit discourse relations.
method_label: Specifically, we compare standard one-hot word pair representations against low-dimensional ones based on Brown clusters and word embeddings.
method_label: We also consider various word vector combination schemes for deriving discourse segment representations from word vectors, and compare representations based either on all words or limited to head words.
result_label: Our main finding is that denser representations systematically outperform sparser ones and give state-of-the-art performance or above without the need for additional hand-crafted features.

===================================
paper_id: 6633984; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf - title_tfidfcbow200 - title_cbow200 - specter
TITLE: Topic2Vec: Learning Distributed Representations of Topics
ABSTRACT: background_label: Latent Dirichlet Allocation (LDA) mining thematic structure of documents plays an important role in nature language processing and machine learning areas.
background_label: However, the probability distribution from LDA only describes the statistical relationship of occurrences in the corpus and usually in practice, probability is not the best choice for feature representations.
method_label: Recently, embedding methods have been proposed to represent words and documents by learning essential concepts and representations, such as Word2Vec and Doc2Vec.
method_label: The embedded representations have shown more effectiveness than LDA-style representations in many tasks.
method_label: In this paper, we propose the Topic2Vec approach which can learn topic representations in the same semantic vector space with words, as an alternative to probability.
result_label: The experimental results show that Topic2Vec achieves interesting and meaningful results.

===================================
paper_id: 298504; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf - title_tfidfcbow200 - title_cbow200
TITLE: Sound-Word2Vec: Learning Word Representations Grounded in Sounds
ABSTRACT: background_label: To be able to interact better with humans, it is crucial for machines to understand sound - a primary modality of human perception.
background_label: Previous works have used sound to learn embeddings for improved generic textual similarity assessment.
background_label: In this work, we treat sound as a first-class citizen, studying downstream textual tasks which require aural grounding.
objective_label: To this end, we propose sound-word2vec - a new embedding scheme that learns specialized word embeddings grounded in sounds.
method_label: For example, we learn that two seemingly (semantically) unrelated concepts, like leaves and paper are similar due to the similar rustling sounds they make.
method_label: Our embeddings prove useful in textual tasks requiring aural reasoning like text-based sound retrieval and discovering foley sound effects (used in movies).
result_label: Moreover, our embedding space captures interesting dependencies between words and onomatopoeia and outperforms prior work on aurally-relevant word relatedness datasets such as AMEN and ASLex.

===================================
paper_id: 7826531; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - specter
TITLE: Distributional Representations of Words for Short Text Classification
ABSTRACT: background_label: Traditional supervised learning approaches to common NLP tasks depend heavily on manual annotation, which is labor intensive and time consuming, and often suffer from data sparseness.
objective_label: In this paper we show how to mitigate the problems in short text classification (STC) through word embeddings ‐ distributional representations of words learned from large unlabeled data.
objective_label: The word embeddings are trained from the entire English Wikipedia text.
method_label: We assume that a short text document is a specific sample of one distribution in a Bayesian framework.
method_label: A Gaussian process approach is used to model the distribution of words.
method_label: The task of classification becomes a simple problem of selecting the most probable Gaussian distribution.
method_label: This approach is compared with those based on the classical maximum entropy (MaxEnt) model and the Latent Dirichlet Allocation (LDA) approach.
result_label: Our approach achieved better performance and also showed advantages in dealing with unseen words.

===================================
paper_id: 126584; YEAR: 2007
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited - title_tfidf - abs_tfidf - title_tfidfcbow200 - title_cbow200 - abs_cbow200 - specter - abs_tfidfcbow200
TITLE: SemEval-2007 Task 10: English Lexical Substitution Task
ABSTRACT: background_label: In this paper we describe the English Lexical Substitution task for SemEval.
background_label: In the task, annotators and systems find an alternative substitute word or phrase for a target word in context.
method_label: The task involves both finding the synonyms and disambiguating the context.
background_label: Participating systems are free to use any lexical resource.
result_label: There is a subtask which requires identifying cases where the word is functioning as part of a multiword in the sentence and detecting what that multiword is.

===================================
paper_id: 16730027; YEAR: 2011
adju relevance: Irrelevant (0)
difference: 2; annotator4: 2; annotator3: 0
sources: specter
TITLE: Identifying Word Translations from Comparable Corpora Using Latent Topic Models
ABSTRACT: background_label: AbstractA topic model outputs a set of multinomial distributions over words for each topic.
objective_label: In this paper, we investigate the value of bilingual topic models, i.e., a bilingual Latent Dirichlet Allocation model for finding translations of terms in comparable corpora without using any linguistic resources.
method_label: Experiments on a document-aligned English-Italian Wikipedia corpus confirm that the developed methods which only use knowledge from word-topic distributions outperform methods based on similarity measures in the original word-document space.
result_label: The best results, obtained by combining knowledge from wordtopic distributions with similarity measures in the original space, are also reported.

===================================
paper_id: 174799702; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Pooled Contextualized Embeddings for Named Entity Recognition
ABSTRACT: background_label: AbstractContextual string embeddings are a recent type of contextualized word embedding that were shown to yield state-of-the-art results when utilized in a range of sequence labeling tasks.
background_label: They are based on character-level language models which treat text as distributions over characters and are capable of generating embeddings for any string of characters within any textual context.
background_label: However, such purely character-based approaches struggle to produce meaningful embeddings if a rare string is used in a underspecified context.
method_label: To address this drawback, we propose a method in which we dynamically aggregate contextualized embeddings of each unique string that we encounter.
method_label: We then use a pooling operation to distill a global word representation from all contextualized instances.
result_label: We evaluate these pooled contextualized embeddings on common named entity recognition (NER) tasks such as CoNLL-03 and WNUT and show that our approach significantly improves the state-of-the-art for NER.
result_label: We make all code and pre-trained models available to the research community for use and reproduction.

===================================
paper_id: 57880430; YEAR: 2011
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: Cost-Sensitive Machine Learning
ABSTRACT: background_label: In machine learning applications, practitioners must take into account the cost associated with the algorithm.
background_label: These costs include: Cost of acquiring training data Cost of data annotation/labeling and cleaning Computational cost for model fitting, validation, and testing Cost of collecting features/attributes for test data Cost of user feedback collection Cost of incorrect prediction/classification Cost-Sensitive Machine Learning is one of the first books to provide an overview of the current research efforts and problems in this area.
method_label: It discusses real-world applications that incorporate the cost of learning into the modeling process.
method_label: The first part of the book presents the theoretical underpinnings of cost-sensitive machine learning.
method_label: It describes well-established machine learning approaches for reducing data acquisition costs during training as well as approaches for reducing costs when systems must make predictions for new samples.
method_label: The second part covers real-world applications that effectively trade off different types of costs.
method_label: These applications not only use traditional machine learning approaches, but they also incorporate cutting-edge research that advances beyond the constraining assumptions by analyzing the application needs from first principles.
method_label: Spurring further research on several open problems, this volume highlights the often implicit assumptions in machine learning techniques that were not fully understood in the past.
result_label: The book also illustrates the commercial importance of cost-sensitive machine learning through its coverage of the rapid application developments made by leading companies and academic research labs.

===================================
paper_id: 492338; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: Cost Sensitive Learning of Deep Feature Representations from Imbalanced Data
ABSTRACT: background_label: Class imbalance is a common problem in the case of real-world object detection and classification tasks.
background_label: Data of some classes is abundant making them an over-represented majority, and data of other classes is scarce, making them an under-represented minority.
background_label: This imbalance makes it challenging for a classifier to appropriately learn the discriminating boundaries of the majority and minority classes.
objective_label: In this work, we propose a cost sensitive deep neural network which can automatically learn robust feature representations for both the majority and minority classes.
method_label: During training, our learning procedure jointly optimizes the class dependent costs and the neural network parameters.
method_label: The proposed approach is applicable to both binary and multi-class problems without any modification.
method_label: Moreover, as opposed to data level approaches, we do not alter the original data distribution which results in a lower computational cost during the training process.
result_label: We report the results of our experiments on six major image classification datasets and show that the proposed approach significantly outperforms the baseline algorithms.
result_label: Comparisons with popular data sampling techniques and cost sensitive classifiers demonstrate the superior performance of our proposed method.

===================================
paper_id: 129431; YEAR: 2002
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: Topic-sensitive PageRank
ABSTRACT: background_label: In the original PageRank algorithm for improving the ranking of search-query results, a single PageRank vector is computed, using the link structure of the Web, to capture the relative "importance" of Web pages, independent of any particular search query.
objective_label: To yield more accurate search results, we propose computing a set of PageRank vectors, biased using a set of representative topics, to capture more accurately the notion of importance with respect to a particular topic.
method_label: By using these (precomputed) biased PageRank vectors to generate query-specific importance scores for pages at query time, we show that we can generate more accurate rankings than with a single, generic PageRank vector.
method_label: For ordinary keyword search queries, we compute the topic-sensitive PageRank scores for pages satisfying the query using the topic of the query keywords.
result_label: For searches done in context (e.g., when the search query is performed by highlighting words in a Web page), we compute the topic-sensitive PageRank scores using the topic of the context in which the query appeared.

===================================
paper_id: 16470894; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: Cross-Lingual Semantic Similarity of Words as the Similarity of Their Semantic Word Responses
ABSTRACT: objective_label: AbstractWe propose a new approach to identifying semantically similar words across languages.
objective_label: The approach is based on an idea that two words in different languages are similar if they are likely to generate similar words (which includes both source and target language words) as their top semantic word responses.
method_label: Semantic word responding is a concept from cognitive science which addresses detecting most likely words that humans output as free word associations given some cue word.
method_label: The method consists of two main steps: (1) it utilizes a probabilistic multilingual topic model trained on comparable data to learn and quantify the semantic word responses, (2) it provides ranked lists of similar words according to the similarity of their semantic word response vectors.
result_label: We evaluate our approach in the task of bilingual lexicon extraction (BLE) for a variety of language pairs.
result_label: We show that in the cross-lingual settings without any language pair dependent knowledge the response-based method of similarity is more robust and outperforms current state-of-the art methods that directly operate in the semantic space of latent cross-lingual concepts/topics.

===================================
paper_id: 8264070; YEAR: 2012
adju relevance: Irrelevant (0)
difference: 1; annotator4: 0; annotator3: 1
sources: abs_tfidfcbow200 - abs_tfidf
TITLE: Concurrent Acquisition of Word Meaning and Lexical Categories
ABSTRACT: background_label: AbstractLearning the meaning of words from ambiguous and noisy context is a challenging task for language learners.
background_label: It has been suggested that children draw on syntactic cues such as lexical categories of words to constrain potential referents of words in a complex scene.
background_label: Although the acquisition of lexical categories should be interleaved with learning word meanings, it has not previously been modeled in that fashion.
objective_label: In this paper, we investigate the interplay of word learning and category induction by integrating an LDA-based word class learning module with a probabilistic word learning model.
result_label: Our results show that the incrementally induced word classes significantly improve word learning, and their contribution is comparable to that of manually assigned part of speech categories.

===================================
paper_id: 125692266; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: Learning network representations
ABSTRACT: background_label: Abstract In this review I present several representation learning methods, and discuss the latest advancements with emphasis in applications to network science.
background_label: Representation learning is a set of techniques that has the goal of efficiently mapping data structures into convenient latent spaces.
method_label: Either for dimensionality reduction or for gaining semantic content, this type of feature embeddings has demonstrated to be useful, for example, for node classification or link prediction tasks, among many other relevant applications to networks.
method_label: I provide a description of the state-of-the-art of network representation learning as well as a detailed account of the connections with other fields of study such as continuous word embeddings and deep learning architectures.
result_label: Finally, I provide a broad view of several applications of these techniques to networks in various domains.

===================================
paper_id: 11233717; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter
TITLE: SubGram: Extending Skip-gram Word Representation with Substrings
ABSTRACT: background_label: Skip-gram (word2vec) is a recent method for creating vector representations of words ("distributed word representations") using a neural network.
background_label: The representation gained popularity in various areas of natural language processing, because it seems to capture syntactic and semantic information about words without any explicit supervision in this respect.
method_label: We propose SubGram, a refinement of the Skip-gram model to consider also the word structure during the training process, achieving large gains on the Skip-gram original test set.

===================================
paper_id: 13599696; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidf - abs_tfidf - title_tfidfcbow200 - abs_tfidfcbow200
TITLE: Learning Multilingual Word Representations using a Bag-of-Words Autoencoder
ABSTRACT: background_label: Recent work on learning multilingual word representations usually relies on the use of word-level alignements (e.g.
background_label: infered with the help of GIZA++) between translated sentences, in order to align the word embeddings in different languages.
objective_label: In this workshop paper, we investigate an autoencoder model for learning multilingual word representations that does without such word-level alignements.
method_label: The autoencoder is trained to reconstruct the bag-of-word representation of given sentence from an encoded representation extracted from its translation.
result_label: We evaluate our approach on a multilingual document classification task, where labeled data is available only for one language (e.g.
result_label: English) while classification must be performed in a different language (e.g.
method_label: French).
result_label: In our experiments, we observe that our method compares favorably with a previously proposed method that exploits word-level alignments to learn word representations.

===================================
paper_id: 24412850; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Visual Exploration of Semantic Relationships in Neural Word Embeddings
ABSTRACT: background_label: Constructing distributed representations for words through neural language models and using the resulting vector spaces for analysis has become a crucial component of natural language processing (NLP).
background_label: However, despite their widespread application, little is known about the structure and properties of these spaces.
background_label: To gain insights into the relationship between words, the NLP community has begun to adapt high-dimensional visualization techniques.
background_label: In particular, researchers commonly use t-distributed stochastic neighbor embeddings (t-SNE) and principal component analysis (PCA) to create two-dimensional embeddings for assessing the overall structure and exploring linear relationships (e.g., word analogies), respectively.
background_label: Unfortunately, these techniques often produce mediocre or even misleading results and cannot address domain-specific visualization challenges that are crucial for understanding semantic relationships in word embeddings.
method_label: Here, we introduce new embedding techniques for visualizing semantic and syntactic analogies, and the corresponding tests to determine whether the resulting views capture salient structures.
method_label: Additionally, we introduce two novel views for a comprehensive study of analogy relationships.
method_label: Finally, we augment t-SNE embeddings to convey uncertainty information in order to allow a reliable interpretation.
result_label: Combined, the different views address a number of domain-specific tasks difficult to solve with existing tools.

===================================
paper_id: 7880109; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_tfidf - title_tfidfcbow200 - abs_cbow200
TITLE: Deep convolutional acoustic word embeddings using word-pair side information
ABSTRACT: background_label: Recent studies have been revisiting whole words as the basic modelling unit in speech recognition and query applications, instead of phonetic units.
background_label: Such whole-word segmental systems rely on a function that maps a variable-length speech segment to a vector in a fixed-dimensional space; the resulting acoustic word embeddings need to allow for accurate discrimination between different word types, directly in the embedding space.
method_label: We compare several old and new approaches in a word discrimination task.
method_label: Our best approach uses side information in the form of known word pairs to train a Siamese convolutional neural network (CNN): a pair of tied networks that take two speech segments as input and produce their embeddings, trained with a hinge loss that separates same-word pairs and different-word pairs by some margin.
method_label: A word classifier CNN performs similarly, but requires much stronger supervision.
result_label: Both types of CNNs yield large improvements over the best previously published results on the word discrimination task.

===================================
paper_id: 3204831; YEAR: 1996
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Unsupervised Learning of Word-Category Guessing Rules
ABSTRACT: background_label: Words unknown to the lexicon present a substantial problem to part-of-speech tagging.
objective_label: In this paper we present a technique for fully unsupervised statistical acquisition of rules which guess possible parts-of-speech for unknown words.
method_label: Three complementary sets of word-guessing rules are induced from the lexicon and a raw corpus: prefix morphological rules, suffix morphological rules and ending-guessing rules.
method_label: The learning was performed on the Brown Corpus data and rule-sets, with a highly competitive performance, were produced and compared with the state-of-the-art.

===================================
paper_id: 10705753; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: title_cbow200 - abs_tfidf - title_tfidfcbow200 - abs_tfidfcbow200
TITLE: Online Learning of Interpretable Word Embeddings
ABSTRACT: background_label: Word embeddings encode semantic meanings of words into low-dimension word vectors.
background_label: In most word embeddings, one cannot interpret the meanings of specific dimensions of those word vectors.
background_label: Nonnegative matrix factorization (NMF) has been proposed to learn interpretable word embeddings via non-negative constraints.
background_label: However, NMF methods suffer from scale and memory issue because they have to maintain a global matrix for learning.
method_label: To alleviate this challenge, we propose online learning of interpretable word embeddings from streaming text data.
result_label: Experiments show that our model consistently outperforms the state-of-the-art word embedding methods in both representation ability and interpretability.
other_label: The source code of this paper can be obtained from http: //github.com/skTim/OIWE.

===================================
paper_id: 16732830; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: title_tfidf - title_tfidfcbow200 - specter
TITLE: Topic Quality Metrics Based on Distributed Word Representations
ABSTRACT: background_label: Automated evaluation of topic quality remains an important unsolved problem in topic modeling and represents a major obstacle for development and evaluation of new topic models.
background_label: Previous attempts at the problem have been formulated as variations on the coherence and/or mutual information of top words in a topic.
result_label: In this work, we propose several new metrics for evaluating topic quality with the help of distributed word representations; our experiments suggest that the new metrics are a better match for human judgement, which is the gold standard in this case, than previously developed approaches.

===================================
paper_id: 7392978; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: cited - title_tfidf - abs_tfidf - title_tfidfcbow200 - title_cbow200 - abs_cbow200 - specter - abs_tfidfcbow200
TITLE: Problems With Evaluation of Word Embeddings Using Word Similarity Tasks
ABSTRACT: background_label: Lacking standardized extrinsic evaluation methods for vector representations of words, the NLP community has relied heavily on word similarity tasks as a proxy for intrinsic evaluation of word vectors.
background_label: Word similarity evaluation, which correlates the distance between vectors and human judgments of semantic similarity is attractive, because it is computationally inexpensive and fast.
objective_label: In this paper we present several problems associated with the evaluation of word vectors on word similarity datasets, and summarize existing solutions.
result_label: Our study suggests that the use of word similarity tasks for evaluation of word vectors is not sustainable and calls for further research on evaluation methods.

===================================
paper_id: 3047293; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter
TITLE: Learning Representations for Weakly Supervised Natural Language Processing Tasks
ABSTRACT: background_label: Finding the right representations for words is critical for building accurate NLP systems when domain-specific labeled data for the task is scarce.
objective_label: This article investigates novel techniques for extracting features from n-gram models, Hidden Markov Models, and other statistical language models, including a novel Partial Lattice Markov Random Field model.
result_label: Experiments on part-of-speech tagging and information extraction, among other tasks, indicate that features taken from statistical language models, in combination with more traditional features, outperform traditional representations alone, and that graphical model representations outperform n-gram models, especially on sparse and polysemous words.

===================================
paper_id: 1968500; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidf - title_tfidfcbow200 - abs_cbow200
TITLE: Learning Word Representations for Sentiment Analysis
ABSTRACT: background_label: Word embedding has been proven to be a useful model for various natural language processing tasks.
background_label: Traditional word embedding methods merely take into account word distributions independently from any specific tasks.
background_label: Hence, the resulting representations could be sub-optimal for a given task.
background_label: In the context of sentiment analysis, there are various types of prior knowledge available, e.g., sentiment labels of documents from available datasets or polarity values of words from sentiment lexicons.
method_label: We incorporate such prior sentiment information at both word level and document level in order to investigate the influence each word has on the sentiment label of both target word and context words.
result_label: By evaluating the performance of sentiment analysis in each category, we find the best way of incorporating prior sentiment information.
result_label: Experimental results on real-world datasets demonstrate that the word representations learnt by DLJT2 can significantly improve the sentiment analysis performance.
result_label: We prove that incorporating prior sentiment knowledge into the embedding process has the potential to learn better representations for sentiment analysis.

===================================
paper_id: 8455737; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200
TITLE: Convolutional Sentence Kernel from Word Embeddings for Short Text Categorization
ABSTRACT: background_label: This paper introduces a convolutional sentence kernel based on word embeddings.
background_label: Our kernel overcomes the sparsity issue that arises when classifying short documents or in case of little training data.
result_label: Experiments on six sentence datasets showed statistically significant higher accuracy over the standard linear kernel with ngram features and other proposed models.

===================================
paper_id: 18352657; YEAR: 2009
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter
TITLE: Visualizing Topics with Multi-Word Expressions
ABSTRACT: method_label: We describe a new method for visualizing topics, the distributions over terms that are automatically extracted from large text corpora using latent variable models.
method_label: Our method finds significant $n$-grams related to a topic, which are then used to help understand and interpret the underlying distribution.
method_label: Compared with the usual visualization, which simply lists the most probable topical terms, the multi-word expressions provide a better intuitive impression for what a topic is"about.
method_label: "Our approach is based on a language model of arbitrary length expressions, for which we develop a new methodology based on nested permutation tests to find significant phrases.
method_label: We show that this method outperforms the more standard use of $\chi^2$ and likelihood ratio tests.
result_label: We illustrate the topic presentations on corpora of scientific abstracts and news articles.

===================================
paper_id: 6758088; YEAR: 2012
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Inducing Crosslingual Distributed Representations of Words
ABSTRACT: background_label: ABSTRACTDistributed representations of words have proven extremely useful in numerous natural language processing tasks.
background_label: Their appeal is that they can help alleviate data sparsity problems common to supervised learning.
background_label: Methods for inducing these representations require only unlabeled language data, which are plentiful for many natural languages.
method_label: In this work, we induce distributed representations for a pair of languages jointly.
method_label: We treat it as a multitask learning problem where each task corresponds to a single word, and task relatedness is derived from co-occurrence statistics in bilingual parallel data.
method_label: These representations can be used for a number of crosslingual learning tasks, where a learner can be trained on annotations present in one language and applied to test data in another.
result_label: We show that our representations are informative by using them for crosslingual document classification, where classifiers trained on these representations substantially outperform strong baselines (e.g.
result_label: machine translation) when applied to a new language.

===================================
paper_id: 1365212; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Word-Entity Duet Representations for Document Ranking
ABSTRACT: background_label: This paper presents a word-entity duet framework for utilizing knowledge bases in ad-hoc retrieval.
background_label: In this work, the query and documents are modeled by word-based representations and entity-based representations.
method_label: Ranking features are generated by the interactions between the two representations, incorporating information from the word space, the entity space, and the cross-space connections through the knowledge graph.
method_label: To handle the uncertainties from the automatically constructed entity representations, an attention-based ranking model AttR-Duet is developed.
method_label: With back-propagation from ranking labels, the model learns simultaneously how to demote noisy entities and how to rank documents with the word-entity duet.
result_label: Evaluation results on TREC Web Track ad-hoc task demonstrate that all of the four-way interactions in the duet are useful, the attention mechanism successfully steers the model away from noisy entities, and together they significantly outperform both word-based and entity-based learning to rank systems.

===================================
paper_id: 27554628; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200 - abs_cbow200
TITLE: Training word embeddings for deep learning in biomedical text mining tasks
ABSTRACT: background_label: Most word embedding methods are proposed with general purpose which take a word as a basic unit and learn embeddings according to words' external contexts.
background_label: However, in biomedical text mining, there are many biomedical entities and syntactic chunks which contain rich domain information, and the semantic meaning of a word is also strongly related to those information.
method_label: Hence, we present a biomedical domain-specific word embedding model by incorporating stem, chunk and entity to train word embeddings.
method_label: We also present two deep learning architectures respectively for two biomedical text mining tasks, by which we evaluate our word embeddings and compare them with other models.
result_label: Experimental results show that our biomedical domain-specific word embeddings overall outperform other general-purpose word embeddings in these deep learning methods for biomedical text mining tasks.

===================================
paper_id: 3572919; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf - title_tfidf - abs_tfidfcbow200
TITLE: Improving Word Alignment using Word Similarity
ABSTRACT: background_label: We show that semantic relationships can be used to improve word alignment, in addition to the lexical and syntactic features that are typically used.
method_label: In this paper, we present a method based on a neural network to automatically derive word similarity from monolingual data.
method_label: We present an extension to word alignment models that exploits word similarity.
result_label: Our experiments, in both large-scale and resourcelimited settings, show improvements in word alignment tasks as well as translation tasks.

===================================
paper_id: 5258986; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200
TITLE: Deconvolutional Paragraph Representation Learning
ABSTRACT: background_label: Learning latent representations from long text sequences is an important first step in many natural language processing applications.
background_label: Recurrent Neural Networks (RNNs) have become a cornerstone for this challenging task.
background_label: However, the quality of sentences during RNN-based decoding (reconstruction) decreases with the length of the text.
objective_label: We propose a sequence-to-sequence, purely convolutional and deconvolutional autoencoding framework that is free of the above issue, while also being computationally efficient.
method_label: The proposed method is simple, easy to implement and can be leveraged as a building block for many applications.
method_label: We show empirically that compared to RNNs, our framework is better at reconstructing and correcting long paragraphs.
result_label: Quantitative evaluation on semi-supervised text classification and summarization tasks demonstrate the potential for better utilization of long unlabeled text data.

===================================
paper_id: 20088543; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidfcbow200 - abs_tfidf
TITLE: From Image to Text Classification: A Novel Approach based on Clustering Word Embeddings
ABSTRACT: objective_label: In this paper, we propose a novel approach for text classification based on clustering word embeddings, inspired by the bag of visual words model, which is widely used in computer vision.
method_label: After each word in a collection of documents is represented as word vector using a pre-trained word embeddings model, a k-means algorithm is applied on the word vectors in order to obtain a fixed-size set of clusters.
method_label: The centroid of each cluster is interpreted as a super word embedding that embodies all the semantically related word vectors in a certain region of the embedding space.
method_label: Every embedded word in the collection of documents is then assigned to the nearest cluster centroid.
method_label: In the end, each document is represented as a bag of super word embeddings by computing the frequency of each super word embedding in the respective document.
method_label: We also diverge from the idea of building a single vocabulary for the entire collection of documents, and propose to build class-specific vocabularies for better performance.
result_label: Using this kind of representation, we report results on two text mining tasks, namely text categorization by topic and polarity classification.
result_label: On both tasks, our model yields better performance than the standard bag of words.

===================================
paper_id: 26442775; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Regularized and Retrofitted models for Learning Sentence Representation with Context
ABSTRACT: background_label: Vector representation of sentences is important for many text processing tasks that involve classifying, clustering, or ranking sentences.
background_label: For solving these tasks, bag-of-word based representation has been used for a long time.
background_label: In recent years, distributed representation of sentences learned by neural models from unlabeled data has been shown to outperform traditional bag-of-words representations.
method_label: However, most existing methods belonging to the neural models consider only the content of a sentence, and disregard its relations with other sentences in the context.
method_label: In this paper, we first characterize two types of contexts depending on their scope and utility.
method_label: We then propose two approaches to incorporate contextual information into content-based models.
method_label: We evaluate our sentence representation models in a setup, where context is available to infer sentence vectors.
result_label: Experimental results demonstrate that our proposed models outshine existing models on three fundamental tasks, such as, classifying, clustering, and ranking sentences.

===================================
paper_id: 27280855; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: title_tfidf - title_tfidfcbow200 - title_cbow200
TITLE: Learning Word Representations with Regularization from Prior Knowledge
ABSTRACT: background_label: AbstractConventional word embeddings are trained with specific criteria (e.g., based on language modeling or co-occurrence) inside a single information source, disregarding the opportunity for further calibration using external knowledge.
objective_label: This paper presents a unified framework that leverages pre-learned or external priors, in the form of a regularizer, for enhancing conventional language model-based embedding learning.
method_label: We consider two types of regularizers.
method_label: The first type is derived from topic distribution by running latent Dirichlet allocation on unlabeled data.
method_label: The second type is based on dictionaries that are created with human annotation efforts.
method_label: To effectively learn with the regularizers, we propose a novel data structure, trajectory softmax, in this paper.
method_label: The resulting embeddings are evaluated by word similarity and sentiment classification.
result_label: Experimental results show that our learning framework with regularization from prior knowledge improves embedding quality across multiple datasets, compared to a diverse collection of baseline methods.

===================================
paper_id: 21675189; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: title_cbow200 - title_tfidf - title_tfidfcbow200
TITLE: Unsupervised Learning of Style-sensitive Word Vectors
ABSTRACT: objective_label: This paper presents the first study aimed at capturing stylistic similarity between words in an unsupervised manner.
method_label: We propose extending the continuous bag of words (CBOW) model (Mikolov et al., 2013) to learn style-sensitive word vectors using a wider context window under the assumption that the style of all the words in an utterance is consistent.
method_label: In addition, we introduce a novel task to predict lexical stylistic similarity and to create a benchmark dataset for this task.
result_label: Our experiment with this dataset supports our assumption and demonstrates that the proposed extensions contribute to the acquisition of style-sensitive word embeddings.

===================================
paper_id: 3513086; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Unsupervised Learning of Semantic Audio Representations
ABSTRACT: background_label: Even in the absence of any explicit semantic annotation, vast collections of audio recordings provide valuable information for learning the categorical structure of sounds.
background_label: We consider several class-agnostic semantic constraints that apply to unlabeled nonspeech audio: (i) noise and translations in time do not change the underlying sound category, (ii) a mixture of two sound events inherits the categories of the constituents, and (iii) the categories of events in close temporal proximity are likely to be the same or related.
background_label: Without labels to ground them, these constraints are incompatible with classification loss functions.
method_label: However, they may still be leveraged to identify geometric inequalities needed for triplet loss-based training of convolutional neural networks.
result_label: The result is low-dimensional embeddings of the input spectrograms that recover 41% and 84% of the performance of their fully-supervised counterparts when applied to downstream query-by-example sound retrieval and sound event classification tasks, respectively.
result_label: Moreover, in limited-supervision settings, our unsupervised embeddings double the state-of-the-art classification performance.

===================================
paper_id: 9549307; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: title_tfidf - title_tfidfcbow200 - title_cbow200 - abs_cbow200
TITLE: Learning Word Representations from Relational Graphs
ABSTRACT: background_label: Attributes of words and relations between two words are central to numerous tasks in Artificial Intelligence such as knowledge representation, similarity measurement, and analogy detection.
background_label: Often when two words share one or more attributes in common, they are connected by some semantic relations.
background_label: On the other hand, if there are numerous semantic relations between two words, we can expect some of the attributes of one of the words to be inherited by the other.
method_label: Motivated by this close connection between attributes and relations, given a relational graph in which words are inter- connected via numerous semantic relations, we propose a method to learn a latent representation for the individual words.
method_label: The proposed method considers not only the co-occurrences of words as done by existing approaches for word representation learning, but also the semantic relations in which two words co-occur.
method_label: To evaluate the accuracy of the word representations learnt using the proposed method, we use the learnt word representations to solve semantic word analogy problems.
result_label: Our experimental results show that it is possible to learn better word representations by using semantic semantics between words.

===================================
paper_id: 186206883; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 2; annotator4: 2; annotator3: 0
sources: specter
TITLE: Putting words in context: LSTM language models and lexical ambiguity
ABSTRACT: background_label: In neural network models of language, words are commonly represented using context-invariant representations (word embeddings) which are then put in context in the hidden layers.
background_label: Since words are often ambiguous, representing the contextually relevant information is not trivial.
method_label: We investigate how an LSTM language model deals with lexical ambiguity in English, designing a method to probe its hidden representations for lexical and contextual information about words.
result_label: We find that both types of information are represented to a large extent, but also that there is room for improvement for contextual information.

===================================
paper_id: 202542757; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 1; annotator4: 0; annotator3: 1
sources: title_tfidf - title_tfidfcbow200 - title_cbow200
TITLE: Knowledge Enhanced Contextual Word Representations
ABSTRACT: background_label: Contextual word representations, typically trained on unstructured, unlabeled text, do not contain any explicit grounding to real world entities and are often unable to remember facts about those entities.
method_label: We propose a general method to embed multiple knowledge bases (KBs) into large scale models, and thereby enhance their representations with structured, human-curated knowledge.
method_label: For each KB, we first use an integrated entity linker to retrieve relevant entity embeddings, then update contextual word representations via a form of word-to-entity attention.
method_label: In contrast to previous approaches, the entity linkers and self-supervised language modeling objective are jointly trained end-to-end in a multitask setting that combines a small amount of entity linking supervision with a large amount of raw text.
result_label: After integrating WordNet and a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and word sense disambiguation.
result_label: KnowBert's runtime is comparable to BERT's and it scales to large KBs.

===================================
paper_id: 6157443; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Polyglot Neural Language Models: A Case Study in Cross-Lingual Phonetic Representation Learning
ABSTRACT: background_label: We introduce polyglot language models, recurrent neural network models trained to predict symbol sequences in many different languages using shared representations of symbols and conditioning on typological information about the language to be predicted.
method_label: We apply these to the problem of modeling phone sequences---a domain in which universal symbol inventories and cross-linguistically shared feature representations are a natural fit.
result_label: Intrinsic evaluation on held-out perplexity, qualitative analysis of the learned representations, and extrinsic evaluation in two downstream applications that make use of phonetic features show (i) that polyglot models better generalize to held-out data than comparable monolingual models and (ii) that polyglot phonetic feature representations are of higher quality than those learned monolingually.

===================================
paper_id: 15666681; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidfcbow200
TITLE: Intent detection using semantically enriched word embeddings
ABSTRACT: background_label: State-of-the-art targeted language understanding systems rely on deep learning methods using 1-hot word vectors or off-the-shelf word embeddings.
background_label: While word embeddings can be enriched with information from semantic lexicons (such as WordNet and PPDB) to improve their semantic representation, most previous research on word-embedding enriching has focused on improving intrinsic word-level tasks such as word analogy and antonym detection.
objective_label: In this work, we enrich word embeddings to force semantically similar or dissimilar words to be closer or farther away in the embedding space to improve the performance of an extrinsic task, namely, intent detection for spoken language understanding.
method_label: We utilize several semantic lexicons, such as WordNet, PPDB, and Macmillan Dictionary to enrich the word embeddings and later use them as initial representation of words for intent detection.
method_label: Thus, we enrich embeddings outside the neural network as opposed to learning the embeddings within the network, and, on top of the embeddings, build bidirectional LSTM for intent detection.
result_label: Our experiments on ATIS and a real log dataset from Microsoft Cortana show that word embeddings enriched with semantic lexicons can improve intent detection.

===================================
paper_id: 17182082; YEAR: 2008
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_tfidf
TITLE: Permutations as a Means to Encode Order in Word Space
ABSTRACT: background_label: We show that sequence information can be encoded into high-dimensional fixed-width vectors using permutations of coordinates.
background_label: Computational models of language often represent words with high-dimensional semantic vectors compiled from word-use statistics.
background_label: A word's semantic vector usually encodes the contexts in which the word appears in a large body of text but ignores word order.
background_label: However, word order often signals a word's grammatical role in a sentence and thus tells of the word's meaning.
method_label: Jones and Mewhort (2007) show that word order can be included in the semantic vectors using holographic reduced representation and convolution.
result_label: We show here that the order information can be captured also by permuting of vector coordinates, thus providing a general and computationally light alternative to convolution.

===================================
paper_id: 15045279; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200 - abs_tfidf
TITLE: sense2vec - A Fast and Accurate Method for Word Sense Disambiguation In Neural Word Embeddings
ABSTRACT: background_label: Neural word representations have proven useful in Natural Language Processing (NLP) tasks due to their ability to efficiently model complex semantic and syntactic word relationships.
background_label: However, most techniques model only one representation per word, despite the fact that a single word can have multiple meanings or"senses".
background_label: Some techniques model words by using multiple vectors that are clustered based on context.
background_label: However, recent neural approaches rarely focus on the application to a consuming NLP algorithm.
background_label: Furthermore, the training process of recent word-sense models is expensive relative to single-sense embedding processes.
method_label: This paper presents a novel approach which addresses these concerns by modeling multiple embeddings for each word based on supervised disambiguation, which provides a fast and accurate way for a consuming NLP model to select a sense-disambiguated embedding.
method_label: We demonstrate that these embeddings can disambiguate both contrastive senses such as nominal and verbal senses as well as nuanced senses such as sarcasm.
result_label: We further evaluate Part-of-Speech disambiguated embeddings on neural dependency parsing, yielding a greater than 8% average error reduction in unlabeled attachment scores across 6 languages.

===================================
paper_id: 36117198; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: DeepMind_Commentary
ABSTRACT: background_label: We agree with Lake and colleagues on their list of key ingredients for building humanlike intelligence, including the idea that model-based reasoning is essential.
background_label: However, we favor an approach that centers on one additional ingredient: autonomy.
objective_label: In particular, we aim toward agents that can both build and exploit their own internal models, with minimal human hand-engineering.
method_label: We believe an approach centered on autonomous learning has the greatest chance of success as we scale toward real-world complexity, tackling domains for which ready-made formal models are not available.
result_label: Here we survey several important examples of the progress that has been made toward building autonomous agents with humanlike abilities, and highlight some outstanding challenges.

===================================
paper_id: 29159089; YEAR: 2006
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: Word spotting for historical documents
ABSTRACT: background_label: Searching and indexing historical handwritten collections are a very challenging problem.
objective_label: We describe an approach called word spotting which involves grouping word images into clusters of similar words by using image matching to find similarity.
method_label: By annotating “interesting” clusters, an index that links words to the locations where they occur can be built automatically.
method_label: Image similarities computed using a number of different techniques including dynamic time warping are compared.
method_label: The word similarities are then used for clustering using both K-means and agglomerative clustering techniques.
result_label: It is shown in a subset of the George Washington collection that such a word spotting technique can outperform a Hidden Markov Model word-based recognition technique in terms of word error rates.

===================================
paper_id: 19143983; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidf - abs_tfidfcbow200
TITLE: SPINE: SParse Interpretable Neural Embeddings
ABSTRACT: background_label: Prediction without justification has limited utility.
background_label: Much of the success of neural models can be attributed to their ability to learn rich, dense and expressive representations.
background_label: While these representations capture the underlying complexity and latent trends in the data, they are far from being interpretable.
method_label: We propose a novel variant of denoising k-sparse autoencoders that generates highly efficient and interpretable distributed word representations (word embeddings), beginning with existing word representations from state-of-the-art methods like GloVe and word2vec.
result_label: Through large scale human evaluation, we report that our resulting word embedddings are much more interpretable than the original GloVe and word2vec embeddings.
result_label: Moreover, our embeddings outperform existing popular word embeddings on a diverse suite of benchmark downstream tasks.

===================================
paper_id: 52811940; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Learning and Evaluating Sparse Interpretable Sentence Embeddings
ABSTRACT: background_label: Previous research on word embeddings has shown that sparse representations, which can be either learned on top of existing dense embeddings or obtained through model constraints during training time, have the benefit of increased interpretability properties: to some degree, each dimension can be understood by a human and associated with a recognizable feature in the data.
objective_label: In this paper, we transfer this idea to sentence embeddings and explore several approaches to obtain a sparse representation.
method_label: We further introduce a novel, quantitative and automated evaluation metric for sentence embedding interpretability, based on topic coherence methods.
result_label: We observe an increase in interpretability compared to dense models, on a dataset of movie dialogs and on the scene descriptions from the MS COCO dataset.

===================================
paper_id: 3626819; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf - title_tfidfcbow200 - title_cbow200 - abs_cbow200
TITLE: Deep contextualized word representations
ABSTRACT: background_label: We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).
method_label: Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus.
method_label: We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis.
result_label: We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.

===================================
paper_id: 52098907; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 2; annotator4: 2; annotator3: 0
sources: title_tfidfcbow200 - abs_tfidf - title_cbow200
TITLE: Dissecting Contextual Word Embeddings: Architecture and Representation
ABSTRACT: background_label: Contextual word representations derived from pre-trained bidirectional language models (biLMs) have recently been shown to provide significant improvements to the state of the art for a wide range of NLP tasks.
background_label: However, many questions remain as to how and why these models are so effective.
background_label: In this paper, we present a detailed empirical study of how the choice of neural architecture (e.g.
objective_label: LSTM, CNN, or self attention) influences both end task accuracy and qualitative properties of the representations that are learned.
method_label: We show there is a tradeoff between speed and accuracy, but all architectures learn high quality contextual representations that outperform word embeddings for four challenging NLP tasks.
method_label: Additionally, all architectures learn representations that vary with network depth, from exclusively morphological based at the word embedding layer through local syntax based in the lower contextual layers to longer range semantics such coreference at the upper layers.
result_label: Together, these results suggest that unsupervised biLMs, independent of architecture, are learning much more about the structure of language than previously appreciated.

===================================
paper_id: 127598; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf
TITLE: Controlled Experiments for Word Embeddings
ABSTRACT: objective_label: An experimental approach to studying the properties of word embeddings is proposed.
background_label: Controlled experiments, achieved through modifications of the training corpus, permit the demonstration of direct relations between word properties and word vector direction and length.
method_label: The approach is demonstrated using the word2vec CBOW model with experiments that independently vary word frequency and word co-occurrence noise.
method_label: The experiments reveal that word vector length depends more or less linearly on both word frequency and the level of noise in the co-occurrence distribution of the word.
result_label: The coefficients of linearity depend upon the word.
result_label: The special point in feature space, defined by the (artificial) word with pure noise in its co-occurrence distribution, is found to be small but non-zero.

===================================
paper_id: 5809776; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidf - title_tfidfcbow200
TITLE: Learning Bilingual Word Representations by Marginalizing Alignments
ABSTRACT: background_label: We present a probabilistic model that simultaneously learns alignments and distributed representations for bilingual data.
method_label: By marginalizing over word alignments the model captures a larger semantic context than prior work relying on hard alignments.
result_label: The advantage of this approach is demonstrated in a cross-lingual classification task, where we outperform the prior published state of the art.

===================================
paper_id: 14276764; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_cbow200 - title_tfidf - abs_tfidf - abs_tfidfcbow200
TITLE: Better Word Representations with Recursive Neural Networks for Morphology
ABSTRACT: background_label: AbstractVector-space word representations have been very successful in recent years at improving performance across a variety of NLP tasks.
background_label: However, common to most existing work, words are regarded as independent entities without any explicit relationship among morphologically related words being modeled.
background_label: As a result, rare and complex words are often poorly estimated, and all unknown words are represented in a rather crude way using only one or a few vectors.
objective_label: This paper addresses this shortcoming by proposing a novel model that is capable of building representations for morphologically complex words from their morphemes.
method_label: We combine recursive neural networks (RNNs), where each morpheme is a basic unit, with neural language models (NLMs) to consider contextual information in learning morphologicallyaware word representations.
result_label: Our learned models outperform existing word representations by a good margin on word similarity tasks across many datasets, including a new dataset we introduce focused on rare words to complement existing ones in an interesting way.

===================================
paper_id: 44119185; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Using Morphological Knowledge in Open-Vocabulary Neural Language Models
ABSTRACT: background_label: AbstractLanguages with productive morphology pose problems for language models that generate words from a fixed vocabulary.
background_label: Although character-based models allow any possible word type to be generated, they are linguistically naïve: they must discover that words exist and are delimited by spaces-basic linguistic facts that are built in to the structure of word-based models.
method_label: We introduce an openvocabulary language model that incorporates more sophisticated linguistic knowledge by predicting words using a mixture of three generative processes: (1) by generating words as a sequence of characters, (2) by directly generating full word forms, and (3) by generating words as a sequence of morphemes that are combined using a hand-written morphological analyzer.
result_label: Experiments on Finnish, Turkish, and Russian show that our model outperforms character sequence models and other strong baselines on intrinsic and extrinsic measures.
result_label: Furthermore, we show that our model learns to exploit morphological knowledge encoded in the analyzer, and, as a byproduct, it can perform effective unsupervised morphological disambiguation.

===================================
paper_id: 2188439; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Documents and Dependencies: an Exploration of Vector Space Models for Semantic Composition
ABSTRACT: background_label: AbstractIn most previous research on distributional semantics, Vector Space Models (VSMs) of words are built either from topical information (e.g., documents in which a word is present), or from syntactic/semantic types of words (e.g., dependency parse links of a word in sentences), but not both.
objective_label: In this paper, we explore the utility of combining these two representations to build VSM for the task of semantic composition of adjective-noun phrases.
method_label: Through extensive experiments on benchmark datasets, we find that even though a type-based VSM is effective for semantic composition, it is often outperformed by a VSM built using a combination of topic-and type-based statistics.
method_label: We also introduce a new evaluation task wherein we predict the composed vector representation of a phrase from the brain activity of a human subject reading that phrase.
result_label: We exploit a large syntactically parsed corpus of 16 billion tokens to build our VSMs, with vectors for both phrases and words, and make them publicly available.

===================================
paper_id: 629094; YEAR: 2010
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidf - title_tfidf - specter - abs_tfidfcbow200
TITLE: Word Representations: A Simple and General Method for Semi-Supervised Learning
ABSTRACT: background_label: AbstractIf we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features.
method_label: We evaluate Brown clusters, Collobert and Weston (2008) We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines.
result_label: We find further improvements by combining different word representations.
other_label: You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize.
other_label: com/projects/wordreprs/

===================================
paper_id: 1541076; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidfcbow200 - abs_tfidf - title_tfidfcbow200 - abs_cbow200
TITLE: Portuguese Word Embeddings: Evaluating on Word Analogies and Natural Language Tasks
ABSTRACT: background_label: Word embeddings have been found to provide meaningful representations for words in an efficient way; therefore, they have become common in Natural Language Processing sys- tems.
objective_label: In this paper, we evaluated different word embedding models trained on a large Portuguese corpus, including both Brazilian and European variants.
method_label: We trained 31 word embedding models using FastText, GloVe, Wang2Vec and Word2Vec.
method_label: We evaluated them intrinsically on syntactic and semantic analogies and extrinsically on POS tagging and sentence semantic similarity tasks.
result_label: The obtained results suggest that word analogies are not appropriate for word embedding evaluation; task-specific evaluations appear to be a better option.

===================================
paper_id: 43927675; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: title_tfidfcbow200
TITLE: How much does a word weigh? Weighting word embeddings for word sense induction
ABSTRACT: background_label: The paper describes our participation in the first shared task on word sense induction and disambiguation for the Russian language RUSSE'2018 (Panchenko et al., 2018).
method_label: For each of several dozens of ambiguous words, the participants were asked to group text fragments containing it according to the senses of this word, which were not provided beforehand, therefore the"induction"part of the task.
method_label: For instance, a word"bank"and a set of text fragments (also known as"contexts") in which this word occurs, e.g.
method_label: "bank is a financial institution that accepts deposits"and"river bank is a slope beside a body of water"were given.
method_label: A participant was asked to cluster such contexts in the unknown in advance number of clusters corresponding to, in this case, the"company"and the"area"senses of the word"bank".
method_label: The organizers proposed three evaluation datasets of varying complexity and text genres based respectively on texts of Wikipedia, Web pages, and a dictionary of the Russian language.
method_label: We present two experiments: a positive and a negative one, based respectively on clustering of contexts represented as a weighted average of word embeddings and on machine translation using two state-of-the-art production neural machine translation systems.
method_label: Our team showed the second best result on two datasets and the third best result on the remaining one dataset among 18 participating teams.
result_label: We managed to substantially outperform competitive state-of-the-art baselines from the previous years based on sense embeddings.

===================================
paper_id: 2820782; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - specter
TITLE: Learning Word Meta-Embeddings
ABSTRACT: background_label: AbstractWord embeddings -distributed representations of words -in deep learning are beneficial for many tasks in NLP.
background_label: However, different embedding sets vary greatly in quality and characteristics of the captured information.
objective_label: Instead of relying on a more advanced algorithm for embedding learning, this paper proposes an ensemble approach of combining different public embedding sets with the aim of learning metaembeddings.
method_label: Experiments on word similarity and analogy tasks and on part-of-speech tagging show better performance of metaembeddings compared to individual embedding sets.
method_label: One advantage of metaembeddings is the increased vocabulary coverage.
other_label: We release our metaembeddings publicly at http:// cistern.cis.lmu.de/meta-emb.

===================================
paper_id: 9558665; YEAR: 1989
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: Word Association Norms, Mutual Information and Lexicography
ABSTRACT: background_label: AbstractThe term word assaciation is used in a very particular sense in the psycholinguistic literature.
background_label: (Generally speaking, subjects respond quicker than normal to the word "nurse" if it follows a highly associated word such as "doctor.")
background_label: We wilt extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntactic co-occurrence constraints between verbs and prepositions (content word/function word).
objective_label: This paper will propose a new objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora.
method_label: (The standard method of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly and unreliable.)
result_label: The , proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, waki,~g it possible to estimate norms for tens of thousands of words.

===================================
paper_id: 10183099; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: title_cbow200 - title_tfidf - title_tfidfcbow200
TITLE: Learning Rare Word Representations using Semantic Bridging
ABSTRACT: method_label: We propose a methodology that adapts graph embedding techniques (DeepWalk (Perozzi et al., 2014) and node2vec (Grover and Leskovec, 2016)) as well as cross-lingual vector space mapping approaches (Least Squares and Canonical Correlation Analysis) in order to merge the corpus and ontological sources of lexical knowledge.
method_label: We also perform comparative analysis of the used algorithms in order to identify the best combination for the proposed system.
method_label: We then apply this to the task of enhancing the coverage of an existing word embedding's vocabulary with rare and unseen words.
result_label: We show that our technique can provide considerable extra coverage (over 99%), leading to consistent performance gain (around 10% absolute gain is achieved with w2v-gn-500K cf.\S 3.3) on the Rare Word Similarity dataset.

===================================
paper_id: 10721657; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Quantificational features in distributional word representations
ABSTRACT: background_label: AbstractDo distributional word representations encode the linguistic regularities that theories of meaning argue they should encode?
background_label: We address this question in the case of the logical properties (monotonicity, force) of quantificational words such as everything (in the object domain) and always (in the time domain).
method_label: Using the vector offset approach to solving word analogies, we find that the skip-gram model of distributional semantics behaves in a way that is remarkably consistent with encoding these features in some domains, with accuracy approaching 100%, especially with mediumsized context windows.
result_label: Accuracy in others domains was less impressive.
result_label: We compare the performance of the model to the behavior of human participants, and find that humans performed well even where the models struggled.

===================================
paper_id: 13154790; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidf - specter - abs_tfidfcbow200
TITLE: On the Convergent Properties of Word Embedding Methods
ABSTRACT: background_label: Do word embeddings converge to learn similar things over different initializations?
background_label: How repeatable are experiments with word embeddings?
background_label: Are all word embedding techniques equally reliable?
objective_label: In this paper we propose evaluating methods for learning word representations by their consistency across initializations.
method_label: We propose a measure to quantify the similarity of the learned word representations under this setting (where they are subject to different random initializations).
result_label: Our preliminary results illustrate that our metric not only measures a intrinsic property of word embedding methods but also correlates well with other evaluation metrics on downstream tasks.
result_label: We believe our methods are is useful in characterizing robustness -- an important property to consider when developing new word embedding methods.

===================================
paper_id: 16447573; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited - title_tfidf - abs_tfidf - title_tfidfcbow200 - title_cbow200 - abs_cbow200 - specter - abs_tfidfcbow200
TITLE: Distributed Representations of Words and Phrases and their Compositionality
ABSTRACT: background_label: The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships.
method_label: In this paper we present several extensions that improve both the quality of the vectors and the training speed.
method_label: By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations.
method_label: We also describe a simple alternative to the hierarchical softmax called negative sampling.
method_label: An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases.
method_label: For example, the meanings of"Canada"and"Air"cannot be easily combined to obtain"Air Canada".
result_label: Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.

===================================
paper_id: 13430845; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf - title_tfidf - title_tfidfcbow200 - abs_tfidfcbow200
TITLE: A feature-word-topic model for image annotation and retrieval
ABSTRACT: background_label: Image annotation is a process of finding appropriate semantic labels for images in order to obtain a more convenient way for indexing and searching images on the Web.
method_label: This article proposes a novel method for image annotation based on combining feature-word distributions, which map from visual space to word space, and word-topic distributions, which form a structure to capture label relationships for annotation.
method_label: We refer to this type of model as Feature-Word-Topic models.
method_label: The introduction of topics allows us to efficiently take word associations, such as {ocean, fish, coral} or {desert, sand, cactus}, into account for image annotation.
method_label: Unlike previous topic-based methods, we do not consider topics as joint distributions of words and visual features, but as distributions of words only.
method_label: Feature-word distributions are utilized to define weights in computation of topic distributions for annotation.
method_label: By doing so, topic models in text mining can be applied directly in our method.
result_label: Our Feature-word-topic model, which exploits Gaussian Mixtures for feature-word distributions, and probabilistic Latent Semantic Analysis (pLSA) for word-topic distributions, shows that our method is able to obtain promising results in image annotation and retrieval.

===================================
paper_id: 8346846; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter
TITLE: Phrase Representations for Multiword Expressions
ABSTRACT: background_label: AbstractRecent works in Natural Language Processing (NLP) using neural networks have focused on learning dense word representations to perform classification tasks.
background_label: When dealing with phrase prediction problems, is is common practice to use special tagging schemes to identify segments boundaries.
background_label: This allows these tasks to be expressed as common word tagging problems.
objective_label: In this paper, we propose to learn fixed-size representations for arbitrarily sized chunks.
method_label: We introduce a model that takes advantage of such representations to perform phrase tagging by directly identifying and classifying phrases.
result_label: We evaluate our approach on the task of multiword expression (MWE) tagging and show that our model outperforms the stateof-the-art model for this task.

===================================
paper_id: 11236762; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_cbow200 - specter - abs_tfidfcbow200
TITLE: Integrating and Evaluating Neural Word Embeddings in Information Retrieval
ABSTRACT: background_label: Recent advances in neural language models have contributed new methods for learning distributed vector representations of words (also called word embeddings).
background_label: Two such methods are the continuous bag-of-words model and the skipgram model.
method_label: These methods have been shown to produce embeddings that capture higher order relationships between words that are highly effective in natural language processing tasks involving the use of word similarity and word analogy.
background_label: Despite these promising results, there has been little analysis of the use of these word embeddings for retrieval.
objective_label: Motivated by these observations, in this paper, we set out to determine how these word embeddings can be used within a retrieval model and what the benefit might be.
method_label: To this aim, we use neural word embeddings within the well known translation language model for information retrieval.
method_label: This language model captures implicit semantic relations between the words in queries and those in relevant documents, thus producing more accurate estimations of document relevance.
method_label: The word embeddings used to estimate neural language models produce translations that differ from previous translation language model approaches; differences that deliver improvements in retrieval effectiveness.
result_label: The models are robust to choices made in building word embeddings and, even more so, our results show that embeddings do not even need to be produced from the same corpus being used for retrieval.

===================================
paper_id: 1524421; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200 - specter
TITLE: Morphological Priors for Probabilistic Neural Word Embeddings
ABSTRACT: background_label: Word embeddings allow natural language processing systems to share statistical information across related words.
background_label: These embeddings are typically based on distributional statistics, making it difficult for them to generalize to rare or unseen words.
objective_label: We propose to improve word embeddings by incorporating morphological information, capturing shared sub-word features.
method_label: Unlike previous work that constructs word embeddings directly from morphemes, we combine morphological and distributional information in a unified probabilistic framework, in which the word embedding is a latent variable.
method_label: The morphological information provides a prior distribution on the latent word embeddings, which in turn condition a likelihood function over an observed corpus.
result_label: This approach yields improvements on intrinsic word similarity evaluations, and also in the downstream task of part-of-speech tagging.

===================================
paper_id: 7037110; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Context-Dependent Word Representation for Neural Machine Translation
ABSTRACT: background_label: We first observe a potential weakness of continuous vector representations of symbols in neural machine translation.
background_label: That is, the continuous vector representation, or a word embedding vector, of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of the word.
background_label: This has the consequence that the encoder and decoder recurrent networks in neural machine translation need to spend substantial amount of their capacity in disambiguating source and target words based on the context which is defined by a source sentence.
method_label: Based on this observation, in this paper we propose to contextualize the word embedding vectors using a nonlinear bag-of-words representation of the source sentence.
method_label: Additionally, we propose to represent special tokens (such as numbers, proper nouns and acronyms) with typed symbols to facilitate translating those words that are not well-suited to be translated via continuous vectors.
result_label: The experiments on En-Fr and En-De reveal that the proposed approaches of contextualization and symbolization improves the translation quality of neural machine translation systems significantly.

===================================
paper_id: 16718717; YEAR: 2006
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_tfidf
TITLE: Aligning ASL for Statistical Translation Using a Discriminative Word Model
ABSTRACT: method_label: We describe a method to align ASL video subtitles with a closed-caption transcript.
background_label: Our alignments are partial, based on spotting words within the video sequence, which consists of joined (rather than isolated) signs with unknown word boundaries.
method_label: We start with windows known to contain an example of a word, but not limited to it.
method_label: We estimate the start and end of the word in these examples using a voting method.
method_label: This provides a small number of training examples (typically three per word).
method_label: Since there is no shared structure, we use a discriminative rather than a generative word model.
method_label: While our word spotters are not perfect, they are sufficient to establish an alignment.
result_label: We demonstrate that quite small numbers of good word spotters results in an alignment good enough to produce simple English-ASL translations, both by phrase matching and using word substitution.

===================================
paper_id: 5120787; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter - title_tfidf - abs_tfidf - title_tfidfcbow200 - title_cbow200
TITLE: Improving Topic Models with Latent Feature Word Representations
ABSTRACT: background_label: Probabilistic topic models are widely used to discover latent topics in document collections, while latent feature vector representations of words have been used to obtain high performance in many NLP tasks.
method_label: In this paper, we extend two different Dirichlet multinomial topic models by incorporating latent feature vector representations of words trained on very large corpora to improve the word-topic mapping learnt on a smaller corpus.
result_label: Experimental results show that by using information from the external corpora, our new models produce significant improvements on topic coherence, document clustering and document classification tasks, especially on datasets with few or short documents.

===================================
paper_id: 9798309; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: Risk-sensitive Reinforcement Learning
ABSTRACT: background_label: We derive a family of risk-sensitive reinforcement learning methods for agents, who face sequential decision-making tasks in uncertain environments.
method_label: By applying a utility function to the temporal difference (TD) error, nonlinear transformations are effectively applied not only to the received rewards but also to the true transition probabilities of the underlying Markov decision process.
method_label: When appropriate utility functions are chosen, the agents' behaviors express key features of human behavior as predicted by prospect theory (Kahneman and Tversky, 1979), for example different risk-preferences for gains and losses as well as the shape of subjective probability curves.
method_label: We derive a risk-sensitive Q-learning algorithm, which is necessary for modeling human behavior when transition probabilities are unknown, and prove its convergence.
method_label: As a proof of principle for the applicability of the new framework we apply it to quantify human behavior in a sequential investment task.
result_label: We find, that the risk-sensitive variant provides a significantly better fit to the behavioral data and that it leads to an interpretation of the subject's responses which is indeed consistent with prospect theory.
result_label: The analysis of simultaneously measured fMRI signals show a significant correlation of the risk-sensitive TD error with BOLD signal change in the ventral striatum.
result_label: In addition we find a significant correlation of the risk-sensitive Q-values with neural activity in the striatum, cingulate cortex and insula, which is not present if standard Q-values are used.

===================================
paper_id: 13401571; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: Topic Segmentation with a Structured Topic Model
ABSTRACT: background_label: AbstractWe present a new hierarchical Bayesian model for unsupervised topic segmentation.
method_label: This new model integrates a point-wise boundary sampling algorithm used in Bayesian segmentation into a structured topic model that can capture a simple hierarchical topic structure latent in documents.
method_label: We develop an MCMC inference algorithm to split/merge segment(s).
result_label: Experimental results show that our model outperforms previous unsupervised segmentation methods using only lexical information on Choi's datasets and two meeting transcripts and has performance comparable to those previous methods on two written datasets.

===================================
paper_id: 19968336; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: word representation or word embedding in Persian text
ABSTRACT: background_label: Text processing is one of the sub-branches of natural language processing.
background_label: Recently, the use of machine learning and neural networks methods has been given greater consideration.
background_label: For this reason, the representation of words has become very important.
objective_label: This article is about word representation or converting words into vectors in Persian text.
method_label: In this research GloVe, CBOW and skip-gram methods are updated to produce embedded vectors for Persian words.
method_label: In order to train a neural networks, Bijankhan corpus, Hamshahri corpus and UPEC corpus have been compound and used.
method_label: Finally, we have 342,362 words that obtained vectors in all three models for this words.
result_label: These vectors have many usage for Persian natural language processing.

===================================
paper_id: 27667018; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: title_tfidfcbow200 - abs_cbow200 - abs_tfidfcbow200
TITLE: Semantic Structure and Interpretability of Word Embeddings
ABSTRACT: background_label: Dense word embeddings, which encode semantic meanings of words to low dimensional vector spaces have become very popular in natural language processing (NLP) research due to their state-of-the-art performances in many NLP tasks.
background_label: Word embeddings are substantially successful in capturing semantic relations among words, so a meaningful semantic structure must be present in the respective vector spaces.
background_label: However, in many cases, this semantic structure is broadly and heterogeneously distributed across the embedding dimensions, which makes interpretation a big challenge.
method_label: In this study, we propose a statistical method to uncover the latent semantic structure in the dense word embeddings.
method_label: To perform our analysis we introduce a new dataset (SEMCAT) that contains more than 6500 words semantically grouped under 110 categories.
method_label: We further propose a method to quantify the interpretability of the word embeddings; the proposed method is a practical alternative to the classical word intrusion test that requires human intervention.

===================================
paper_id: 13805769; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200 - title_tfidf - abs_tfidf - specter
TITLE: Multilingual Distributed Representations without Word Alignment
ABSTRACT: background_label: Distributed representations of meaning are a natural way to encode covariance relationships between words and phrases in NLP.
background_label: By overcoming data sparsity problems, as well as providing information about semantic relatedness which is not available in discrete representations, distributed representations have proven useful in many NLP tasks.
background_label: Recent work has shown how compositional semantic representations can successfully be applied to a number of monolingual applications such as sentiment analysis.
background_label: At the same time, there has been some initial success in work on learning shared word-level representations across languages.
method_label: We combine these two approaches by proposing a method for learning distributed representations in a multilingual setup.
method_label: Our model learns to assign similar embeddings to aligned sentences and dissimilar ones to sentence which are not aligned while not requiring word alignments.
method_label: We show that our representations are semantically informative and apply them to a cross-lingual document classification task where we outperform the previous state of the art.
result_label: Further, by employing parallel corpora of multiple language pairs we find that our model learns representations that capture semantic relationships across languages for which no parallel data was used.

===================================
paper_id: 2815993; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_tfidf - abs_cbow200 - specter
TITLE: Topic Modeling for Short Texts with Auxiliary Word Embeddings
ABSTRACT: background_label: For many applications that require semantic understanding of short texts, inferring discriminative and coherent latent topics from short texts is a critical and fundamental task.
background_label: Conventional topic models largely rely on word co-occurrences to derive topics from a collection of documents.
background_label: However, due to the length of each document, short texts are much more sparse in terms of word co-occurrences.
background_label: Data sparsity therefore becomes a bottleneck for conventional topic models to achieve good results on short texts.
method_label: On the other hand, when a human being interprets a piece of short text, the understanding is not solely based on its content words, but also her background knowledge (e.g., semantically related words).
result_label: The recent advances in word embedding offer effective learning of word semantic relations from a large corpus.
background_label: Exploiting such auxiliary word embeddings to enrich topic modeling for short texts is the main focus of this paper.
objective_label: To this end, we propose a simple, fast, and effective topic model for short texts, named GPU-DMM.
method_label: Based on the Dirichlet Multinomial Mixture (DMM) model, GPU-DMM promotes the semantically related words under the same topic during the sampling process by using the generalized Polya urn (GPU) model.
method_label: In this sense, the background knowledge about word semantic relatedness learned from millions of external documents can be easily exploited to improve topic modeling for short texts.
result_label: Through extensive experiments on two real-world short text collections in two languages, we show that GPU-DMM achieves comparable or better topic representations than state-of-the-art models, measured by topic coherence.
result_label: The learned topic representation leads to the best accuracy in text classification task, which is used as an indirect evaluation.

===================================
paper_id: 931054; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited - title_tfidf - abs_tfidf - title_tfidfcbow200 - title_cbow200 - abs_cbow200 - specter - abs_tfidfcbow200
TITLE: Bilingual Word Embeddings for Phrase-Based Machine Translation
ABSTRACT: background_label: AbstractWe introduce bilingual word embeddings: semantic embeddings associated across two languages in the context of neural language models.
method_label: We propose a method to learn bilingual embeddings from a large unlabeled corpus, while utilizing MT word alignments to constrain translational equivalence.
method_label: The new embeddings significantly out-perform baselines in word semantic similarity.
result_label: A single semantic similarity feature induced with bilingual embeddings adds near half a BLEU point to the results of NIST08 Chinese-English machine translation task.

===================================
paper_id: 7602284; YEAR: 2011
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter
TITLE: Language Models as Representations for Weakly Supervised NLP Tasks
ABSTRACT: background_label: AbstractFinding the right representation for words is critical for building accurate NLP systems when domain-specific labeled data for the task is scarce.
objective_label: This paper investigates language model representations, in which language models trained on unlabeled corpora are used to generate real-valued feature vectors for words.
method_label: We investigate ngram models and probabilistic graphical models, including a novel lattice-structured Markov Random Field.
result_label: Experiments indicate that language model representations outperform traditional representations, and that graphical model representations outperform ngram models, especially on sparse and polysemous words.

===================================
paper_id: 2085726; YEAR: 2010
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: cited - title_tfidf - abs_tfidf - title_tfidfcbow200 - title_cbow200 - abs_cbow200 - specter - abs_tfidfcbow200
TITLE: BabelNet: Building a Very Large Multilingual Semantic Network
ABSTRACT: background_label: AbstractIn this paper we present BabelNet -a very large, wide-coverage multilingual semantic network.
method_label: The resource is automatically constructed by means of a methodology that integrates lexicographic and encyclopedic knowledge from WordNet and Wikipedia.
method_label: In addition Machine Translation is also applied to enrich the resource with lexical information for all languages.
result_label: We conduct experiments on new and existing gold-standard datasets to show the high quality and coverage of the resource.

===================================
paper_id: 15778456; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf - title_tfidfcbow200 - title_cbow200
TITLE: A Latent Concept Topic Model for Robust Topic Inference Using Word Embeddings
ABSTRACT: background_label: AbstractUncovering thematic structures of SNS and blog posts is a crucial yet challenging task, because of the severe data sparsity induced by the short length of texts and diverse use of vocabulary.
background_label: This hinders effective topic inference of traditional LDA because it infers topics based on document-level co-occurrence of words.
method_label: To robustly infer topics in such contexts, we propose a latent concept topic model (LCTM).
method_label: Unlike LDA, LCTM reveals topics via co-occurrence of latent concepts, which we introduce as latent variables to capture conceptual similarity of words.
method_label: More specifically, LCTM models each topic as a distribution over the latent concepts, where each latent concept is a localized Gaussian distribution over the word embedding space.
method_label: Since the number of unique concepts in a corpus is often much smaller than the number of unique words, LCTM is less susceptible to the data sparsity.
result_label: Experiments on the 20Newsgroups show the effectiveness of LCTM in dealing with short texts as well as the capability of the model in handling held-out documents with a high degree of OOV words.

===================================
paper_id: 184488112; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: Chinese Embedding via Stroke and Glyph Information: A Dual-channel View
ABSTRACT: background_label: Recent studies have consistently given positive hints that morphology is helpful in enriching word embeddings.
background_label: In this paper, we argue that Chinese word embeddings can be substantially enriched by the morphological information hidden in characters which is reflected not only in strokes order sequentially, but also in character glyphs spatially.
objective_label: Then, we propose a novel Dual-channel Word Embedding (DWE) model to realize the joint learning of sequential and spatial information of characters.
result_label: Through the evaluation on both word similarity and word analogy tasks, our model shows its rationality and superiority in modelling the morphology of Chinese.

===================================
paper_id: 5757459; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200 - title_tfidfcbow200 - abs_cbow200
TITLE: A Distribution-based Model to Learn Bilingual Word Embeddings
ABSTRACT: background_label: AbstractWe introduce a distribution based model to learn bilingual word embeddings from monolingual data.
background_label: It is simple, effective and does not require any parallel data or any seed lexicon.
background_label: We take advantage of the fact that word embeddings are usually in form of dense real-valued lowdimensional vector and therefore the distribution of them can be accurately estimated.
method_label: A novel cross-lingual learning objective is proposed which directly matches the distributions of word embeddings in one language with that in the other language.
method_label: During the joint learning process, we dynamically estimate the distributions of word embeddings in two languages respectively and minimize the dissimilarity between them through standard back propagation algorithm.
method_label: Our learned bilingual word embeddings allow to group each word and its translations together in the shared vector space.
result_label: We demonstrate the utility of the learned embeddings on the task of finding word-to-word translations from monolingual corpora.
result_label: Our model achieved encouraging performance on data in both related languages and substantially different languages.

===================================
paper_id: 53615701; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Interpretable Word Embeddings for Medical Domain
ABSTRACT: background_label: Word embeddings are finding their increasing application in a variety of biomedical Natural Language Processing (bioNLP) tasks, ranging from drug discovery to automated disease diagnosis.
background_label: While these word embeddings in their entirety have shown meaningful syntactic and semantic regularities, however, the meaning of individual dimensions remains elusive.
background_label: This becomes problematic both in general and particularly in sensitive domains such as bio-medicine, wherein, the interpretability of results is crucial to its widespread adoption.
objective_label: To address this issue, in this study, we aim to improve the interpretability of pre-trained word embeddings generated from a text corpora, and in doing so provide a systematic approach to formalize the problem.
method_label: More specifically, we exploit the rich categorical knowledge present in the biomedical domain, and propose to learn a transformation matrix that transforms the input embeddings to a new space where they are both interpretable and retain their original expressive features.
result_label: Experiments conducted on the largest available biomedical corpus suggests that the model is capable of performing interpretability that resembles closely to the human-level intuition.

===================================
paper_id: 8728609; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 1; annotator4: 0; annotator3: 1
sources: abs_tfidfcbow200 - abs_tfidf - abs_cbow200
TITLE: Multi-view Recurrent Neural Acoustic Word Embeddings
ABSTRACT: background_label: Recent work has begun exploring neural acoustic word embeddings---fixed-dimensional vector representations of arbitrary-length speech segments corresponding to words.
background_label: Such embeddings are applicable to speech retrieval and recognition tasks, where reasoning about whole words may make it possible to avoid ambiguous sub-word representations.
objective_label: The main idea is to map acoustic sequences to fixed-dimensional vectors such that examples of the same word are mapped to similar vectors, while different-word examples are mapped to very different vectors.
method_label: In this work we take a multi-view approach to learning acoustic word embeddings, in which we jointly learn to embed acoustic sequences and their corresponding character sequences.
method_label: We use deep bidirectional LSTM embedding models and multi-view contrastive losses.
method_label: We study the effect of different loss variants, including fixed-margin and cost-sensitive losses.
result_label: Our acoustic word embeddings improve over previous approaches for the task of word discrimination.
result_label: We also present results on other tasks that are enabled by the multi-view approach, including cross-view word discrimination and word similarity.

===================================
paper_id: 118716003; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200 - title_tfidf
TITLE: A Systematic Study of Leveraging Subword Information for Learning Word Representations
ABSTRACT: background_label: The use of subword-level information (e.g., characters, character n-grams, morphemes) has become ubiquitous in modern word representation learning.
background_label: Its importance is attested especially for morphologically rich languages which generate a large number of rare words.
background_label: Despite a steadily increasing interest in such subword-informed word representations, their systematic comparative analysis across typologically diverse languages and different tasks is still missing.
objective_label: In this work, we deliver such a study focusing on the variation of two crucial components required for subword-level integration into word representation models: 1) segmentation of words into subword units, and 2) subword composition functions to obtain final word representations.
method_label: We propose a general framework for learning subword-informed word representations that allows for easy experimentation with different segmentation and composition components, also including more advanced techniques based on position embeddings and self-attention.
method_label: Using the unified framework, we run experiments over a large number of subword-informed word representation configurations (60 in total) on 3 tasks (general and rare word similarity, dependency parsing, fine-grained entity typing) for 5 languages representing 3 language types.
result_label: Our main results clearly indicate that there is no"one-sizefits-all"configuration, as performance is both language- and task-dependent.
result_label: We also show that configurations based on unsupervised segmentation (e.g., BPE, Morfessor) are sometimes comparable to or even outperform the ones based on supervised word segmentation.

===================================
paper_id: 52902947; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Learning Robust, Transferable Sentence Representations for Text Classification
ABSTRACT: background_label: Despite deep recurrent neural networks (RNNs) demonstrate strong performance in text classification, training RNN models are often expensive and requires an extensive collection of annotated data which may not be available.
background_label: To overcome the data limitation issue, existing approaches leverage either pre-trained word embedding or sentence representation to lift the burden of training RNNs from scratch.
method_label: In this paper, we show that jointly learning sentence representations from multiple text classification tasks and combining them with pre-trained word-level and sentence level encoders result in robust sentence representations that are useful for transfer learning.
result_label: Extensive experiments and analyses using a wide range of transfer and linguistic tasks endorse the effectiveness of our approach.

===================================
paper_id: 10028211; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter - abs_cbow200 - abs_tfidfcbow200
TITLE: Jointly optimizing word representations for lexical and sentential tasks with the C-PHRASE model
ABSTRACT: method_label: We introduce C-PHRASE, a distributional semantic model that learns word representations by optimizing context prediction for phrases at all levels in a syntactic tree, from single words to full sentences.
background_label: C-PHRASE outperforms the state-of-theart C-BOW model on a variety of lexical tasks.
method_label: Moreover, since C-PHRASE word vectors are induced through a compositional learning objective (modeling the contexts of words combined into phrases), when they are summed, they produce sentence representations that rival those generated by ad-hoc compositional models.

===================================
paper_id: 14079772; YEAR: 2006
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200 - title_tfidf - title_tfidfcbow200 - title_cbow200 - abs_tfidfcbow200
TITLE: BiTAM: Bilingual Topic AdMixture Models For Word Alignment
ABSTRACT: background_label: We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation.
method_label: Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.
method_label: Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels).
method_label: These models enable word-alignment process to leverage topical contents of document-pairs.
method_label: Efficient variational approximation algorithms are designed for inference and parameter estimation.
method_label: With the inferred latent topics, BiTAM models facilitate coherent pairing of bilingual linguistic entities that share common topical aspects.
result_label: Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality.

===================================
paper_id: 57570672; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter - title_tfidf - abs_tfidf - title_tfidfcbow200 - title_cbow200
TITLE: Improving Topic Models with Latent Feature Word Representations
ABSTRACT: background_label: Probabilistic topic models are widely used to discover latent topics in document collections, while latent feature vector representations of words have been used to obtain high performance in many NLP tasks.
method_label: In this paper, we extend two different Dirichlet multinomial topic models by incorporating latent feature vector representations of words trained on very large corpora to improve the word-topic mapping learnt on a smaller corpus.
result_label: Experimental results show that by using information from the external corpora, our new models produce significant improvements on topic coherence, document clustering and document classification tasks, especially on datasets with few or short documents.

===================================
paper_id: 719598; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200 - title_tfidf - abs_tfidf - specter
TITLE: Word Semantic Representations using Bayesian Probabilistic Tensor Factorization
ABSTRACT: background_label: Many forms of word relatedness have been developed, providing different perspectives on word similarity.
method_label: We introduce a Bayesian probabilistic tensor factorization model for synthesizing a single word vector representation and per-perspective linear transformations from any number of word similarity matrices.
method_label: The resulting word vectors, when combined with the per-perspective linear transformation, approximately recreate while also regularizing and generalizing, each word similarity perspective.
method_label: Our method can combine manually created semantic resources with neural word embeddings to separate synonyms and antonyms, and is capable of generalizing to words outside the vocabulary of any particular perspective.
result_label: We evaluated the word embeddings with GRE antonym questions, the result achieves the state-ofthe-art performance.

===================================
paper_id: 30578862; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - abs_tfidfcbow200
TITLE: Word Embeddings for Multi-label Document Classification
ABSTRACT: background_label: AbstractIn this paper, we analyze and evaluate word embeddings for representation of longer texts in the multi-label document classification scenario.
method_label: The embeddings are used in three convolutional neural network topologies.
method_label: The experiments are realized on the CzechČTK and English Reuters-21578 standard corpora.
method_label: We compare the results of word2vec static and trainable embeddings with randomly initialized word vectors.
result_label: We conclude that initialization does not play an important role for classification.
result_label: However, learning of word vectors is crucial to obtain good results.

===================================
paper_id: 15747275; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter
TITLE: Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec
ABSTRACT: background_label: Distributed dense word vectors have been shown to be effective at capturing token-level semantic and syntactic regularities in language, while topic models can form interpretable representations over documents.
method_label: In this work, we describe lda2vec, a model that learns dense word vectors jointly with Dirichlet-distributed latent document-level mixtures of topic vectors.
method_label: In contrast to continuous dense document representations, this formulation produces sparse, interpretable document mixtures through a non-negative simplex constraint.
method_label: Our method is simple to incorporate into existing automatic differentiation frameworks and allows for unsupervised document representations geared for use by scientists while simultaneously learning word vectors and the linear relationships between them.

===================================
paper_id: 52274975; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidf - title_tfidfcbow200
TITLE: Distilled Wasserstein Learning for Word Embedding and Topic Modeling
ABSTRACT: method_label: We propose a novel Wasserstein method with a distillation mechanism, yielding joint learning of word embeddings and topics.
method_label: The proposed method is based on the fact that the Euclidean distance between word embeddings may be employed as the underlying distance in the Wasserstein topic model.
method_label: The word distributions of topics, their optimal transports to the word distributions of documents, and the embeddings of words are learned in a unified framework.
method_label: When learning the topic model, we leverage a distilled underlying distance matrix to update the topic distributions and smoothly calculate the corresponding optimal transports.
method_label: Such a strategy provides the updating of word embeddings with robust guidance, improving the algorithmic convergence.
result_label: As an application, we focus on patient admission records, in which the proposed method embeds the codes of diseases and procedures and learns the topics of admissions, obtaining superior performance on clinically-meaningful disease network construction, mortality prediction as a function of admission codes, and procedure recommendation.

===================================
paper_id: 202577797; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf - title_tfidf
TITLE: Multi-view and Multi-source Transfers in Neural Topic Modeling with Pretrained Topic and Word Embeddings
ABSTRACT: background_label: Though word embeddings and topics are complementary representations, several past works have only used pre-trained word embeddings in (neural) topic modeling to address data sparsity problem in short text or small collection of documents.
background_label: However, no prior work has employed (pre-trained latent) topics in transfer learning paradigm.
method_label: In this paper, we propose an approach to (1) perform knowledge transfer using latent topics obtained from a large source corpus, and (2) jointly transfer knowledge via the two representations (or views) in neural topic modeling to improve topic quality, better deal with polysemy and data sparsity issues in a target corpus.
method_label: In doing so, we first accumulate topics and word representations from one or many source corpora to build a pool of topics and word vectors.
method_label: Then, we identify one or multiple relevant source domain(s) and take advantage of corresponding topics and word features via the respective pools to guide meaningful learning in the sparse target domain.
method_label: We quantify the quality of topic and document representations via generalization (perplexity), interpretability (topic coherence) and information retrieval (IR) using short-text, long-text, small and large document collections from news and medical domains.
result_label: We have demonstrated the state-of-the-art results on topic modeling with the proposed framework.

===================================
paper_id: 49654540; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200
TITLE: Latent Semantic Analysis Approach for Document Summarization Based on Word Embeddings
ABSTRACT: background_label: Since the amount of information on the internet is growing rapidly, it is not easy for a user to find relevant information for his/her query.
background_label: To tackle this issue, much attention has been paid to Automatic Document Summarization.
background_label: The key point in any successful document summarizer is a good document representation.
background_label: The traditional approaches based on word overlapping mostly fail to produce that kind of representation.
method_label: Word embedding, distributed representation of words, has shown an excellent performance that allows words to match on semantic level.
result_label: Naively concatenating word embeddings makes the common word dominant which in turn diminish the representation quality.
method_label: In this paper, we employ word embeddings to improve the weighting schemes for calculating the input matrix of Latent Semantic Analysis method.
method_label: Two embedding-based weighting schemes are proposed and then combined to calculate the values of this matrix.
method_label: The new weighting schemes are modified versions of the augment weight and the entropy frequency.
method_label: The new schemes combine the strength of the traditional weighting schemes and word embedding.
result_label: The proposed approach is experimentally evaluated on three well-known English datasets, DUC 2002, DUC 2004 and Multilingual 2015 Single-document Summarization for English.
result_label: The proposed model performs comprehensively better compared to the state-of-the-art methods, by at least 1% ROUGE points, leading to a conclusion that it provides a better document representation and a better document summary as a result.

===================================
paper_id: 14120418; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200 - abs_tfidf - title_tfidfcbow200 - abs_tfidfcbow200
TITLE: Topic-Aware Deep Compositional Models for Sentence Classification
ABSTRACT: background_label: In recent years, deep compositional models have emerged as a popular technique for representation learning of sentence in computational linguistic and natural language processing.
background_label: These models normally train various forms of neural networks on top of pretrained word embeddings using a task-specific corpus.
background_label: However, most of these works neglect the multisense nature of words in the pretrained word embeddings.
objective_label: In this paper we introduce topic models to enrich the word embeddings for multisenses of words.
method_label: The integration of the topic model with various semantic compositional processes leads to topic-aware convolutional neural network and topic-aware long short term memory networks.
method_label: Different from previous multisense word embeddings models that assign multiple independent and sense-specific embeddings to each word, our proposed models are lightweight and have flexible frameworks that regard word sense as the composition of two parts: a general sense derived from a large corpus and a topic-specific sense derived from a task-specific corpus.
method_label: In addition, our proposed models focus on semantic composition instead of word understanding.
method_label: With the help of topic models, we can integrate the topic-specific sense at word-level before the composition and sentence-level after the composition.
result_label: Comprehensive experiments on five public sentence classification datasets are conducted and the results show that our proposed topic-aware deep compositional models produce competitive or better performance than other text representation learning methods.

===================================
paper_id: 202120592; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf - abs_cbow200 - abs_tfidfcbow200
TITLE: How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings
ABSTRACT: background_label: Replacing static word embeddings with contextualized word representations has yielded significant improvements on many NLP tasks.
background_label: However, just how contextual are the contextualized representations produced by models such as ELMo and BERT?
background_label: Are there infinitely many context-specific representations for each word, or are words essentially assigned one of a finite number of word-sense representations?
background_label: For one, we find that the contextualized representations of all words are not isotropic in any layer of the contextualizing model.
method_label: While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this self-similarity is much lower in upper layers.
result_label: This suggests that upper layers of contextualizing models produce more context-specific representations, much like how upper layers of LSTMs produce more task-specific representations.
result_label: In all layers of ELMo, BERT, and GPT-2, on average, less than 5% of the variance in a word's contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualized representations.

===================================
paper_id: 13342211; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: Learning Efficient Representations for Reinforcement Learning
ABSTRACT: background_label: Markov decision processes (MDPs) are a well studied framework for solving sequential decision making problems under uncertainty.
background_label: Exact methods for solving MDPs based on dynamic programming such as policy iteration and value iteration are effective on small problems.
background_label: In problems with a large discrete state space or with continuous state spaces, a compact representation is essential for providing an efficient approximation solutions to MDPs.
background_label: Commonly used approximation algorithms involving constructing basis functions for projecting the value function onto a low dimensional subspace, and building a factored or hierarchical graphical model to decompose the transition and reward functions.
background_label: However, hand-coding a good compact representation for a given reinforcement learning (RL) task can be quite difficult and time consuming.
background_label: Recent approaches have attempted to automatically discover efficient representations for RL.
objective_label: In this thesis proposal, we discuss the problems of automatically constructing structured kernel for kernel based RL, a popular approach to learning non-parametric approximations for value function.
method_label: We explore a space of kernel structures which are built compositionally from base kernels using a context-free grammar.
method_label: We examine a greedy algorithm for searching over the structure space.
result_label: To demonstrate how the learned structure can represent and approximate the original RL problem in terms of compactness and efficiency, we plan to evaluate our method on a synthetic problem and compare it to other RL baselines.

===================================
paper_id: 18849671; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 1; annotator4: 0; annotator3: 1
sources: specter - abs_tfidf - abs_tfidfcbow200
TITLE: A word prediction methodology for automatic sentence completion
ABSTRACT: background_label: Word prediction generally relies on n-grams occurrence statistics, which may have huge data storage requirements and does not take into account the general meaning of the text.
objective_label: We propose an alternative methodology, based on Latent Semantic Analysis, to address these issues.
method_label: An asymmetric Word-Word frequency matrix is employed to achieve higher scalability with large training datasets than the classic Word-Document approach.
method_label: We propose a function for scoring candidate terms for the missing word in a sentence.
method_label: We show how this function approximates the probability of occurrence of a given candidate word.
result_label: Experimental results show that the proposed approach outperforms non neural network language models.

===================================
paper_id: 67745045; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_tfidf - abs_tfidfcbow200
TITLE: Short Text Topic Model with Word Embeddings and Context Information
ABSTRACT: background_label: Due to the length limitation of short texts, classical topic models based on document-level word co-occurrence information fail to distill semantically coherent topics from short text collections.
background_label: Word embeddings are trained from large corpus and inherently encoded with general word semantic information, hence they can be supplemental knowledge to guide topic modeling.
background_label: General Polya Urn Dirichlet Multinomial Mixture (GPU-DMM) model is the first attempt which leverages word embeddings as external knowledge to enhance topic coherence for short text.
background_label: However, word embeddings are usually trained on large corpus, the encoded semantic information is not necessarily suitable for training dataset.
method_label: In this work, we improve the GPU-DMM model by leveraging both context information and word embeddings to distill semantic relatedness of word pairs, which can be further leveraged in model inference process to improve topic coherence.
result_label: Experimental results of two tasks on two real world short text collections show that our model gains comparable or better performance than GPU-DMM model and other state-of-art short text topic models.

===================================
paper_id: 32156977; YEAR: 2005
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited - title_tfidf - abs_tfidf - title_tfidfcbow200 - title_cbow200 - abs_cbow200 - specter - abs_tfidfcbow200
TITLE: Property of average precision and its generalization: An examination of evaluation indicator for information retrieval experiments
ABSTRACT: background_label: In information retrieval experiments, indicators for measuring effectiveness of the systems or methods are important.
background_label: Average precision is often used as an indicator for evaluating ranked output of documents in standard retrieval experiments.
objective_label: This report examines some properties of this indicator.
method_label: First, we clarify mathematically that relevant documents at a higher position in the ranked list contribute much more to increasing the score of average precision.
method_label: Second, influence of detecting unknown relevant documents on the score is discussed.
method_label: Third, we examine statistical variation of average precision scores caused by fluctuations of results from relevance judgments.
objective_label: Another issue of this report is to explore evaluation indicators using data of multi-grade relevance.
method_label: After reviewing indicators proposed by other researchers, i.e., modified sliding ratio, normalized discounted cumulative gain (nDCG), Q-measure and so on, a new indicator, generalized average precision is developed.
result_label: We compare these indicators empirically using a simple and artificial example.

===================================
paper_id: 10109787; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200 - specter
TITLE: Task-Oriented Learning of Word Embeddings for Semantic Relation Classification
ABSTRACT: method_label: We present a novel learning method for word embeddings designed for relation classification.
method_label: Our word embeddings are trained by predicting words between noun pairs using lexical relation-specific features on a large unlabeled corpus.
method_label: This allows us to explicitly incorporate relation-specific information into the word embeddings.
method_label: The learned word embeddings are then used to construct feature vectors for a relation classification model.
method_label: On a well-established semantic relation classification task, our method significantly outperforms a baseline based on a previously introduced word embedding method, and compares favorably to previous state-of-the-art models that use syntactic information or manually constructed external resources.

===================================
paper_id: 25714896; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf - abs_cbow200 - abs_tfidfcbow200
TITLE: Context encoders as a simple but powerful extension of word2vec
ABSTRACT: background_label: With a simple architecture and the ability to learn meaningful word embeddings efficiently from texts containing billions of words, word2vec remains one of the most popular neural language models used today.
background_label: However, as only a single embedding is learned for every word in the vocabulary, the model fails to optimally represent words with multiple meanings.
background_label: Additionally, it is not possible to create embeddings for new (out-of-vocabulary) words on the spot.
objective_label: Based on an intuitive interpretation of the continuous bag-of-words (CBOW) word2vec model's negative sampling training objective in terms of predicting context based similarities, we motivate an extension of the model we call context encoders (ConEc).
method_label: By multiplying the matrix of trained word2vec embeddings with a word's average context vector, out-of-vocabulary (OOV) embeddings and representations for a word with multiple meanings can be created based on the word's local contexts.
result_label: The benefits of this approach are illustrated by using these word embeddings as features in the CoNLL 2003 named entity recognition (NER) task.

===================================
paper_id: 82456167; YEAR: 2007
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: Janeway's Immunobiology
ABSTRACT: background_label: Part I An Introduction to Immunobiology and Innate Immunity 1.
background_label: Basic Concepts in Immunology 2.
background_label: Innate Immunity Part II The Recognition of Antigen 3.
background_label: Antigen Recognition by B-cell and T-cell Receptors 4.
method_label: The Generation of Lymphocyte Antigen Receptors 5.
method_label: Antigen Presentation to T Lymphocytes Part III The Development of Mature Lymphocyte Receptor Repertoires 6.
method_label: Signaling Through Immune System Receptors 7.
result_label: The Development and Survival of Lymphocytes Part IV The Adaptive Immune Response 8.
background_label: T Cell-Mediated Immunity 9.
background_label: The Humoral Immune Response 10.
background_label: Dynamics of Adaptive Immunity 11.
other_label: The Mucosal Immune System Part V The Immune System in Health and Disease 12.
background_label: Failures of Host Defense Mechanism 13.
other_label: Allergy and Hypersensitivity 14.
other_label: Autoimmunity and Transplantation 15.
other_label: Manipulation of the Immune Response Part VI The Origins of Immune Responses 16.
other_label: Evolution of the Immune System Appendix I Immunologists' Toolbox Appendix II CD Antigens Appendix III Cytokines and their Receptors Appendix IV Chemokines and their Receptors Appendix V Immunological Constants

===================================
paper_id: 18708263; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200
TITLE: Active Discriminative Word Embedding Learning
ABSTRACT: objective_label: AbstractWe propose a new active learning (AL) method for text classification based on convolutional neural networks (CNNs).
method_label: In AL, one selects the instances to be manually labeled with the aim of maximizing model performance with minimal effort.
method_label: Neural models capitalize on word embeddings as features, tuning these to the task at hand.
method_label: We argue that AL strategies for neural text classification should focus on selecting instances that most affect the embedding space (i.e., induce discriminative word representations).
method_label: This is in contrast to traditional AL approaches (e.g., uncertainty sampling), which specify higher level objectives.
method_label: We propose a simple approach that selects instances containing words whose embeddings are likely to be updated with the greatest magnitude, thereby rapidly learning discriminative, task-specific embeddings.
result_label: Empirical results show that our method outperforms baseline AL approaches.

===================================
paper_id: 26376992; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - abs_tfidf - title_cbow200
TITLE: Temporal Word Analogies: Identifying Lexical Replacement with Diachronic Word Embeddings
ABSTRACT: background_label: AbstractThis paper introduces the concept of temporal word analogies: pairs of words which occupy the same semantic space at different points in time.
background_label: One well-known property of word embeddings is that they are able to effectively model traditional word analogies ("word w 1 is to word w 2 as word w 3 is to word w 4 ") through vector addition.
method_label: Here, I show that temporal word analogies ("word w 1 at time t α is like word w 2 at time t β ") can effectively be modeled with diachronic word embeddings, provided that the independent embedding spaces from each time period are appropriately transformed into a common vector space.
method_label: When applied to a diachronic corpus of news articles, this method is able to identify temporal word analogies such as "Ronald Reagan in 1987 is like Bill Clinton in 1997", or "Walkman in 1987 is like iPod in 2007".
method_label: BackgroundThe meanings of utterances change over time, due both to changes within the linguistic system and to changes in the state of the world.
method_label: For example, the meaning of the word awful has changed over the past few centuries from something like "aweinspiring" to something more like "very bad", due to a process of semantic drift.
method_label: On the other hand, the phrase president of the United States has meant different things at different points in time due to the fact that different people have occupied that same position at different times.
method_label: These are very different types of changes, and the latter may not even be considered a linguistic phenomenon, but both types of change are relevant to the concept of temporal word analogies.I define a temporal word analogy (TWA) as a pair of words which occupy a similar semantic space at different points in time.
result_label: For example, assuming that there is a semantic space associated with "President of the USA", this space was occupied by Ronald Reagan in the 1980s, and by Bill Clinton in the 1990s.
background_label: So a temporal analogy holds: "Ronald Reagan in 1987 is like Bill Clinton in 1997".Distributional semantics methods, particularly vector-space models of word meanings, have been employed to study both semantic change and word analogies, and as such are well-suited for the task of identifying TWAs.
background_label: The principle behind these models, that the meaning of words can be captured by looking at the contexts in which they appear (i.e.
background_label: other words), is not a recent idea, and is generally attributed to Harris (1954 ) or Firth (1957 .
background_label: The modern era of applying this principle algorithmically began with latent semantic analysis (LSA) (Landauer and Dumais, 1997), and the recent explosion in popularity of word embeddings is largely due to the very effective word2vec neural network approach to computing word embeddings (Mikolov et al., 2013a) .
background_label: In these types of vector space models (VSMs), the meaning of a word is represented as a multi-dimensional vector, and semantically-related words tend to have vectors that relate to one another in regular ways (e.g.
background_label: by occupying nearby points in the vector space).
background_label: One factor in word embeddings' recent popularity is their eye-catching ability to model word analogies using vector addition, as in the well-known example king + man − woman = queen (Mikolov et al., 2013b) .
other_label: Sagi et al.
result_label: (2011) were the first to advocate the use of distributional semantics methods (specifically LSA) to automate and quantify large-scale studies of semantic change, in contrast to a more traditional approach in which a researcher inspects 448

===================================
paper_id: 2640922; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: A Comparison of Word Embeddings for English and Cross-Lingual Chinese Word Sense Disambiguation
ABSTRACT: background_label: Word embeddings are now ubiquitous forms of word representation in natural language processing.
background_label: There have been applications of word embeddings for monolingual word sense disambiguation (WSD) in English, but few comparisons have been done.
objective_label: This paper attempts to bridge that gap by examining popular embeddings for the task of monolingual English WSD.
method_label: Our simplified method leads to comparable state-of-the-art performance without expensive retraining.
method_label: Cross-Lingual WSD - where the word senses of a word in a source language e come from a separate target translation language f - can also assist in language learning; for example, when providing translations of target vocabulary for learners.
method_label: Thus we have also applied word embeddings to the novel task of cross-lingual WSD for Chinese and provide a public dataset for further benchmarking.
method_label: We have also experimented with using word embeddings for LSTM networks and found surprisingly that a basic LSTM network does not work well.
result_label: We discuss the ramifications of this outcome.

===================================
paper_id: 118988729; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: A Microphotonic Astrocomb
ABSTRACT: background_label: One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers.
background_label: It has already been realized that the large number of exactly known optical frequencies provided by laser frequency combs ('astrocombs') can significantly surpass conventionally used hollow-cathode lamps as calibration light sources.
background_label: A remaining challenge, however, is generation of frequency combs with lines resolvable by astronomical spectrometers.
method_label: Here we demonstrate an astrocomb generated via soliton formation in an on-chip microphotonic resonator ('microresonator') with a resolvable line spacing of 23.7 GHz.
method_label: This comb is providing wavelength calibration on the 10 cm/s radial velocity level on the GIANO-B high-resolution near-infrared spectrometer.
result_label: As such, microresonator frequency combs have the potential of providing broadband wavelength calibration for the next-generation of astronomical instruments in planet-hunting and cosmological research.

