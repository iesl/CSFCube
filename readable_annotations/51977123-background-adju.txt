======================================================================
paper_id: 51977123; YEAR: 2008
TITLE: Methods to integrate a language model with semantic information for a word prediction component
ABSTRACT: background_label: Most current word prediction systems make use of n-gram language models (LM) to estimate the probability of the following word in a phrase.
background_label: In the past years there have been many attempts to enrich such language models with further syntactic or semantic information.
objective_label: We want to explore the predictive powers of Latent Semantic Analysis (LSA), a method that has been shown to provide reliable information on long-distance semantic dependencies between words in a context.
method_label: We present and evaluate here several methods that integrate LSA-based information with a standard language model: a semantic cache, partial reranking, and different forms of interpolation.
result_label: We found that all methods show significant improvements, compared to the 4-gram baseline, and most of them to a simple cache model as well.
===================================
paper_id: 5741058; YEAR: 2009
adju relevance: Identical (+3)
difference: 1; annotator1: 3; annotator2: 2
sources: abs_tfidf - specter - title_tfidf
TITLE: Language Models Based on Semantic Composition
ABSTRACT: background_label: In this paper we propose a novel statistical language model to capture long-range semantic dependencies.
objective_label: Specifically, we apply the concept of semantic composition to the problem of constructing predictive history representations for upcoming words.
method_label: We also examine the influence of the underlying semantic space on the composition task by comparing spatial semantic representations against topic-based ones.
method_label: The composition models yield reductions in perplexity when combined with a standard n-gram language model over the n-gram model alone.
result_label: We also obtain perplexity reductions when integrating our models with a structured language model.

===================================
paper_id: 8238767; YEAR: 2000
adju relevance: Identical (+3)
difference: 0; annotator1: 3; annotator2: 3
sources: abs_tfidfcbow200 - abs_cbow200 - specter - abs_tfidf - title_tfidf
TITLE: Exploiting latent semantic information in statistical language modeling
ABSTRACT: background_label: Statistical language models used in large-vocabulary speech recognition must properly encapsulate the various constraints, both local and global, present in the language.
background_label: While local constraints are readily captured through n-gram modeling, global constraints, such as long-term semantic dependencies, have been more difficult to handle within a data-driven formalism.
objective_label: This paper focuses on the use of latent semantic analysis, a paradigm that automatically uncovers the salient semantic relationships between words and documents in a given corpus.
method_label: In this approach, (discrete) words and documents are mapped onto a (continuous) semantic vector space, in which familiar clustering techniques can be applied.
method_label: This leads to the specification of a powerful framework for automatic semantic classification, as well as the derivation of several language model families with various smoothing properties.
method_label: Because of their large-span nature, these language models are well suited to complement conventional n-grams.
method_label: An integrative formulation is proposed for harnessing this synergy, in which the latent semantic information is used to adjust the standard n-gram probability.
result_label: Such hybrid language modeling compares favorably with the corresponding n-gram baseline: experiments conducted on the Wall Street Journal domain show a reduction in average word error rate of over 20%.
result_label: This paper concludes with a discussion of intrinsic tradeoffs, such as the influence of training data selection on the resulting performance.

===================================
paper_id: 18849671; YEAR: 2015
adju relevance: Identical (+3)
difference: 0; annotator1: 3; annotator2: 3
sources: specter
TITLE: A word prediction methodology for automatic sentence completion
ABSTRACT: background_label: Word prediction generally relies on n-grams occurrence statistics, which may have huge data storage requirements and does not take into account the general meaning of the text.
objective_label: We propose an alternative methodology, based on Latent Semantic Analysis, to address these issues.
method_label: An asymmetric Word-Word frequency matrix is employed to achieve higher scalability with large training datasets than the classic Word-Document approach.
method_label: We propose a function for scoring candidate terms for the missing word in a sentence.
method_label: We show how this function approximates the probability of occurrence of a given candidate word.
result_label: Experimental results show that the proposed approach outperforms non neural network language models.

===================================
paper_id: 9205274; YEAR: 2013
adju relevance: Similar (+2)
difference: 1; annotator1: 2; annotator2: 1
sources: specter - abs_cbow200 - abs_tfidf
TITLE: Dependency Language Models for Sentence Completion
ABSTRACT: background_label: AbstractSentence completion is a challenging semantic modeling task in which models must choose the most appropriate word from a given set to complete a sentence.
background_label: Although a variety of language models have been applied to this task in previous work, none of the existing approaches incorporate syntactic information.
objective_label: In this paper we propose to tackle this task using a pair of simple language models in which the probability of a sentence is estimated as the probability of the lexicalisation of a given syntactic dependency tree.
result_label: We apply our approach to the Microsoft Research Sentence Completion Challenge and show that it improves on n-gram language models by 8.7 percentage points, achieving the highest accuracy reported to date apart from neural language models that are more complex and expensive to train.

===================================
paper_id: 5087912; YEAR: 2000
adju relevance: Similar (+2)
difference: 1; annotator1: 2; annotator2: 1
sources: title_tfidf - specter
TITLE: A Classification Approach to Word Prediction
ABSTRACT: objective_label: The eventual goal of a language model is to accurately predict the value of a missing word given its context.
objective_label: We present an approach to word prediction that is based on learning a representation for each word as a function of words and linguistics predicates in its context.
objective_label: This approach raises a few new questions that we address.
method_label: First, in order to learn good word representations it is necessary to use an expressive representation of the context.
method_label: We present a way that uses external knowledge to generate expressive context representations, along with a learning method capable of handling the large number of features generated this way that can, potentially, contribute to each prediction.
method_label: Second, since the number of words ``competing'' for each prediction is large, there is a need to ``focus the attention'' on a smaller subset of these.
method_label: We exhibit the contribution of a ``focus of attention'' mechanism to the performance of the word predictor.
result_label: Finally, we describe a large scale experimental study in which the approach presented is shown to yield significant improvements in word prediction tasks.

===================================
paper_id: 17626741; YEAR: 2003
adju relevance: Similar (+2)
difference: 0; annotator1: 2; annotator2: 2
sources: cited
TITLE: Exploiting long distance collocational relations in predictive typing
ABSTRACT: background_label: In this paper, we report about some preliminary experiments in which we tried to improve the performance of a state-of-the-art Predictive Typing system for the German language by adding a collocation-based prediction component.
objective_label: This component tries to exploit the fact that texts have a topic and are semantically coherent.
background_label: Thus, the appearance in a text of a certain word can be a cue that other, semantically related words are likely to appear soon.
method_label: The collocation-based module exploits this kind of topical/semantic relatedness by relying on statistics about the co-occurrence of words within a large window of text in the training corpus.
result_label: Our current experimental results indicate that using the collocation-based prediction module has a small but consistent positive effect on the performance of the system.

===================================
paper_id: 16281035; YEAR: 2006
adju relevance: Similar (+2)
difference: 1; annotator1: 2; annotator2: 1
sources: abs_tfidf
TITLE: Applying Part-of-Seech Enhanced LSA to Automatic Essay Grading
ABSTRACT: background_label: Latent Semantic Analysis (LSA) is a widely used Information Retrieval method based on"bag-of-words"assumption.
background_label: However, according to general conception, syntax plays a role in representing meaning of sentences.
background_label: Thus, enhancing LSA with part-of-speech (POS) information to capture the context of word occurrences appears to be theoretically feasible extension.
method_label: The approach is tested empirically on a automatic essay grading system using LSA for document similarity comparisons.
method_label: A comparison on several POS-enhanced LSA models is reported.
result_label: Our findings show that the addition of contextual information in the form of POS tags can raise the accuracy of the LSA-based scoring models up to 10.77 per cent.

===================================
paper_id: 2188439; YEAR: 2013
adju relevance: Similar (+2)
difference: 2; annotator1: 0; annotator2: 2
sources: specter - abs_tfidfcbow200
TITLE: Documents and Dependencies: an Exploration of Vector Space Models for Semantic Composition
ABSTRACT: background_label: AbstractIn most previous research on distributional semantics, Vector Space Models (VSMs) of words are built either from topical information (e.g., documents in which a word is present), or from syntactic/semantic types of words (e.g., dependency parse links of a word in sentences), but not both.
objective_label: In this paper, we explore the utility of combining these two representations to build VSM for the task of semantic composition of adjective-noun phrases.
method_label: Through extensive experiments on benchmark datasets, we find that even though a type-based VSM is effective for semantic composition, it is often outperformed by a VSM built using a combination of topic-and type-based statistics.
method_label: We also introduce a new evaluation task wherein we predict the composed vector representation of a phrase from the brain activity of a human subject reading that phrase.
result_label: We exploit a large syntactically parsed corpus of 16 billion tokens to build our VSMs, with vectors for both phrases and words, and make them publicly available.

===================================
paper_id: 13036002; YEAR: 2011
adju relevance: Similar (+2)
difference: 0; annotator1: 2; annotator2: 2
sources: title_tfidf - specter
TITLE: On Word Prediction Methods
ABSTRACT: background_label: This paper evaluates prediction and topic modelling methods through the task of word prediction.
method_label: In our word prediction experiment, we compare some existing and two novel methods, including a version of Cooccurrence, two versions of K-Nearest-Neighbor method and Latent semantic indexing[5], against a baseline algorithm.
method_label: Furthermore, we explore the effects of using different similarity functions on the accuracies of our prediction methods.
result_label: Finally, without much modifications to the framework, we were also able to perform tag classification on StackOverflow posts.

===================================
paper_id: 80895; YEAR: 2016
adju relevance: Similar (+2)
difference: 1; annotator1: 2; annotator2: 1
sources: abs_tfidf - specter
TITLE: Two Discourse Driven Language Models for Semantics
ABSTRACT: background_label: Natural language understanding often requires deep semantic knowledge.
background_label: Expanding on previous proposals, we suggest that some important aspects of semantic knowledge can be modeled as a language model if done at an appropriate level of abstraction.
method_label: We develop two distinct models that capture semantic frame chains and discourse information while abstracting over the specific mentions of predicates and entities.
method_label: For each model, we investigate four implementations: a"standard"N-gram language model and three discriminatively trained"neural"language models that generate embeddings for semantic frames.
result_label: The quality of the semantic language models (SemLM) is evaluated both intrinsically, using perplexity and a narrative cloze test and extrinsically - we show that our SemLM helps improve performance on semantic natural language processing tasks such as co-reference resolution and discourse parsing.

===================================
paper_id: 3660647; YEAR: 2018
adju relevance: Similar (+2)
difference: 1; annotator1: 1; annotator2: 2
sources: specter - abs_tfidfcbow200 - abs_cbow200 - abs_tfidf - title_tfidf
TITLE: Hybrid Model For Word Prediction Using Naive Bayes and Latent Information
ABSTRACT: background_label: Historically, the Natural Language Processing area has been given too much attention by many researchers.
background_label: One of the main motivation beyond this interest is related to the word prediction problem, which states that given a set words in a sentence, one can recommend the next word.
method_label: In literature, this problem is solved by methods based on syntactic or semantic analysis.
method_label: Solely, each of these analysis cannot achieve practical results for end-user applications.
result_label: For instance, the Latent Semantic Analysis can handle semantic features of text, but cannot suggest words considering syntactical rules.
background_label: On the other hand, there are models that treat both methods together and achieve state-of-the-art results, e.g.
background_label: Deep Learning.
background_label: These models can demand high computational effort, which can make the model infeasible for certain types of applications.
background_label: With the advance of the technology and mathematical models, it is possible to develop faster systems with more accuracy.
method_label: This work proposes a hybrid word suggestion model, based on Naive Bayes and Latent Semantic Analysis, considering neighbouring words around unfilled gaps.
result_label: Results show that this model could achieve 44.2% of accuracy in the MSR Sentence Completion Challenge.

===================================
paper_id: 13808346; YEAR: 2003
adju relevance: Similar (+2)
difference: 1; annotator1: 2; annotator2: 1
sources: cited
TITLE: Testing the Efficacy of Part-of-Speech Information in Word Completion
ABSTRACT: background_label: We investigate the effect of incorporating syntactic information into a word-completion algorithm.
method_label: We introduce two new algorithms that combine part-of-speech tag trigrams with word bigrams, and evaluate them with a test-bench constructed for the purpose.
result_label: The results show a small but statistically significant improvement in keystroke savings for one of our algorithms over baselines that use only word n-grams.

===================================
paper_id: 21012910; YEAR: 2017
adju relevance: Related (+1)
difference: 0; annotator1: 1; annotator2: 1
sources: title_cbow200 - title_tfidfcbow200
TITLE: Modelling Word Burstiness in Natural Language: A Generalised Polya Process for Document Language Models in Information Retrieval
ABSTRACT: background_label: We introduce a generalised multivariate Polya process for document language modelling.
background_label: The framework outlined here generalises a number of statistical language models used in information retrieval for modelling document generation.
method_label: In particular, we show that the choice of replacement matrix M ultimately defines the type of random process and therefore defines a particular type of document language model.
method_label: We show that a particular variant of the general model is useful for modelling term-specific burstiness.
result_label: Furthermore, via experimentation we show that this variant significantly improves retrieval effectiveness over a strong baseline on a number of small test collections.

===================================
paper_id: 761216; YEAR: 2015
adju relevance: Related (+1)
difference: 0; annotator1: 1; annotator2: 1
sources: specter
TITLE: Modeling Order in Neural Word Embeddings at Scale
ABSTRACT: background_label: Natural Language Processing (NLP) systems commonly leverage bag-of-words co-occurrence techniques to capture semantic and syntactic word relationships.
background_label: The resulting word-level distributed representations often ignore morphological information, though character-level embeddings have proven valuable to NLP tasks.
objective_label: We propose a new neural language model incorporating both word order and character order in its embedding.
method_label: The model produces several vector spaces with meaningful substructure, as evidenced by its performance of 85.8% on a recent word-analogy task, exceeding best published syntactic word-analogy scores by a 58% error margin.
result_label: Furthermore, the model includes several parallel training methods, most notably allowing a skip-gram network with 160 billion parameters to be trained overnight on 3 multi-core CPUs, 14x larger than the previous largest neural network.

===================================
paper_id: 62205532; YEAR: 1996
adju relevance: Related (+1)
difference: 0; annotator1: 1; annotator2: 1
sources: abs_tfidf
TITLE: Latent semantic analysis for text-based research
ABSTRACT: background_label: Latent semantic analysis (LSA) is a statistical model of word usage that permits comparisons of semantic similarity between pieces of textual information.
objective_label: This paper summarizes three experiments that illustrate how LSA may be used in text-based research.
method_label: Two experiments describe methods for analyzing a subject’s essay for determining from what text a subject learned the information and for grading the quality of information cited in the essay.
method_label: The third experiment describes using LSA to measure the coherence and comprehensibility of texts.

===================================
paper_id: 1668200; YEAR: 2003
adju relevance: Related (+1)
difference: 0; annotator1: 1; annotator2: 1
sources: abs_tfidf
TITLE: Automatic Evaluation Of Students' Answers Using Syntactically Enhanced LSA
ABSTRACT: background_label: Latent semantic analysis (LSA) has been used in several intelligent tutoring systems(ITS's) for assessing students' learning by evaluating their answers to questions in the tutoring domain.
background_label: It is based on word-document co-occurrence statistics in the training corpus and a dimensionality reduction technique.
background_label: However, it doesn't consider the word-order or syntactic information, which can improve the knowledge representation and therefore lead to better performance of an ITS.
method_label: We present here an approach called Syntactically Enhanced LSA (SELSA) which generalizes LSA by considering a word along with its syntactic neighborhood given by the part-of-speech tag of its preceding word, as a unit of knowledge representation.
result_label: The experimental results on Auto-Tutor task to evaluate students' answers to basic computer science questions by SELSA and its comparison with LSA are presented in terms of several cognitive measures.
result_label: SELSA is able to correctly evaluate a few more answers than LSA but is having less correlation with human evaluators than LSA has.
result_label: It also provides better discrimination of syntactic-semantic knowledge representation than LSA.

===================================
paper_id: 3834706; YEAR: 2018
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator2: 0
sources: specter
TITLE: Neural Lattice Language Models
ABSTRACT: objective_label: In this work, we propose a new language modeling paradigm that has the ability to perform both prediction and moderation of information flow at multiple granularities: neural lattice language models.
method_label: These models construct a lattice of possible paths through a sentence and marginalize across this lattice to calculate sequence probabilities or optimize parameters.
method_label: This approach allows us to seamlessly incorporate linguistic intuitions - including polysemy and existence of multi-word lexical items - into our language model.
result_label: Experiments on multiple language modeling tasks show that English neural lattice language models that utilize polysemous embeddings are able to improve perplexity by 9.95% relative to a word-level baseline, and that a Chinese model that handles multi-character tokens is able to improve perplexity by 20.94% relative to a character-level baseline.

===================================
paper_id: 14529778; YEAR: 2013
adju relevance: Related (+1)
difference: 0; annotator1: 1; annotator2: 1
sources: title_cbow200 - title_tfidfcbow200 - title_tfidf
TITLE: Incorporating semantic information to selection of web texts for language model of spoken dialogue system
ABSTRACT: objective_label: A novel text selection approach for training a language model (LM) with Web texts is proposed for automatic speech recognition (ASR) of spoken dialogue systems.
method_label: Compared to the conventional approach based on perplexity criterion, the proposed approach introduces a semantic-level relevance measure with the back-end knowledge base used in the dialogue system.
objective_label: We focus on the predicate-argument (P-A) structure characteristic to the domain in order to filter semantically relevant sentences in the domain.
method_label: Several choices of statistical models and combination methods with the perplexity measure are investigated in this paper.
result_label: Experimental evaluations in two different domains demonstrate the effectiveness and generality of the proposed approach.
result_label: The combination method realizes significant improvement not only in ASR accuracy but also in semantic and dialogue-level accuracy.

===================================
paper_id: 67855689; YEAR: 2019
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator2: 0
sources: abs_tfidfcbow200 - abs_cbow200
TITLE: Semantic Hilbert Space for Text Representation Learning
ABSTRACT: background_label: Capturing the meaning of sentences has long been a challenging task.
background_label: Current models tend to apply linear combinations of word features to conduct semantic composition for bigger-granularity units e.g.
background_label: phrases, sentences, and documents.
background_label: However, the semantic linearity does not always hold in human language.
background_label: For instance, the meaning of the phrase `ivory tower' can not be deduced by linearly combining the meanings of `ivory' and `tower'.
method_label: To address this issue, we propose a new framework that models different levels of semantic units (e.g.
method_label: sememe, word, sentence, and semantic abstraction) on a single \textit{Semantic Hilbert Space}, which naturally admits a non-linear semantic composition by means of a complex-valued vector word representation.
result_label: An end-to-end neural network~\footnote{https://github.com/wabyking/qnn} is proposed to implement the framework in the text classification task, and evaluation results on six benchmarking text classification datasets demonstrate the effectiveness, robustness and self-explanation power of the proposed model.
result_label: Furthermore, intuitive case studies are conducted to help end users to understand how the framework works.

===================================
paper_id: 28848885; YEAR: 2016
adju relevance: Related (+1)
difference: 0; annotator1: 1; annotator2: 1
sources: abs_tfidfcbow200 - title_cbow200 - title_tfidfcbow200 - abs_cbow200
TITLE: Nasari: Integrating explicit knowledge and corpus statistics for a multilingual representation of concepts and entities
ABSTRACT: background_label: AbstractOwing to the need for a deep understanding of linguistic items, semantic representation is considered to be one of the fundamental components of several applications in Natural Language Processing and Artificial Intelligence.
background_label: As a result, semantic representation has been one of the prominent research areas in lexical semantics over the past decades.
background_label: However, due mainly to the lack of large sense-annotated corpora, most existing representation techniques are limited to the lexical level and thus cannot be effectively applied to individual word senses.
method_label: In this paper we put forward a novel multilingual vector representation, called Nasari, which not only enables accurate representation of word senses in different languages, but it also provides two main advantages over existing approaches: (1) high coverage, including both concepts and named entities, (2) comparability across languages and linguistic levels (i.e., words, senses and concepts), thanks to the representation of linguistic items in a single unified semantic space and in a joint embedded space, respectively.
other_label: Moreover, our representations are flexible, can be applied to multiple applications and are freely available at http://lcl.uniroma1.it/nasari/.
result_label: As evaluation benchmark, we opted for four different tasks, namely, word similarity, sense clustering, domain labeling, and Word Sense Disambiguation, for each of which we report state-of-the-art performance on several standard datasets across different languages.

===================================
paper_id: 8353882; YEAR: 2015
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator2: 0
sources: abs_tfidf - specter
TITLE: An Empirical Comparison Between N-gram and Syntactic Language Models for Word Ordering
ABSTRACT: background_label: Syntactic language models and N-gram language models have both been used in word ordering.
background_label: In this paper, we give an empirical comparison between N-gram and syntactic language models on word order task.
background_label: Our results show that the quality of automatically-parsed training data has a relatively small impact on syntactic models.
result_label: Both of syntactic and N-gram models can benefit from large-scale raw text.
method_label: Compared with N-gram models, syntactic models give overall better performance, but they require much more training time.
result_label: In addition, the two models lead to different error distributions in word ordering.
result_label: A combination of the two models integrates the advantages of each model, achieving the best result in a standard benchmark.

===================================
paper_id: 52010710; YEAR: 2018
adju relevance: Related (+1)
difference: 0; annotator1: 1; annotator2: 1
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: Contextual String Embeddings for Sequence Labeling
ABSTRACT: background_label: AbstractRecent advances in language modeling using recurrent neural networks have made it viable to model language as distributions over characters.
background_label: By learning to predict the next character on the basis of previous characters, such models have been shown to automatically internalize linguistic concepts such as words, sentences, subclauses and even sentiment.
objective_label: In this paper, we propose to leverage the internal states of a trained character language model to produce a novel type of word embedding which we refer to as contextual string embeddings.
method_label: Our proposed embeddings have the distinct properties that they (a) are trained without any explicit notion of words and thus fundamentally model words as sequences of characters, and (b) are contextualized by their surrounding text, meaning that the same word will have different embeddings depending on its contextual use.
result_label: We conduct a comparative evaluation against previous embeddings and find that our embeddings are highly useful for downstream tasks: across four classic sequence labeling tasks we consistently outperform the previous state-of-the-art.
other_label: In particular, we significantly outperform previous work on English and German named entity recognition (NER), allowing us to report new state-of-the-art F1-scores on the CONLL03 shared task.We release all code and pre-trained language models in a simple-to-use framework to the research community, to enable reproduction of these experiments and application of our proposed embeddings to other tasks: https://github.com/zalandoresearch/flair

===================================
paper_id: 16248711; YEAR: 2016
adju relevance: Related (+1)
difference: 1; annotator1: 0; annotator2: 1
sources: abs_tfidf
TITLE: Comparative study of LSA vs Word2vec embeddings in small corpora: a case study in dreams database
ABSTRACT: background_label: Word embeddings have been extensively studied in large text datasets.
background_label: However, only a few studies analyze semantic representations of small corpora, particularly relevant in single-person text production studies.
method_label: In the present paper, we compare Skip-gram and LSA capabilities in this scenario, and we test both techniques to extract relevant semantic patterns in single-series dreams reports.
method_label: LSA showed better performance than Skip-gram in small size training corpus in two semantic tests.
method_label: As a study case, we show that LSA can capture relevant words associations in dream reports series, even in cases of small number of dreams or low-frequency words.
result_label: We propose that LSA can be used to explore words associations in dreams reports, which could bring new insight into this classic research area of psychology

===================================
paper_id: 12416658; YEAR: 2015
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator2: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: Higher-order Lexical Semantic Models for Non-factoid Answer Reranking
ABSTRACT: background_label: Lexical semantic models provide robust performance for question answering, but, in general, can only capitalize on direct evidence seen during training.
background_label: For example, monolingual alignment models acquire term alignment probabilities from semi-structured data such as question-answer pairs; neural network language models learn term embeddings from unstructured text.
background_label: All this knowledge is then used to estimate the semantic similarity between question and answer candidates.
method_label: We introduce a higher-order formalism that allows all these lexical semantic models to chain direct evidence to construct indirect associations between question and answer texts, by casting the task as the traversal of graphs that encode direct term associations.
other_label: Using a corpus of 10,000 questions from Yahoo!
method_label: Answers, we experimentally demonstrate that higher-order methods are broadly applicable to alignment and language models, across both word and syntactic representations.
result_label: We show that an important criterion for success is controlling for the semantic drift that accumulates during graph traversal.
result_label: All in all, the proposed higher-order approach improves five out of the six lexical semantic models investigated, with relative gains of up to +13% over their first-order variants.

===================================
paper_id: 17400830; YEAR: 2008
adju relevance: Related (+1)
difference: 0; annotator1: 1; annotator2: 1
sources: abs_tfidfcbow200 - abs_cbow200 - abs_tfidf
TITLE: Measuring Semantic Distance using Distributional Profiles of Concepts
ABSTRACT: background_label: Semantic distance is a measure of how close or distant in meaning two units of language are.
background_label: A large number of important natural language problems, including machine translation and word sense disambiguation, can be viewed as semantic distance problems.
background_label: The two dominant approaches to estimating semantic distance are the WordNet-based semantic measures and the corpus-based distributional measures.
method_label: In this thesis, I compare them, both qualitatively and quantitatively, and identify the limitations of each.
objective_label: This thesis argues that estimating semantic distance is essentially a property of concepts (rather than words) and that two concepts are semantically close if they occur in similar contexts.
method_label: Instead of identifying the co-occurrence (distributional) profiles of words (distributional hypothesis), I argue that distributional profiles of concepts (DPCs) can be used to infer the semantic properties of concepts and indeed to estimate semantic distance more accurately.
method_label: I propose a new hybrid approach to calculating semantic distance that combines corpus statistics and a published thesaurus (Macquarie Thesaurus).
background_label: The algorithm determines estimates of the DPCs using the categories in the thesaurus as very coarse concepts and, notably, without requiring any sense-annotated data.
method_label: Even though the use of only about 1000 concepts to represent the vocabulary of a language seems drastic, I show that the method achieves results better than the state-of-the-art in a number of natural language tasks.
method_label: I show how cross-lingual DPCs can be created by combining text in one language with a thesaurus from another.
method_label: Using these cross-lingual DPCs, we can solve problems in one, possibly resource-poor, language using a knowledge source from another, possibly resource-rich, language.
method_label: I show that the approach is also useful in tasks that inherently involve two or more languages, such as machine translation and multilingual text summarization.
method_label: The proposed approach is computationally inexpensive, it can estimate both semantic relatedness and semantic similarity, and it can be applied to all parts of speech.
result_label: Extensive experiments on ranking word pairs as per semantic distance, real-word spelling correction, solving Reader’s Digest word choice problems, determining word sense dominance, word sense disambiguation, and word translation show that the new approach is markedly superior to previous ones.

===================================
paper_id: 195347873; YEAR: 2016
adju relevance: Related (+1)
difference: 1; annotator1: 2; annotator2: 1
sources: abs_tfidf - abs_cbow200
TITLE: Comparative study of LSA vs Word2vec embeddings in small corpora: a case study in dreams database
ABSTRACT: background_label: Abstract-This summary presents the results obtained in our work, Comparative study of LSA vs Word2vec embeddings in small corpora: a case study in dreams database [1].
background_label: BackgroundThe main idea behind word embeddings is that words with similar meanings tend to occur in similar contexts.
background_label: Base in this hypothesis, word embeddings describe each word in a vectorial space, where words with similar meanings are located close to each other.
background_label: Word embeddings have been extensively studied in large text datasets.
method_label: However, only a few studies analyze semantic representations of small corpora, particularly relevant in single-person text production studies.In our paper [1], we compare the two most used embeddings (Skip-gram and LSA) capabilities in this scenario, and we test both techniques to identify word associations in dream reports series.
method_label: Dream content analysisMost of the newest dream content analysis methods are based on frequency word-counting of predefined categories in dreams reports [2] .
method_label: A well known limitation of this approach is the impossibility of identifying the meaning of the counted words, which are determined by the context in which they appear.
method_label: To tackle this problem, we set out to study the capabilities of word embeddings to capture relevant word associations in dream reports series.
method_label: This is the first time in which word embeddings has been applied to dream content analysis.
result_label: Summary of ResultsFirstly, we test LSA and skip-grams performance in two semantic task for different corpus size.
background_label: As it is known that the optimal embeddings dimensions depends on the corpus size [3], we also vary the number dimensions and use the best result for each corpus size.
background_label: We found that Skip-gram models has a steeper learning curve, outperforming LSA when the models are trained with medium to large datasets.
method_label: However, when the corpus size is reduced, Skip-gram's performance has a severe decrease, thus LSA becoming the more suitable tool.Secondly, we test word embeddings capabilities to identify word associations in dream reports series.
method_label: In particular, we test whether these tools were able to capture accurately, in different manually annotated dream reports series, the semantic neighborhood of the word run.
result_label: We found that LSA can effectively differentiate different word usage pattern even in cases of series with low number of dreams and low frequency of target words.
method_label: ConclusionIn our work, we show in two semantic tests that LSA is more appropriate in small-size corpus scenarios than the well-used skip-gram model.
result_label: Also we show that LSA can accurately quantify words associations in dreams reports.
result_label: This is a step forward in the application of word embeddings to the analysis of dream content.
method_label: We propose that LSA can be used to explore word associations in dreams reports, which could bring new insight into this classic field of psychological research.
result_label: On one hand, the validation of semantic metrics to analyze word associations in dream reports promises a much more accurate quantification of socially-shared meaning in dream reports, with great potential application in psychiatric diagnosis [4] and dream decoding research [5]

===================================
paper_id: 61894598; YEAR: 1990
adju relevance: Related (+1)
difference: 0; annotator1: 1; annotator2: 1
sources: cited
TITLE: Indexing by Latent Semantic Analysis
ABSTRACT: method_label: A new method for automatic indexing and retrieval is described.
objective_label: The approach is to take advantage of implicit higher-order structure in the association of terms with documents (“semantic structure”) in order to improve the detection of relevant documents on the basis of terms found in queries.
method_label: The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca.
method_label: 100 orthogonal factors from which the original matrix can be approximated by linear combination.
method_label: Documents are represented by ca.
method_label: 100 item vectors of factor weights.
method_label: Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned.
result_label: initial tests find this completely automatic method for retrieval to be promising.

===================================
paper_id: 49587517; YEAR: 2018
adju relevance: Related (+1)
difference: 0; annotator1: 1; annotator2: 1
sources: specter - abs_tfidfcbow200
TITLE: Language Modeling for Morphologically Rich Languages: Character-Aware Modeling for Word-Level Prediction
ABSTRACT: background_label: Neural architectures are prominent in the construction of language models (LMs).
background_label: However, word-level prediction is typically agnostic of subword-level information (characters and character sequences) and operates over a closed vocabulary, consisting of a limited word set.
background_label: Indeed, while subword-aware models boost performance across a variety of NLP tasks, previous work did not evaluate the ability of these models to assist next-word prediction in language modeling tasks.
background_label: Such subword-level informed models should be particularly effective for morphologically-rich languages (MRLs) that exhibit high type-to-token ratios.
method_label: In this work, we present a large-scale LM study on 50 typologically diverse languages covering a wide variety of morphological systems, and offer new LM benchmarks to the community, while considering subword-level information.
method_label: The main technical contribution of our work is a novel method for injecting subword-level information into semantic word vectors, integrated into the neural language modeling training, to facilitate word-level prediction.
method_label: We conduct experiments in the LM setting where the number of infrequent words is large, and demonstrate strong perplexity gains across our 50 languages, especially for morphologically-rich languages.
result_label: Our code and data sets are publicly available.

===================================
paper_id: 53080999; YEAR: 2018
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator2: 0
sources: abs_cbow200 - abs_tfidfcbow200 - specter
TITLE: Language Modeling with Sparse Product of Sememe Experts
ABSTRACT: background_label: Most language modeling methods rely on large-scale data to statistically learn the sequential patterns of words.
background_label: In this paper, we argue that words are atomic language units but not necessarily atomic semantic units.
method_label: Inspired by HowNet, we use sememes, the minimum semantic units in human languages, to represent the implicit semantics behind words for language modeling, named Sememe-Driven Language Model (SDLM).
method_label: More specifically, to predict the next word, SDLM first estimates the sememe distribution gave textual context.
method_label: Afterward, it regards each sememe as a distinct semantic expert, and these experts jointly identify the most probable senses and the corresponding word.
method_label: In this way, SDLM enables language models to work beyond word-level manipulation to fine-grained sememe-level semantics and offers us more powerful tools to fine-tune language models and improve the interpretability as well as the robustness of language models.
result_label: Experiments on language modeling and the downstream application of headline gener- ation demonstrate the significant effect of SDLM.
other_label: Source code and data used in the experiments can be accessed at https:// github.com/thunlp/SDLM-pytorch.

===================================
paper_id: 1458999; YEAR: 2005
adju relevance: Related (+1)
difference: 0; annotator1: 1; annotator2: 1
sources: specter
TITLE: Integrating word relationships into language models
ABSTRACT: objective_label: In this paper, we propose a novel dependency language modeling approach for information retrieval.
background_label: The approach extends the existing language modeling approach by relaxing the independence assumption.
objective_label: Our goal is to build a language model in which various word relationships can be integrated.
method_label: In this work, we integrate two types of relationship extracted from WordNet and co-occurrence relationships respectively.
method_label: The integrated model has been tested on several TREC collections.
result_label: The results show that our model achieves substantial and significant improvements with respect to the models without these relationships.
result_label: These results clearly show the benefit of integrating word relationships into language models for IR.

===================================
paper_id: 15310708; YEAR: 2014
adju relevance: Related (+1)
difference: 1; annotator1: 2; annotator2: 1
sources: abs_tfidfcbow200 - abs_tfidf
TITLE: N-gram-Based Low-Dimensional Representation for Document Classification
ABSTRACT: background_label: The bag-of-words (BOW) model is the common approach for classifying documents, where words are used as feature for training a classifier.
background_label: This generally involves a huge number of features.
background_label: Some techniques, such as Latent Semantic Analysis (LSA) or Latent Dirichlet Allocation (LDA), have been designed to summarize documents in a lower dimension with the least semantic information loss.
background_label: Some semantic information is nevertheless always lost, since only words are considered.
objective_label: Instead, we aim at using information coming from n-grams to overcome this limitation, while remaining in a low-dimension space.
background_label: Many approaches, such as the Skip-gram model, provide good word vector representations very quickly.
objective_label: We propose to average these representations to obtain representations of n-grams.
method_label: All n-grams are thus embedded in a same semantic space.
method_label: A K-means clustering can then group them into semantic concepts.
method_label: The number of features is therefore dramatically reduced and documents can be represented as bag of semantic concepts.
result_label: We show that this model outperforms LSA and LDA on a sentiment classification task, and yields similar results than a traditional BOW-model with far less features.

===================================
paper_id: 13538306; YEAR: 2002
adju relevance: Related (+1)
difference: 1; annotator1: 2; annotator2: 1
sources: specter
TITLE: Exploiting Headword Dependency And Predictive Clustering For Language Modeling
ABSTRACT: background_label: This paper presents several practical ways of incorporating linguistic structure into language models.
method_label: A headword detector is first applied to detect the headword of each phrase in a sentence.
method_label: A permuted headword trigram model (PHTM) is then generated from the annotated corpus.
method_label: Finally, PHTM is extended to a cluster PHTM (C-PHTM) by defining clusters for similar words in the corpus.
result_label: We evaluated the proposed models on the realistic application of Japanese Kana-Kanji conversion.
result_label: Experiments show that C-PHTM achieves 15% error rate reduction over the word trigram model.
result_label: This demonstrates that the use of simple methods such as the headword trigram and predictive clustering can effectively capture long distance word dependency, and substantially outperform a word trigram model.

===================================
paper_id: 162168707; YEAR: 2019
adju relevance: Related (+1)
difference: 0; annotator1: 1; annotator2: 1
sources: title_cbow200 - title_tfidf
TITLE: Acoustic-to-Word Models with Conversational Context Information
ABSTRACT: background_label: Conversational context information, higher-level knowledge that spans across sentences, can help to recognize a long conversation.
background_label: However, existing speech recognition models are typically built at a sentence level, and thus it may not capture important conversational context information.
background_label: The recent progress in end-to-end speech recognition enables integrating context with other available information (e.g., acoustic, linguistic resources) and directly recognizing words from speech.
objective_label: In this work, we present a direct acoustic-to-word, end-to-end speech recognition model capable of utilizing the conversational context to better process long conversations.
result_label: We evaluate our proposed approach on the Switchboard conversational speech corpus and show that our system outperforms a standard end-to-end speech recognition system.

===================================
paper_id: 2084342; YEAR: 2014
adju relevance: Related (+1)
difference: 0; annotator1: 1; annotator2: 1
sources: abs_tfidf - title_tfidf
TITLE: A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval
ABSTRACT: objective_label: In this paper, we propose a new latent semantic model that incorporates a convolutional-pooling structure over word sequences to learn low-dimensional, semantic vector representations for search queries and Web documents.
method_label: In order to capture the rich contextual structures in a query or a document, we start with each word within a temporal context window in a word sequence to directly capture contextual features at the word n-gram level.
method_label: Next, the salient word n-gram features in the word sequence are discovered by the model and are then aggregated to form a sentence-level feature vector.
method_label: Finally, a non-linear transformation is applied to extract high-level semantic information to generate a continuous vector representation for the full text string.
method_label: The proposed convolutional latent semantic model (CLSM) is trained on clickthrough data and is evaluated on a Web document ranking task using a large-scale, real-world data set.
result_label: Results show that the proposed model effectively captures salient semantic information in queries and documents for the task while significantly outperforming previous state-of-the-art semantic models.

===================================
paper_id: 52074926; YEAR: 2018
adju relevance: Related (+1)
difference: 0; annotator1: 1; annotator2: 1
sources: title_cbow200 - title_tfidf
TITLE: Exploiting Rich Syntactic Information for Semantic Parsing with Graph-to-Sequence Model
ABSTRACT: background_label: Existing neural semantic parsers mainly utilize a sequence encoder, i.e., a sequential LSTM, to extract word order features while neglecting other valuable syntactic information such as dependency graph or constituent trees.
method_label: In this paper, we first propose to use the \textit{syntactic graph} to represent three types of syntactic information, i.e., word order, dependency and constituency features.
method_label: We further employ a graph-to-sequence model to encode the syntactic graph and decode a logical form.
result_label: Experimental results on benchmark datasets show that our model is comparable to the state-of-the-art on Jobs640, ATIS and Geo880.
result_label: Experimental results on adversarial examples demonstrate the robustness of the model is also improved by encoding more syntactic information.

===================================
paper_id: 11706860; YEAR: 2015
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator2: 0
sources: specter - abs_tfidfcbow200
TITLE: Learning to Represent Words in Context with Multilingual Supervision
ABSTRACT: background_label: We present a neural network architecture based on bidirectional LSTMs to compute representations of words in the sentential contexts.
background_label: These context-sensitive word representations are suitable for, e.g., distinguishing different word senses and other context-modulated variations in meaning.
method_label: To learn the parameters of our model, we use cross-lingual supervision, hypothesizing that a good representation of a word in context will be one that is sufficient for selecting the correct translation into a second language.
result_label: We evaluate the quality of our representations as features in three downstream tasks: prediction of semantic supersenses (which assign nouns and verbs into a few dozen semantic classes), low resource machine translation, and a lexical substitution task, and obtain state-of-the-art results on all of these.

===================================
paper_id: 12982389; YEAR: 2001
adju relevance: Related (+1)
difference: 1; annotator1: 2; annotator2: 1
sources: cited
TITLE: A Bit of Progress in Language Modeling
ABSTRACT: background_label: In the past several years, a number of different language modeling improvements over simple trigram models have been found, including caching, higher-order n-grams, skipping, interpolated Kneser-Ney smoothing, and clustering.
background_label: We present explorations of variations on, or of the limits of, each of these techniques, including showing that sentence mixture models may have more potential.
background_label: While all of these techniques have been studied separately, they have rarely been studied in combination.
method_label: We find some significant interactions, especially with smoothing and clustering techniques.
method_label: We compare a combination of all techniques together to a Katz smoothed trigram model with no count cutoffs.
result_label: We achieve perplexity reductions between 38% and 50% (1 bit of entropy), depending on training data size, as well as a word error rate reduction of 8.9%.
result_label: Our perplexity reductions are perhaps the highest reported compared to a fair baseline.
result_label: This is the extended version of the paper; it contains additional details and proofs, and is designed to be a good introduction to the state of the art in language modeling.

===================================
paper_id: 14736559; YEAR: 2014
adju relevance: Related (+1)
difference: 0; annotator1: 1; annotator2: 1
sources: abs_cbow200 - specter - abs_tfidf
TITLE: Rehabilitation of Count-based Models for Word Vector Representations
ABSTRACT: background_label: Recent works on word representations mostly rely on predictive models.
background_label: Distributed word representations (aka word embeddings) are trained to optimally predict the contexts in which the corresponding words tend to appear.
background_label: Such models have succeeded in capturing word similarties as well as semantic and syntactic regularities.
objective_label: Instead, we aim at reviving interest in a model based on counts.
method_label: We present a systematic study of the use of the Hellinger distance to extract semantic representations from the word co-occurence statistics of large text corpora.
method_label: We show that this distance gives good performance on word similarity and analogy tasks, with a proper type and size of context, and a dimensionality reduction based on a stochastic low-rank approximation.
method_label: Besides being both simple and intuitive, this method also provides an encoding function which can be used to infer unseen words or phrases.
result_label: This becomes a clear advantage compared to predictive models which must train these new words.

===================================
paper_id: 3365209; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator2: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: MSRC: multimodal spatial regression with semantic context for phrase grounding
ABSTRACT: background_label: Given a textual description of an image, phrase grounding localizes objects in the image referred by query phrases in the description.
background_label: State-of-the-art methods treat phrase grounding as a ranking problem and address it by retrieving a set of proposals according to the query’s semantics, which are limited by the performance of independent proposal generation systems and ignore useful cues from context in the description.
method_label: In this paper, we propose a novel multimodal spatial regression with semantic context (MSRC) system which not only predicts the location of ground truth based on proposal bounding boxes, but also refines prediction results by penalizing similarities of different queries coming from same sentences.
method_label: There are two advantages of MSRC: First, it sidesteps the performance upper bound from independent proposal generation systems by adopting regression mechanism.
method_label: Second, MSRC not only encodes the semantics of a query phrase, but also considers its relation with context (i.e., other queries from the same sentence) via a context refinement network.
result_label: Experiments show MSRC system achieves a significant improvement in accuracy on two popular datasets: Flickr30K Entities and Refer-it Game, with 6.64 and 5.28% increase over the state of the arts, respectively.

===================================
paper_id: 10749859; YEAR: 2003
adju relevance: Irrelevant (0)
difference: 1; annotator1: 0; annotator2: 1
sources: title_tfidfcbow200 - title_cbow200
TITLE: Meta-Learning Orthographic and Contextual Models for Language Independent Named Entity Recognition
ABSTRACT: background_label: This paper presents a named entity classification system that utilises both orthographic and contextual information.
method_label: The random subspace method was employed to generate and refine attribute models.
method_label: Supervised and unsupervised learning techniques used in the recombination of models to produce the final results.

===================================
paper_id: 31491993; YEAR: 2004
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator2: 0
sources: title_tfidf
TITLE: Language design methods based on semantic principles
ABSTRACT: background_label: Two language design methods based on principles derived from the denotational approach to programming language semantics are described and illustrated by an application to the language Pascal.
method_label: The principles are, firstly, the correspondence between parametric and declarative mechanisms, and secondly, a principle of abstraction for programming languages adapted from set theory.
method_label: Several useful extensions and generalizations of Pascal emerge by applying these principles, including a solution to the array parameter problem, and a modularization facility.

===================================
paper_id: 47390681; YEAR: 1989
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator2: 0
sources: cited
TITLE: Word Association Norms, Mutual Information, And Lexicography
ABSTRACT: background_label: The term word association is used in a very particular sense in the psycholinguistic literature.
background_label: (Generally speaking, subjects respond quicker than normal to the word "nurse" if it follows a highly associated word such as "doctor.")
background_label: We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntactic co-occurrence constraints between verbs and prepositions (content word/function word).
objective_label: This paper will propose a new objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora.
method_label: (The standard method of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly and unreliable.)
result_label: The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words.

===================================
paper_id: 430897; YEAR: 2010
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator2: 0
sources: abs_cbow200
TITLE: Integrating Joint n-gram Features into a Discriminative Training Framework
ABSTRACT: background_label: AbstractPhonetic string transduction problems, such as letter-to-phoneme conversion and name transliteration, have recently received much attention in the NLP community.
background_label: In the past few years, two methods have come to dominate as solutions to supervised string transduction: generative joint n-gram models, and discriminative sequence models.
background_label: Both approaches benefit from their ability to consider large, flexible spans of source context when making transduction decisions.
background_label: However, they encode this context in different ways, providing their respective models with different information.
method_label: To combine the strengths of these two systems, we include joint n-gram features inside a state-of-the-art discriminative sequence model.
method_label: We evaluate our approach on several letter-to-phoneme and transliteration data sets.
result_label: Our results indicate an improvement in overall performance with respect to both the joint n-gram approach and traditional feature sets for discriminative models.

===================================
paper_id: 40420741; YEAR: 2009
adju relevance: Irrelevant (0)
difference: 1; annotator1: 0; annotator2: 1
sources: abs_cbow200 - specter
TITLE: An investigation into feature construction to assist word sense disambiguation
ABSTRACT: background_label: Identifying the correct sense of a word in context is crucial for many tasks in natural language processing (machine translation is an example).
background_label: State-of-the art methods for Word Sense Disambiguation (WSD) build models using hand-crafted features that usually capturing shallow linguistic information.
background_label: Complex background knowledge, such as semantic relationships, are typically either not used, or used in specialised manner, due to the limitations of the feature-based modelling techniques used.
background_label: On the other hand, empirical results from the use of Inductive Logic Programming (ILP) systems have repeatedly shown that they can use diverse sources of background knowledge when constructing models.
objective_label: In this paper, we investigate whether this ability of ILP systems could be used to improve the predictive accuracy of models for WSD.
result_label: Specifically, we examine the use of a general-purpose ILP system as a method to construct a set of features using semantic, syntactic and lexical information.
background_label: This feature-set is then used by a common modelling technique in the field (a support vector machine) to construct a classifier for predicting the sense of a word.
method_label: In our investigation we examine one-shot and incremental approaches to feature-set construction applied to monolingual and bilingual WSD tasks.
method_label: The monolingual tasks use 32 verbs and 85 verbs and nouns (in English) from the SENSEVAL-3 and SemEval-2007 benchmarks; while the bilingual WSD task consists of 7 highly ambiguous verbs in translating from English to Portuguese.
result_label: The results are encouraging: the ILP-assisted models show substantial improvements over those that simply use shallow features.
result_label: In addition, incremental feature-set construction appears to identify smaller and better sets of features.
result_label: Taken together, the results suggest that the use of ILP with diverse sources of background knowledge provide a way for making substantial progress in the field of WSD.

===================================
paper_id: 11202365; YEAR: 1996
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator2: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Integrating Multiple Knowledge Sources to Disambiguate Word Sense: An Exemplar-Based Approach
ABSTRACT: background_label: In this paper, we present a new approach for word sense disambiguation (WSD) using an exemplar-based learning algorithm.
method_label: This approach integrates a diverse set of knowledge sources to disambiguate word sense, including part of speech of neighboring words, morphological form, the unordered set of surrounding words, local collocations, and verb-object syntactic relation.
method_label: We tested our WSD program, named {\sc Lexas}, on both a common data set used in previous work, as well as on a large sense-tagged corpus that we separately constructed.
result_label: {\sc Lexas} achieves a higher accuracy on the common data set, and performs better than the most frequent heuristic on the highly ambiguous words in the large corpus tagged with the refined senses of {\sc WordNet}.

===================================
paper_id: 4819317; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator2: 0
sources: abs_cbow200
TITLE: How useful are corpus-based methods for extrapolating psycholinguistic variables?
ABSTRACT: background_label: Subjective ratings for age of acquisition, concreteness, affective valence, and many other variables are an important element of psycholinguistic research.
background_label: However, even for well-studied languages, ratings usually cover just a small part of the vocabulary.
objective_label: A possible solution involves using corpora to build a semantic similarity space and to apply machine learning techniques to extrapolate existing ratings to previously unrated words.
method_label: We conduct a systematic comparison of two extrapolation techniques: k-nearest neighbours, and random forest, in combination with semantic spaces built using latent semantic analysis, topic model, a hyperspace analogue to language (HAL)-like model, and a skip-gram model.
method_label: A variant of the k-nearest neighbours method used with skip-gram word vectors gives the most accurate predictions but the random forest method has an advantage of being able to easily incorporate additional predictors.
method_label: We evaluate the usefulness of the methods by exploring how much of the human performance in a lexical decision task can be explained by extrapolated ratings for age of acquisition and how precisely we can assign words to discrete categories based on extrapolated ratings.
result_label: We find that at least some of the extrapolation methods may introduce artefacts to the data and produce results that could lead to different conclusions that would be reached based on the human ratings.
result_label: From a practical point of view, the usefulness of ratings extrapolated with the described methods may be limited.

===================================
paper_id: 14483066; YEAR: 2012
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator2: 0
sources: abs_tfidfcbow200 - abs_cbow200 - abs_tfidf
TITLE: Semi-Supervised and Latent-Variable Models of Natural Language Semantics
ABSTRACT: background_label: This thesis focuses on robust analysis of natural language semantics.
background_label: A primary bottleneck for semantic processing of text lies in the scarcity of high-quality and large amounts of annotated data that provide complete information about the semantic structure of natural language expressions.
objective_label: In this dissertation, we study statistical models tailored to solve problems in computational semantics, with a focus on modeling structure that is not visible in annotated text data.
method_label: We first investigate supervised methods for modeling two kinds of semantic phenomena in language.
method_label: First, we focus on the problem of paraphrase identification, which attempts to recognize whether two sentences convey the same meaning.
method_label: Second, we concentrate on shallow semantic parsing, adopting the theory of frame semantics (Fillmore, 1982).
method_label: Frame semantics offers deep linguistic analysis that exploits the use of lexical semantic properties and relationships among semantic frames and roles.
result_label: Unfortunately, the datasets used to train our paraphrase and frame-semantic parsing models are too small to lead to robust performance.
background_label: Therefore, a common trait in our methods is the hypothesis of hidden structure in the data.
method_label: To this end, we employ conditional loglinear models over structures, that are firstly capable of incorporating a wide variety of features gathered from the data as well as various lexica, and secondly use latent variables to model missing information in annotated data.
method_label: Our approaches towards solving these two problems achieve state-of-the-art accuracy on standard corpora.
method_label: For the frame-semantic parsing problem, we present fast inference techniques for jointly modeling the semantic roles of a given predicate.
method_label: We experiment with linear program formulations, and use a commercial solver as well as an exact dual decomposition technique that breaks the role labeling problem into several overlapping components.
method_label: Continuing with the theme of hypothesizing hidden structure in data for modeling natural language semantics, we present methods to leverage large volumes of unlabeled data to improve upon the shallow semantic parsing task.
method_label: We work within the framework of graph-based semi-supervised learning, a powerful method that associates similar natural language types, and helps propagate supervised annotations to unlabeled data.
method_label: We use this framework to improve frame-semantic parsing performance on unknown predicates that are absent in annotated data.
result_label: We also present a family of novel

===================================
paper_id: 15483540; YEAR: 2008
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator2: 0
sources: title_cbow200 - title_tfidfcbow200 - title_tfidf
TITLE: A Computational Model to Disentangle Semantic Information Embedded in Word Association Norms
ABSTRACT: background_label: Two well-known databases of semantic relationships between pairs of words used in psycholinguistics, feature-based and association-based, are studied as complex networks.
objective_label: We propose an algorithm to disentangle feature based relationships from free association semantic networks.
method_label: The algorithm uses the rich topology of the free association semantic network to produce a new set of relationships between words similar to those observed in feature production norms.

===================================
paper_id: 5735444; YEAR: 1997
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator2: 0
sources: cited
TITLE: Language model adaptation using mixtures and an exponentially decaying cache
ABSTRACT: background_label: Presents two techniques for language model adaptation.
method_label: The first is based on the use of mixtures of language models: the training text is partitioned according to topic, a language model is constructed for each component and, at recognition time, appropriate weightings are assigned to each component to model the observed style of language.
method_label: The second technique is based on augmenting the standard trigram model with a cache component in which the words' recurrence probabilities decay exponentially over time.
result_label: Both techniques yield a significant reduction in perplexity over the baseline trigram language model when faced with a multi-domain test text, the mixture-based model giving a 24% reduction and the cache-based model giving a 14% reduction.
result_label: The two techniques attack the problem of adaptation at different scales, and as a result can be used in parallel to give a total perplexity reduction of 30%.

===================================
paper_id: 6583976; YEAR: 2000
adju relevance: Irrelevant (0)
difference: 2; annotator1: 0; annotator2: 2
sources: title_tfidfcbow200 - abs_tfidfcbow200 - title_cbow200 - specter
TITLE: Improving Chunking by Means of Lexical-Contextual Information in Statistical Language Models
ABSTRACT: background_label: In this work, we present a stochastic approach to shallow parsing.
background_label: Most of the current approaches to shallow parsing have a common characteristic: they take the sequence of lexical tags proposed by a POS tagger as input for the chunking process.
method_label: Our system produces tagging and chunking in a single process using an Integrated Language Model (ILM) formalized as Markov Models.
method_label: This model integrates several knowledge sources: lexical probabilities, a contextual Language Model (LM) for every chunk, and a contextual LM for the sentences.
method_label: We have extended the ILM by adding lexical information to the contextual LMs.
method_label: We have applied this approach to the CoNLL-2000 shared task improving the performance of the chunker.

===================================
paper_id: 262273; YEAR: 2010
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator2: 0
sources: specter - abs_cbow200
TITLE: Discriminative Word Alignment by Linear Modeling
ABSTRACT: background_label: Word alignment plays an important role in many NLP tasks as it indicates the correspondence between words in a parallel text.
background_label: Although widely used to align large bilingual corpora, generative models are hard to extend to incorporate arbitrary useful linguistic information.
objective_label: This article presents a discriminative framework for word alignment based on a linear model.
method_label: Within this framework, all knowledge sources are treated as feature functions, which depend on a source language sentence, a target language sentence, and the alignment between them.
method_label: We describe a number of features that could produce symmetric alignments.
method_label: Our model is easy to extend and can be optimized with respect to evaluation metrics directly.
method_label: The model achieves state-of-the-art alignment quality on three word alignment shared tasks for five language pairs with varying divergence and richness of resources.
result_label: We further show that our approach improves translation performance for various statistical machine translation systems.

===================================
paper_id: 2524712; YEAR: 2009
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator2: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: A Unified Model of Phrasal and Sentential Evidence for Information Extraction
ABSTRACT: background_label: Information Extraction (IE) systems that extract role fillers for events typically look at the local context surrounding a phrase when deciding whether to extract it.
background_label: Often, however, role fillers occur in clauses that are not directly linked to an event word.
method_label: We present a new model for event extraction that jointly considers both the local context around a phrase along with the wider sentential context in a probabilistic framework.
method_label: Our approach uses a sentential event recognizer and a plausible role-filler recognizer that is conditioned on event sentences.
result_label: We evaluate our system on two IE data sets and show that our model performs well in comparison to existing IE systems that rely on local phrasal context.

===================================
paper_id: 7443908; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator2: 0
sources: title_cbow200 - specter - title_tfidf
TITLE: Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling
ABSTRACT: background_label: Recurrent neural networks have been very successful at predicting sequences of words in tasks such as language modeling.
background_label: However, all such models are based on the conventional classification framework, where the model is trained against one-hot targets, and each word is represented both as an input and as an output in isolation.
background_label: This causes inefficiencies in learning both in terms of utilizing all of the information and in terms of the number of parameters needed to train.
method_label: We introduce a novel theoretical framework that facilitates better learning in language modeling, and show that our framework leads to tying together the input embedding and the output projection matrices, greatly reducing the number of trainable variables.
result_label: Our framework leads to state of the art performance on the Penn Treebank with a variety of network models.

===================================
paper_id: 16889062; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator2: 0
sources: specter - title_tfidf
TITLE: Character-Word LSTM Language Models
ABSTRACT: background_label: We present a Character-Word Long Short-Term Memory Language Model which both reduces the perplexity with respect to a baseline word-level language model and reduces the number of parameters of the model.
method_label: Character information can reveal structural (dis)similarities between words and can even be used when a word is out-of-vocabulary, thus improving the modeling of infrequent and unknown words.
result_label: By concatenating word and character embeddings, we achieve up to 2.77% relative improvement on English compared to a baseline model with a similar amount of parameters and 4.57% on Dutch.
result_label: Moreover, we also outperform baseline word-level models with a larger number of parameters.

===================================
paper_id: 160009395; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator2: 0
sources: title_cbow200
TITLE: Enriching Pre-trained Language Model with Entity Information for Relation Classification
ABSTRACT: background_label: Relation classification is an important NLP task to extract relations between entities.
background_label: The state-of-the-art methods for relation classification are primarily based on Convolutional or Recurrent Neural Networks.
background_label: Recently, the pre-trained BERT model achieves very successful results in many NLP classification / sequence labeling tasks.
background_label: Relation classification differs from those tasks in that it relies on information of both the sentence and the two target entities.
method_label: In this paper, we propose a model that both leverages the pre-trained BERT language model and incorporates information from the target entities to tackle the relation classification task.
method_label: We locate the target entities and transfer the information through the pre-trained architecture and incorporate the corresponding encoding of the two entities.
result_label: We achieve significant improvement over the state-of-the-art method on the SemEval-2010 task 8 relational dataset.

===================================
paper_id: 15965101; YEAR: 2003
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator2: 0
sources: title_tfidf
TITLE: A semantic-based approach to component retrieval
ABSTRACT: background_label: There continues to be a great deal of pressure to design and develop information systems within a short period of time.
background_label: This urgency has reinvigorated research on software reuse, particularly in component based software development.
background_label: One of the major problems associated with component-based development is the difficulty in searching and retrieving reusable components that meet the requirement at hand.
background_label: In part, this problem exists because of the lack of sophisticated query methods and techniques.
objective_label: In this research, a semantic-based approach to component retrieval is presented as a solution to this problem.
method_label: This approach makes use of domain models containing the objectives, processes, actions, actors, and, an ontology of domain terms, their definitions, and relationships with other domain-specific terms.
method_label: A reuse repository is developed that contains the components relevant for the creation of new applications, along with their attributes and methods.
method_label: The natural language interface, domain model, and reusable repository are implemented in a prototype that uses Web and JavaBeans technologies.
result_label: A sample session is provided for an online auction application to illustrate the usefulness of the proposed approach.

===================================
paper_id: 9310292; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator2: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: An Iterative Deep Learning Framework for Unsupervised Discovery of Speech Features and Linguistic Units with Applications on Spoken Term Detection
ABSTRACT: objective_label: In this work we aim to discover high quality speech features and linguistic units directly from unlabeled speech data in a zero resource scenario.
background_label: The results are evaluated using the metrics and corpora proposed in the Zero Resource Speech Challenge organized at Interspeech 2015.
method_label: A Multi-layered Acoustic Tokenizer (MAT) was proposed for automatic discovery of multiple sets of acoustic tokens from the given corpus.
method_label: Each acoustic token set is specified by a set of hyperparameters that describe the model configuration.
method_label: These sets of acoustic tokens carry different characteristics fof the given corpus and the language behind, thus can be mutually reinforced.
method_label: The multiple sets of token labels are then used as the targets of a Multi-target Deep Neural Network (MDNN) trained on low-level acoustic features.
method_label: Bottleneck features extracted from the MDNN are then used as the feedback input to the MAT and the MDNN itself in the next iteration.
method_label: We call this iterative deep learning framework the Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN), which generates both high quality speech features for the Track 1 of the Challenge and acoustic tokens for the Track 2 of the Challenge.
method_label: In addition, we performed extra experiments on the same corpora on the application of query-by-example spoken term detection.
result_label: The experimental results showed the iterative deep learning framework of MAT-DNN improved the detection performance due to better underlying speech features and acoustic tokens.

===================================
paper_id: 17658558; YEAR: 2003
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator2: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: A Bayesian framework for fusing multiple word knowledge models in videotext recognition
ABSTRACT: background_label: Videotext recognition is challenging due to low resolution, diverse fonts/styles, and cluttered background.
background_label: Past methods enhanced recognition by using multiple frame averaging, image interpolation and lexicon correction, but recognition using multi-modality language models has not been explored.
method_label: In this paper, we present a formal Bayesian framework for videotext recognition by combining multiple knowledge using mixture models, and describe a learning approach based on Expectation-Maximization (EM).
method_label: In order to handle unseen words, a back-off smoothing approach derived from the Bayesian model is also presented.
method_label: We exploited a prototype that fuses the model from closed caption and that from the British National Corpus.
method_label: The model from closed caption is based on a unique time distance distribution model of videotext words and closed caption words.
method_label: Our method achieves a significant performance gain, with word recognition rate of 76.8% and character recognition rate of 86.7%.
result_label: The proposed methods also reduce false videotext detection significantly, with a false alarm rate of 8.2% without substantial loss of recall.

===================================
paper_id: 70159782; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator2: 0
sources: abs_tfidfcbow200 - abs_cbow200 - abs_tfidf
TITLE: Word Vector Embeddings and Domain Specific Semantic based Semi-Supervised Ontology Instance Population
ABSTRACT: background_label: An ontology defines a set of representational primitives which model a domain of knowledge or discourse.
background_label: With the arising fields such as information extraction and knowledge management, the role of ontology has become a driving factor of many modern day systems.
background_label: Ontology population, on the other hand, is an inherently problematic process, as it needs manual intervention to prevent the conceptual drift.
background_label: The semantic sensitive word embedding has become a popular topic in natural language processing with its capability to cope with the semantic challenges.
method_label: Incorporating domain specific semantic similarity with the word embeddings could potentially improve the performance in terms of semantic similarity in specific domains.
objective_label: Thus, in this study, we propose a novel way of semi-supervised ontology population through word embeddings and domain specific semantic similarity as the basis.
method_label: We built several models including traditional benchmark models and new types of models which are based on word embeddings.
result_label: Finally, we ensemble them together to come up with a synergistic model which outperformed the candidate models by 33% in comparison to the best performed candidate model.

===================================
paper_id: 36117198; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator2: 0
sources: title_tfidf
TITLE: DeepMind_Commentary
ABSTRACT: background_label: We agree with Lake and colleagues on their list of key ingredients for building humanlike intelligence, including the idea that model-based reasoning is essential.
background_label: However, we favor an approach that centers on one additional ingredient: autonomy.
objective_label: In particular, we aim toward agents that can both build and exploit their own internal models, with minimal human hand-engineering.
method_label: We believe an approach centered on autonomous learning has the greatest chance of success as we scale toward real-world complexity, tackling domains for which ready-made formal models are not available.
result_label: Here we survey several important examples of the progress that has been made toward building autonomous agents with humanlike abilities, and highlight some outstanding challenges.

===================================
paper_id: 26442775; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator2: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Regularized and Retrofitted models for Learning Sentence Representation with Context
ABSTRACT: background_label: Vector representation of sentences is important for many text processing tasks that involve classifying, clustering, or ranking sentences.
background_label: For solving these tasks, bag-of-word based representation has been used for a long time.
background_label: In recent years, distributed representation of sentences learned by neural models from unlabeled data has been shown to outperform traditional bag-of-words representations.
method_label: However, most existing methods belonging to the neural models consider only the content of a sentence, and disregard its relations with other sentences in the context.
method_label: In this paper, we first characterize two types of contexts depending on their scope and utility.
method_label: We then propose two approaches to incorporate contextual information into content-based models.
method_label: We evaluate our sentence representation models in a setup, where context is available to infer sentence vectors.
result_label: Experimental results demonstrate that our proposed models outshine existing models on three fundamental tasks, such as, classifying, clustering, and ranking sentences.

===================================
paper_id: 3178630; YEAR: 2008
adju relevance: Irrelevant (0)
difference: 1; annotator1: 0; annotator2: 1
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: Enhancing text clustering by leveraging Wikipedia semantics
ABSTRACT: background_label: Most traditional text clustering methods are based on "bag of words" (BOW) representation based on frequency statistics in a set of documents.
background_label: BOW, however, ignores the important information on the semantic relationships between key terms.
background_label: To overcome this problem, several methods have been proposed to enrich text representation with external resource in the past, such as WordNet.
background_label: However, many of these approaches suffer from some limitations: 1) WordNet has limited coverage and has a lack of effective word-sense disambiguation ability; 2) Most of the text representation enrichment strategies, which append or replace document terms with their hypernym and synonym, are overly simple.
method_label: In this paper, to overcome these deficiencies, we first propose a way to build a concept thesaurus based on the semantic relations (synonym, hypernym, and associative relation) extracted from Wikipedia.
method_label: Then, we develop a unified framework to leverage these semantic relations in order to enhance traditional content similarity measure for text clustering.
result_label: The experimental results on Reuters and OHSUMED datasets show that with the help of Wikipedia thesaurus, the clustering performance of our method is improved as compared to previous methods.
result_label: In addition, with the optimized weights for hypernym, synonym, and associative concepts that are tuned with the help of a few labeled data users provided, the clustering performance can be further improved.

===================================
paper_id: 119425731; YEAR: 1972
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator2: 0
sources: title_tfidf
TITLE: Unzerlegbare Darstellungen I
ABSTRACT: background_label: LetK be the structure got by forgetting the composition law of morphisms in a given category.
background_label: A linear representation ofK is given by a map V associating with any morphism ϕ: a→e ofK a linear vector space map V(ϕ): V(a)→V(e).
method_label: We classify thoseK having only finitely many isomorphy classes of indecomposable linear representations.
other_label: This classification is related to an old paper by Yoshii [3].

===================================
paper_id: 7357656; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 1; annotator1: 0; annotator2: 1
sources: title_cbow200 - specter - abs_tfidf - title_tfidf
TITLE: Enhancing Translation Language Models with Word Embedding for Information Retrieval
ABSTRACT: background_label: In this paper, we explore the usage of Word Embedding semantic resources for Information Retrieval (IR) task.
background_label: This embedding, produced by a shallow neural network, have been shown to catch semantic similarities between words (Mikolov et al., 2013).
objective_label: Hence, our goal is to enhance IR Language Models by addressing the term mismatch problem.
method_label: To do so, we applied the model presented in the paper Integrating and Evaluating Neural Word Embedding in Information Retrieval by Zuccon et al.
method_label: (2015) that proposes to estimate the translation probability of a Translation Language Model using the cosine similarity between Word Embedding.
result_label: The results we obtained so far did not show a statistically significant improvement compared to classical Language Model.

===================================
paper_id: 8368652; YEAR: 1993
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator2: 0
sources: abs_tfidfcbow200 - abs_cbow200
TITLE: Data-Oriented Methods for Grapheme-to-Phoneme Conversion
ABSTRACT: background_label: It is traditionally assumed that various sources of linguistic knowledge and their interaction should be formalised in order to be able to convert words into their phonemic representations with reasonable accuracy.
objective_label: We show that using supervised learning techniques, based on a corpus of transcribed words, the same and even better performance can be achieved, without explicit modeling of linguistic knowledge.In this paper we present two instances of this approach.
method_label: A first model implements a variant of instance-based learning, in which a weighed similarity metric and a database of prototypical exemplars are used to predict new mappings.
method_label: In the second model, grapheme-to-phoneme mappings are looked up in a compressed text-to-speech lexicon (table lookup) enriched with default mappings.
result_label: We compare performance and accuracy of these approaches to a connectionist (backpropagation) approach and to the linguistic knowledge-based approach.

===================================
paper_id: 10028211; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator2: 0
sources: title_cbow200 - title_tfidfcbow200 - specter
TITLE: Jointly optimizing word representations for lexical and sentential tasks with the C-PHRASE model
ABSTRACT: method_label: We introduce C-PHRASE, a distributional semantic model that learns word representations by optimizing context prediction for phrases at all levels in a syntactic tree, from single words to full sentences.
background_label: C-PHRASE outperforms the state-of-theart C-BOW model on a variety of lexical tasks.
method_label: Moreover, since C-PHRASE word vectors are induced through a compositional learning objective (modeling the contexts of words combined into phrases), when they are summed, they produce sentence representations that rival those generated by ad-hoc compositional models.

===================================
paper_id: 8150809; YEAR: 2000
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator2: 0
sources: cited
TITLE: Entropy-based Pruning of Backoff Language Models
ABSTRACT: background_label: A criterion for pruning parameters from N-gram backoff language models is developed, based on the relative entropy between the original and the pruned model.
background_label: It is shown that the relative entropy resulting from pruning a single N-gram can be computed exactly and efficiently for backoff models.
method_label: The relative entropy measure can be expressed as a relative change in training set perplexity.
method_label: This leads to a simple pruning criterion whereby all N-grams that change perplexity by less than a threshold are removed from the model.
method_label: Experiments show that a production-quality Hub4 LM can be reduced to 26% its original size without increasing recognition error.
method_label: We also compare the approach to a heuristic pruning criterion by Seymore and Rosenfeld (1996), and show that their approach can be interpreted as an approximation to the relative entropy criterion.
result_label: Experimentally, both approaches select similar sets of N-grams (about 85% overlap), with the exact relative entropy criterion giving marginally better performance.

===================================
paper_id: 5802722; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator2: 0
sources: abs_tfidfcbow200
TITLE: Exploiting Sentence and Context Representations in Deep Neural Models for Spoken Language Understanding
ABSTRACT: background_label: This paper presents a deep learning architecture for the semantic decoder component of a Statistical Spoken Dialogue System.
background_label: In a slot-filling dialogue, the semantic decoder predicts the dialogue act and a set of slot-value pairs from a set of n-best hypotheses returned by the Automatic Speech Recognition.
method_label: Most current models for spoken language understanding assume (i) word-aligned semantic annotations as in sequence taggers and (ii) delexicalisation, or a mapping of input words to domain-specific concepts using heuristics that try to capture morphological variation but that do not scale to other domains nor to language variation (e.g., morphology, synonyms, paraphrasing ).
method_label: In this work the semantic decoder is trained using unaligned semantic annotations and it uses distributed semantic representation learning to overcome the limitations of explicit delexicalisation.
method_label: The proposed architecture uses a convolutional neural network for the sentence representation and a long-short term memory network for the context representation.
result_label: Results are presented for the publicly available DSTC2 corpus and an In-car corpus which is similar to DSTC2 but has a significantly higher word error rate (WER).

===================================
paper_id: 52010945; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 1; annotator1: 0; annotator2: 1
sources: title_tfidf
TITLE: On-Device Neural Language Model Based Word Prediction
ABSTRACT: background_label: AbstractRecent developments in deep learning with application to language modeling have led to success in tasks of text processing, summarizing and machine translation.
background_label: However, deploying huge language models on mobile devices for on-device keyboards poses computation as a bottle-neck due to their puny computation capacities.
method_label: In this work, we propose an on-device neural language model based word prediction method that optimizes run-time memory and also provides a realtime prediction environment.
method_label: Our model size is 7.40MB and has average prediction time of 6.47 ms.
result_label: The proposed model outperforms existing methods for word prediction in terms of keystroke savings and word prediction rate and has been successfully commercialized.

===================================
paper_id: 7828379; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 2; annotator1: 2; annotator2: 0
sources: abs_tfidfcbow200
TITLE: Semantic Textual Similarity Methods, Tools, and Applications: A Survey
ABSTRACT: background_label: Measuring Semantic Textual Similarity (STS), between words/ terms, sentences, paragraph and document plays an important role in computer science and computational linguistic.
background_label: It also has many application sover several fields such as Biomedical Informatics and Geoinformation.
objective_label: In this paper, we present a survey on different methods of textual similarity and we also reported about the availability of different software and tools those are useful for STS.
method_label: In natural language processing (NLP), STS is a important component formany tasks such as document summarization, word sense disambiguation, short answer grading, information retrieval and extraction.
method_label: We split out the measures for semantic similarity into three broad categories such as (i) Topological/Knowledge-based (ii) Statistical/Corpus Based (iii) String based.
method_label: More emphasisi s given to the methods related to the WordNet taxonomy.
method_label: Because topological methods, plays an important role to understand intended meaning of an ambiguous word, which is very difficult to process computationally.
method_label: We also propose a new method for measuring semantic similarity between sentences.
method_label: This proposed method, uses the advantages of taxonomy methods and merge these information to a language model.
method_label: It considers the WordNet synsets for lexical relationships between nodes/words and a uni-gram language model is implemented over a large corpus to assign the information content value between the two nodes of different classes.

===================================
paper_id: 15390595; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator2: 0
sources: title_cbow200 - title_tfidfcbow200 - title_tfidf
TITLE: Random walks on discourse spaces: a new generative language model with applications to semantic word embeddings
ABSTRACT: background_label: Semantic word embeddings use vector representations to represent the meaning of a word.
background_label: Methods to create them include Vector Space Methods (VSMs) such as Latent Semantic Analysis (LSA), matrix factorization, generative text models such as Topic Models, and neural nets.
background_label: A flurry of work has resulted from the papers of Mikolov et al.~\cite{mikolov2013efficient}.
background_label: These showed how to solve word analogy tasks very well by leveraging linear structure in word embeddings even though the embeddings were created using highly nonlinear energy based models.
background_label: No clear explanation is known why such linear structure emerges in low-dimensional embeddings.
method_label: This paper presents a loglinear generative model---related to~\citet{mnih2007three}---that models the generation of a text corpus as a random walk in a latent discourse space.
method_label: A novel methodological twist is that the model is solved in closed form by integrating out the random walk.
method_label: This yields a simple method for constructing word embeddings.
method_label: Experiments are presented to support the modeling assumptions as well as the efficacy of the word embeddings for solving analogies.
result_label: This simple model links and provides theoretical support for several prior methods for finding embeddings, as well as provides interpretations for various linear algebraic structures in word embeddings obtained from nonlinear techniques.

===================================
paper_id: 18178267; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator2: 0
sources: title_tfidfcbow200 - title_cbow200 - abs_cbow200
TITLE: Linked knowledge sources for topic classification of microposts: A semantic graph-based approach
ABSTRACT: background_label: AbstractShort text messages a.k.a Microposts (e.g.
background_label: Tweets) have proven to be an effective channel for revealing information about trends and events, ranging from those related to Disaster (e.g.
background_label: hurricane Sandy) to those related to Violence (e.g.
background_label: Egyptian revolution).
objective_label: Being informed about such events as they occur could be extremely important to authorities and emergency professionals by allowing such parties to immediately respond.In this work we study the problem of topic classification (TC) of Microposts, which aims to automatically classify short messages based on the subject(s) discussed in them.
method_label: The accurate TC of Microposts however is a challenging task since the limited number of tokens in a post often implies a lack of sufficient contextual information.In order to provide contextual information to Microposts, we present and evaluate several graph structures surrounding concepts present in linked knowledge sources (KSs).
result_label: Traditional TC techniques enrich the content of Microposts with features extracted only from the Microposts content.
background_label: In contrast our approach relies on the generation of different weighted semantic meta-graphs extracted from linked KSs.
background_label: We introduce a new semantic graph, called category meta-graph.
background_label: This novel meta-graph provides a more fine grained categorisation of concepts providing a set of novel semantic features.
objective_label: Our findings show that such category meta-graph features effectively improve the performance of a topic classifier of Microposts.Furthermore our goal is also to understand which semantic feature contributes to the performance of a topic classifier.
objective_label: For this reason we propose an approach for automatic estimation of accuracy loss of a topic classifier on new, unseen Microposts.
result_label: We introduce and evaluate novel topic similarity measures, which capture the similarity between the KS documents and Microposts at a conceptual level, considering the enriched representation of these documents.Extensive evaluation in the context of Emergency Response (ER) and Violence Detection (VD) revealed that our approach outperforms previous approaches using single KS without linked data and Twitter data only up to 31.4% in terms of F1 measure.
result_label: Our main findings indicate that the new category graph contains useful information for TC and achieves comparable results to previously used semantic graphs.
result_label: Furthermore our results also indicate that the accuracy of a topic classifier can be accurately predicted using the enhanced text representation, outperforming previous approaches considering content-based similarity measures.

===================================
paper_id: 3137136; YEAR: 2002
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator2: 0
sources: title_tfidf
TITLE: Semantic Component Composition
ABSTRACT: background_label: Building complex software systems necessitates the use of component-based architectures.
background_label: In theory, of the set of components needed for a design, only some small portion of them are"custom"; the rest are reused or refactored existing pieces of software.
background_label: Unfortunately, this is an idealized situation.
background_label: Just because two components should work together does not mean that they will work together.
background_label: The"glue"that holds components together is not just technology.
background_label: The contracts that bind complex systems together implicitly define more than their explicit type.
method_label: These"conceptual contracts"describe essential aspects of extra-system semantics: e.g., object models, type systems, data representation, interface action semantics, legal and contractual obligations, and more.
result_label: Designers and developers spend inordinate amounts of time technologically duct-taping systems to fulfill these conceptual contracts because system-wide semantics have not been rigorously characterized or codified.
result_label: This paper describes a formal characterization of the problem and discusses an initial implementation of the resulting theoretical system.

===================================
paper_id: 121164; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 1; annotator1: 0; annotator2: 1
sources: title_tfidfcbow200 - title_cbow200
TITLE: Entity Type Recognition using an Ensemble of Distributional Semantic Models to Enhance Query Understanding
ABSTRACT: background_label: We present an ensemble approach for categorizing search query entities in the recruitment domain.
background_label: Understanding the types of entities expressed in a search query (Company, Skill, Job Title, etc.)
background_label: enables more intelligent information retrieval based upon those entities compared to a traditional keyword-based search.
background_label: Because search queries are typically very short, leveraging a traditional bag-of-words model to identify entity types would be inappropriate due to the lack of contextual information.
method_label: Our approach instead combines clues from different sources of varying complexity in order to collect real-world knowledge about query entities.
method_label: We employ distributional semantic representations of query entities through two models: 1) contextual vectors generated from encyclopedic corpora like Wikipedia, and 2) high dimensional word embedding vectors generated from millions of job postings using word2vec.
background_label: Additionally, our approach utilizes both entity linguistic properties obtained from WordNet and ontological properties extracted from DBpedia.
background_label: We evaluate our approach on a data set created at CareerBuilder; the largest job board in the US.
method_label: The data set contains entities extracted from millions of job seekers/recruiters search queries, job postings, and resume documents.
method_label: After constructing the distributional vectors of search entities, we use supervised machine learning to infer search entity types.
result_label: Empirical results show that our approach outperforms the state-of-the-art word2vec distributional semantics model trained on Wikipedia.
result_label: Moreover, we achieve micro-averaged F 1 score of 97% using the proposed distributional representations ensemble.

===================================
paper_id: 16960204; YEAR: 2008
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator2: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Utilizing Statistical Semantic Similarity Techniques for Ontology Mapping – with Applications to AEC Standard Models
ABSTRACT: objective_label: The objective of this paper is to introduce three semi-automated approaches for ontology mapping using relatedness analysis techniques.
background_label: In the architecture, engineering, and construction (AEC) industry, there exist a number of ontological standards to describe the semantics of building models.
background_label: Although the standards share similar scopes of interest, the task of comparing and mapping concepts among standards is challenging due to their differences in terminologies and perspectives.
method_label: Ontology mapping is therefore necessary to achieve information interoperability, which allows two or more information sources to exchange data and to re-use the data for further purposes.
method_label: The attribute-based approach, corpus-based approach, and name-based approach presented in this paper adopt the statistical relatedness analysis techniques to discover related concepts from heterogeneous ontologies.
method_label: A pilot study is conducted on IFC and CIS/2 ontologies to evaluate the approaches.
result_label: Preliminary results show that the attribute-based approach outperforms the other two approaches in terms of precision and F-measure.

===================================
paper_id: 3687074; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator2: 0
sources: abs_tfidfcbow200
TITLE: A Resource-Light Method for Cross-Lingual Semantic Textual Similarity
ABSTRACT: background_label: Recognizing semantically similar sentences or paragraphs across languages is beneficial for many tasks, ranging from cross-lingual information retrieval and plagiarism detection to machine translation.
background_label: Recently proposed methods for predicting cross-lingual semantic similarity of short texts, however, make use of tools and resources (e.g., machine translation systems, syntactic parsers or named entity recognition) that for many languages (or language pairs) do not exist.
objective_label: In contrast, we propose an unsupervised and a very resource-light approach for measuring semantic similarity between texts in different languages.
method_label: To operate in the bilingual (or multilingual) space, we project continuous word vectors (i.e., word embeddings) from one language to the vector space of the other language via the linear translation model.
method_label: We then align words according to the similarity of their vectors in the bilingual embedding space and investigate different unsupervised measures of semantic similarity exploiting bilingual embeddings and word alignments.
method_label: Requiring only a limited-size set of word translation pairs between the languages, the proposed approach is applicable to virtually any pair of languages for which there exists a sufficiently large corpus, required to learn monolingual word embeddings.
result_label: Experimental results on three different datasets for measuring semantic textual similarity show that our simple resource-light approach reaches performance close to that of supervised and resource intensive methods, displaying stability across different language pairs.
result_label: Furthermore, we evaluate the proposed method on two extrinsic tasks, namely extraction of parallel sentences from comparable corpora and cross lingual plagiarism detection, and show that it yields performance comparable to those of complex resource-intensive state-of-the-art models for the respective tasks.

===================================
paper_id: 5022757; YEAR: 2010
adju relevance: Irrelevant (0)
difference: 2; annotator1: 2; annotator2: 0
sources: abs_tfidf
TITLE: Sparse latent semantic analysis
ABSTRACT: background_label: AbstractLatent semantic analysis (LSA), as one of the most popular unsupervised dimension reduction tools, has a wide range of applications in text mining and information retrieval.
background_label: The key idea of LSA is to learn a projection matrix that maps the high dimensional vector space representations of documents to a lower dimensional latent space, i.e.
objective_label: so called latent topic space.
method_label: In this paper, we propose a new model called Sparse LSA, which produces a sparse projection matrix via the 1 regularization.
method_label: Compared to the traditional LSA, Sparse LSA selects only a small number of relevant words for each topic and hence provides a compact representation of topic-word relationships.
method_label: Moreover, Sparse LSA is computationally very efficient with much less memory usage for storing the projection matrix.
method_label: Furthermore, we propose two important extensions of Sparse LSA: group structured Sparse LSA and non-negative Sparse LSA.
method_label: We conduct experiments on several benchmark datasets and compare Sparse LSA and its extensions with several widely used methods, e.g.
method_label: LSA, Sparse Coding and LDA.
result_label: Empirical results suggest that Sparse LSA achieves similar performance gains to LSA, but is more efficient in projection computation, storage, and also well explain the topic-word relationships.

===================================
paper_id: 3719083; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator2: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: Spicy Adjectives and Nominal Donkeys: Capturing Semantic Deviance Using Compositionality in Distributional Spaces.
ABSTRACT: background_label: Sophisticated senator and legislative onion.
background_label: Whether or not you have ever heard of these things, we all have some intuition that one of them makes much less sense than the other.
background_label: In this paper, we introduce a large dataset of human judgments about novel adjective-noun phrases.
method_label: We use these data to test an approach to semantic deviance based on phrase representations derived with compositional distributional semantic methods, that is, methods that derive word meanings from contextual information, and approximate phrase meanings by combining word meanings.
method_label: We present several simple measures extracted from distributional representations of words and phrases, and we show that they have a significant impact on predicting the acceptability of novel adjective-noun phrases even when a number of alternative measures classically employed in studies of compound processing and bigram plausibility are taken into account.
result_label: Our results show that the extent to which an attributive adjective alters the distributional representation of the noun is the most significant factor in modeling the distinction between acceptable and deviant phrases.
result_label: Our study extends current applications of compositional distributional semantic methods to linguistically and cognitively interesting problems, and it offers a new, quantitatively precise approach to the challenge of predicting when humans will find novel linguistic expressions acceptable and when they will not.

===================================
paper_id: 18941379; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator2: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Integrating word embeddings and traditional NLP features to measure textual entailment and semantic relatedness of sentence pairs
ABSTRACT: background_label: Recent years the distributed representations of words (i.e., word embeddings) have been shown to be able to significantly improve performance in many natural language processing tasks, such as pos-of-tag tagging, chunking, named entity recognition and sentiment polarity judgement, etc.
background_label: However, previous tasks only involve a single sentence.
objective_label: In contrast, this paper evaluates the effectiveness of word embeddings in sentence pair classification or regression problems.
method_label: Specifically, we propose novel simple yet effective features based on word embeddings and extract many traditional linguistic features.
method_label: Then these features serve as input of a classification/regression algorithm in isolation and in combination.
result_label: Evaluations are conducted on three sentence pair classification/regression tasks, i.e., textual entailment, cross-lingual textual entailment and semantic relatedness estimation.
result_label: Experiments on benchmark datasets provided by Semantic Evaluation 2013 and 2014 showed that using word embeddings is able to significantly improve the performance and our results outperform the best achieved results so far.

===================================
paper_id: 148269094; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator2: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: Explaining human performance in psycholinguistic tasks with models of semantic similarity based on prediction and counting : a review and empirical validation
ABSTRACT: background_label: Abstract Recent developments in distributional semantics (Mikolov, Chen, Corrado, & Dean, 2013; Mikolov, Sutskever, Chen, Corrado, & Dean, 2013) include a new class of prediction-based models that are trained on a text corpus and that measure semantic similarity between words.
objective_label: We discuss the relevance of these models for psycholinguistic theories and compare them to more traditional distributional semantic models.
method_label: We compare the models’ performances on a large dataset of semantic priming (Hutchison et al., 2013) and on a number of other tasks involving semantic processing and conclude that the prediction-based models usually offer a better fit to behavioral data.
result_label: Theoretically, we argue that these models bridge the gap between traditional approaches to distributional semantics and psychologically plausible learning principles.
result_label: As an aid to researchers, we release semantic vectors for English and Dutch for a range of models together with a convenient interface that can be used to extract a great number of semantic similarity measures.

===================================
paper_id: 16334135; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator2: 0
sources: abs_cbow200
TITLE: Leveraging Preposition Ambiguity to Assess Compositional Distributional Models of Semantics
ABSTRACT: background_label: Complex interactions among the meanings of words are important factors in the function that maps word meanings to phrase meanings.
background_label: Recently, compositional distributional semantics models (CDSM) have been designed with the goal of emulating these complex interactions; however, experimental results on the effectiveness of CDSM have been difficult to interpret because the current metrics for assessing them do not control for the confound of lexical information.
method_label: We present a new method for assessing the degree to which CDSM capture semantic interactions that dissociates the influences of lexical and compositional information.
method_label: We then provide a dataset for performing this type of assessment and use it to evaluate six compositional models using both co-occurrence based and neural language model input vectors.
result_label: Results show that neural language input vectors are consistently superior to co-occurrence based vectors, that several CDSM capture substantial compositional information, and that, surprisingly, vector addition matches and is in many cases superior to purpose-built paramaterized models.

===================================
paper_id: 13939950; YEAR: 2012
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator2: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Online learning of concepts and words using multimodal LDA and hierarchical Pitman-Yor Language Model
ABSTRACT: objective_label: In this paper, we propose an online algorithm for multimodal categorization based on the autonomously acquired multimodal information and partial words given by human users.
method_label: For multimodal concept formation, multimodal latent Dirichlet allocation (MLDA) using Gibbs sampling is extended to an online version.
method_label: We introduce a particle filter, which significantly improve the performance of the online MLDA, to keep tracking good models among various models with different parameters.
method_label: We also introduce an unsupervised word segmentation method based on hierarchical Pitman-Yor Language Model (HPYLM).
method_label: Since the HPYLM requires no predefined lexicon, we can make the robot system that learns concepts and words in completely unsupervised manner.
result_label: The proposed algorithms are implemented on a real robot and tested using real everyday objects to show the validity of the proposed system.

===================================
paper_id: 6493812; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator2: 0
sources: abs_tfidfcbow200
TITLE: Clustering Comparable Corpora of Russian and Ukrainian Academic Texts: Word Embeddings and Semantic Fingerprints
ABSTRACT: background_label: We present our experience in applying distributional semantics (neural word embeddings) to the problem of representing and clustering documents in a bilingual comparable corpus.
background_label: Our data is a collection of Russian and Ukrainian academic texts, for which topics are their academic fields.
method_label: In order to build language-independent semantic representations of these documents, we train neural distributional models on monolingual corpora and learn the optimal linear transformation of vectors from one language to another.
method_label: The resulting vectors are then used to produce `semantic fingerprints' of documents, serving as input to a clustering algorithm.
method_label: The presented method is compared to several baselines including `orthographic translation' with Levenshtein edit distance and outperforms them by a large margin.
result_label: We also show that language-independent `semantic fingerprints' are superior to multi-lingual clustering algorithms proposed in the previous work, at the same time requiring less linguistic resources.

===================================
paper_id: 3892300; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator2: 0
sources: abs_cbow200 - abs_tfidfcbow200 - specter
TITLE: Jointly learning word embeddings using a corpus and a knowledge base
ABSTRACT: background_label: Methods for representing the meaning of words in vector spaces purely using the information distributed in text corpora have proved to be very valuable in various text mining and natural language processing (NLP) tasks.
background_label: However, these methods still disregard the valuable semantic relational structure between words in co-occurring contexts.
background_label: These beneficial semantic relational structures are contained in manually-created knowledge bases (KBs) such as ontologies and semantic lexicons, where the meanings of words are represented by defining the various relationships that exist among those words.
method_label: We combine the knowledge in both a corpus and a KB to learn better word embeddings.
method_label: Specifically, we propose a joint word representation learning method that uses the knowledge in the KBs, and simultaneously predicts the co-occurrences of two words in a corpus context.
method_label: In particular, we use the corpus to define our objective function subject to the relational constrains derived from the KB.
method_label: We further utilise the corpus co-occurrence statistics to propose two novel approaches, Nearest Neighbour Expansion (NNE) and Hedged Nearest Neighbour Expansion (HNE), that dynamically expand the KB and therefore derive more constraints that guide the optimisation process.
result_label: Our experimental results over a wide-range of benchmark tasks demonstrate that the proposed method statistically significantly improves the accuracy of the word embeddings learnt.
result_label: It outperforms a corpus-only baseline and reports an improvement of a number of previously proposed methods that incorporate corpora and KBs in both semantic similarity prediction and word analogy detection tasks.

===================================
paper_id: 35212658; YEAR: 2011
adju relevance: Irrelevant (0)
difference: 2; annotator1: 2; annotator2: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: A text-based approach to feature modelling: Syntax and semantics of TVL
ABSTRACT: background_label: a b s t r a c tIn the scientific community, feature models are the de-facto standard for representing variability in software product line engineering.
background_label: This is different from industrial settings where they appear to be used much less frequently.
background_label: We and other authors found that in a number of cases, they lack concision, naturalness and expressiveness.
background_label: This is confirmed by industrial experience.When modelling variability, an efficient tool for making models intuitive and concise are feature attributes.
background_label: Yet, the semantics of feature models with attributes is not well understood and most existing notations do not support them at all.
background_label: Furthermore, the graphical nature of feature models' syntax also appears to be a barrier to industrial adoption, both psychological and rational.
method_label: Existing tool support for graphical feature models is lacking or inadequate, and inferior in many regards to tool support for text-based formats.To overcome these shortcomings, we designed TVL, a text-based feature modelling language.
method_label: In terms of expressiveness, TVL subsumes most existing dialects.
objective_label: The main goal of designing TVL was to provide engineers with a human-readable language with a rich syntax to make modelling easy and models natural, but also with a formal semantics to avoid ambiguity and allow powerful automation.

===================================
paper_id: 5702146; YEAR: 2008
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator2: 0
sources: title_tfidf
TITLE: A Component-Based Model and Language for Wireless Sensor Network Applications
ABSTRACT: background_label: Wireless sensor networks are often used by experts in many different fields to gather data pertinent to their work.
background_label: Although their expertise may not include software engineering, these users are expected to produce low-level software for a concurrent, real-time and resource-constrained computing environment.
objective_label: In this paper, we introduce a component-based model for wireless sensor network applications and a language, Insense, for supporting the model.
method_label: An application is modelled as a composition of interacting components and the application model is preserved in the Insense implementation where active components communicate via typed channels.
method_label: The primary design criteria for Insense include: to abstract over low-level concerns for ease of programming; to permit worst-case space and time usage of programs to be determinable; to support the fractal composition of components whilst eliminating implicit dependencies between them; and, to facilitate the construction of low footprint programs suitable for resource-constrained devices.
result_label: This paper presents an overview of the component model and Insense, and demonstrates how they meet the above criteria.

===================================
paper_id: 118988729; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator2: 0
sources: title_tfidf
TITLE: A Microphotonic Astrocomb
ABSTRACT: background_label: One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers.
background_label: It has already been realized that the large number of exactly known optical frequencies provided by laser frequency combs ('astrocombs') can significantly surpass conventionally used hollow-cathode lamps as calibration light sources.
background_label: A remaining challenge, however, is generation of frequency combs with lines resolvable by astronomical spectrometers.
method_label: Here we demonstrate an astrocomb generated via soliton formation in an on-chip microphotonic resonator ('microresonator') with a resolvable line spacing of 23.7 GHz.
method_label: This comb is providing wavelength calibration on the 10 cm/s radial velocity level on the GIANO-B high-resolution near-infrared spectrometer.
result_label: As such, microresonator frequency combs have the potential of providing broadband wavelength calibration for the next-generation of astronomical instruments in planet-hunting and cosmological research.

===================================
paper_id: 12886327; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator2: 0
sources: title_tfidf
TITLE: An Efficient Algorithm to Integrate Network and Attribute Data for Gene Function Prediction
ABSTRACT: background_label: Label propagation methods are extremely well-suited for a variety of biomedical prediction tasks based on network data.
background_label: However, these algorithms cannot be used to integrate feature-based data sources with networks.
objective_label: We propose an efficient learning algorithm to integrate these two types of heterogeneous data sources to perform binary prediction tasks on node features (e.g., gene prioritization, disease gene prediction).
method_label: Our method, LMGraph, consists of two steps.
method_label: In the first step, we extract a small set of "network features" from the nodes of networks that represent connectivity with labeled nodes in the prediction tasks.
method_label: In the second step, we apply a simple weighting scheme in conjunction with linear classifiers to combine these network features with other feature data.
method_label: This two-step procedure allows us to (i) learn highly scalable and computationally efficient linear classifiers, (ii) and seamlessly combine feature-based data sources with networks.
method_label: Our method is much faster than label propagation which is already known to be computationally efficient on large-scale prediction problems.
result_label: Experiments on multiple functional interaction networks from three species (mouse, y, C.elegans) with tens of thousands of nodes and hundreds of binary prediction tasks demonstrate the efficacy of our method.

===================================
paper_id: 82456167; YEAR: 2007
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator2: 0
sources: title_tfidf
TITLE: Janeway's Immunobiology
ABSTRACT: background_label: Part I An Introduction to Immunobiology and Innate Immunity 1.
background_label: Basic Concepts in Immunology 2.
background_label: Innate Immunity Part II The Recognition of Antigen 3.
background_label: Antigen Recognition by B-cell and T-cell Receptors 4.
method_label: The Generation of Lymphocyte Antigen Receptors 5.
method_label: Antigen Presentation to T Lymphocytes Part III The Development of Mature Lymphocyte Receptor Repertoires 6.
method_label: Signaling Through Immune System Receptors 7.
result_label: The Development and Survival of Lymphocytes Part IV The Adaptive Immune Response 8.
background_label: T Cell-Mediated Immunity 9.
background_label: The Humoral Immune Response 10.
background_label: Dynamics of Adaptive Immunity 11.
other_label: The Mucosal Immune System Part V The Immune System in Health and Disease 12.
background_label: Failures of Host Defense Mechanism 13.
other_label: Allergy and Hypersensitivity 14.
other_label: Autoimmunity and Transplantation 15.
other_label: Manipulation of the Immune Response Part VI The Origins of Immune Responses 16.
other_label: Evolution of the Immune System Appendix I Immunologists' Toolbox Appendix II CD Antigens Appendix III Cytokines and their Receptors Appendix IV Chemokines and their Receptors Appendix V Immunological Constants

