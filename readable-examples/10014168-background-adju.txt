======================================================================
paper_id: 10014168; YEAR: 2002
TITLE: Unsupervised Learning of Morphology without Morphemes
ABSTRACT: background_label: The first morphological learner based upon the theory of Whole Word Morphology Ford et al.
background_label: (1997) is outlined, and preliminary evaluation results are presented.
method_label: The program, Whole Word Morphologizer, takes a POS-tagged lexicon as input, induces morphological relationships without attempting to discover or identify morphemes, and is then able to generate new words beyond the learning sample.
result_label: The accuracy (precision) of the generated new words is as high as 80% using the pure Whole Word theory, and 92% after a post-hoc adjustment is added to the routine.
===================================
paper_id: 934992; YEAR: 2002
adju relevance: Identical (+3)
difference: 0; annotator4: 3; annotator3: 3
sources: abs_tfidf - specter
TITLE: Whole word morphologizer: expanding the word-based lexicon: a nonstochastic computational approach.
ABSTRACT: background_label: Whole Word Morphologizer is a small computer implementation of word-based morphology.
background_label: The program automatically identifies morphological relations in a small word-based lexicon, literally learning its morphology, and uses the knowledge it acquires to generate new words.
method_label: It is based on a model of the mental lexicon in which all entries are whole, entire, fully fledged words and relies solely on basic cognitive principles (differentiation and generalization) for the automatic acquisition of morphological relations and the population of the lexicon.

===================================
paper_id: 1709713; YEAR: 2002
adju relevance: Similar (+2)
difference: 0; annotator4: 2; annotator3: 2
sources: title_tfidfcbow200 - specter
TITLE: Unsupervised discovery of morphologically related words based on orthographic and semantic similarity
ABSTRACT: background_label: We present an algorithm that takes an unannotated corpus as its input, and returns a ranked list of probable morphologically related pairs as its output.
method_label: The algorithm tries to discover morphologically related pairs by looking for pairs that are both orthographically and semantically similar, where orthographic similarity is measured in terms of minimum edit distance, and semantic similarity is measured in terms of mutual information.
method_label: The procedure does not rely on a morpheme concatenation model, nor on distributional properties of word substrings (such as affix frequency).
result_label: Experiments with German and English input give encouraging results, both in terms of precision (proportion of good pairs found at various cutoff points of the ranked list), and in terms of a qualitative analysis of the types of morphological patterns discovered by the algorithm.

===================================
paper_id: 14219425; YEAR: 2014
adju relevance: Similar (+2)
difference: 1; annotator4: 2; annotator3: 1
sources: abs_tfidf
TITLE: Co-learning of Word Representations and Morpheme Representations
ABSTRACT: background_label: AbstractThe techniques of using neural networks to learn distributed word representations (i.e., word embeddings) have been used to solve a variety of natural language processing tasks.
background_label: The recently proposed methods, such as CBOW and Skip-gram, have demonstrated their effectiveness in learning word embeddings based on context information such that the obtained word embeddings can capture both semantic and syntactic relationships between words.
background_label: However, it is quite challenging to produce high-quality word representations for rare or unknown words due to their insufficient context information.
objective_label: In this paper, we propose to leverage morphological knowledge to address this problem.
method_label: Particularly, we introduce the morphological knowledge as both additional input representation and auxiliary supervision to the neural network framework.
method_label: As a result, beyond word representations, the proposed neural network model will produce morpheme representations, which can be further employed to infer the representations of rare or unknown words based on their morphological structure.
result_label: Experiments on an analogical reasoning task and several word similarity tasks have demonstrated the effectiveness of our method in producing high-quality words embeddings compared with the state-of-the-art methods.

===================================
paper_id: 1732151; YEAR: 2001
adju relevance: Similar (+2)
difference: 1; annotator4: 2; annotator3: 1
sources: specter
TITLE: A Straightforward Approach to Morphological Analysis and Synthesis
ABSTRACT: background_label: In this paper we present a lexicon-based approach to the problem of morphological processing.
background_label: Full-form words, lemmas and grammatical tags are interconnected in a DAWG.
method_label: Thus, the process of analysis/synthesis is reduced to a search in the graph, which is very fast and can be performed even if several pieces of information are missing from the input.
method_label: The contents of the DAWG are updated using an on-line incremental process.
method_label: The proposed approach is language independent and it does not utilize any morphophonetic rules or any other special linguistic information.

===================================
paper_id: 5990753; YEAR: 2009
adju relevance: Similar (+2)
difference: 0; annotator4: 2; annotator3: 2
sources: abs_tfidf - specter
TITLE: Acquisition of morphological families and derivational series from a machine readable dictionary
ABSTRACT: objective_label: The paper presents a linguistic and computational model aiming at making the morphological structure of the lexicon emerge from the formal and semantic regularities of the words it contains.
background_label: The model is word-based.
method_label: The proposed morphological structure consists of (1) binary relations that connect each headword with words that are morphologically related, and especially with the members of its morphological family and its derivational series, and of (2) the analogies that hold between the words.
result_label: The model has been tested on the lexicon of French using the TLFi machine readable dictionary.

===================================
paper_id: 5191821; YEAR: 2015
adju relevance: Similar (+2)
difference: 0; annotator4: 2; annotator3: 2
sources: specter - abs_tfidf
TITLE: An Unsupervised Method for Uncovering Morphological Chains
ABSTRACT: background_label: Most state-of-the-art systems today produce morphological analysis based only on orthographic patterns.
objective_label: In contrast, we propose a model for unsupervised morphological analysis that integrates orthographic and semantic views of words.
method_label: We model word formation in terms of morphological chains, from base words to the observed words, breaking the chains into parent-child relations.
method_label: We use log-linear models with morpheme and word-level features to predict possible parents, including their modifications, for each word.
method_label: The limited set of candidate parents for each word render contrastive estimation feasible.
result_label: Our model consistently matches or outperforms five state-of-the-art systems on Arabic, English and Turkish.

===================================
paper_id: 7180044; YEAR: 2014
adju relevance: Similar (+2)
difference: 0; annotator4: 2; annotator3: 2
sources: title_tfidf - specter
TITLE: Unsupervised Morphology-Based Vocabulary Expansion
ABSTRACT: background_label: We present a novel way of generating unseen words, which is useful for certain applications such as automatic speech recognition or optical character recognition in low-resource languages.
method_label: We test our vocabulary generator on seven low-resource languages by measuring the decrease in out-of-vocabulary word rate on a held-out test set.
result_label: The languages we study have very different morphological properties; we show how our results differ depending on the morphological complexity of the language.
result_label: In our best result (on Assamese), our approach can predict 29% of the token-based out-of-vocabulary with a small amount of unlabeled training data.

===================================
paper_id: 174802490; YEAR: 2019
adju relevance: Similar (+2)
difference: 0; annotator4: 2; annotator3: 2
sources: abs_tfidfcbow200
TITLE: Derivational Morphological Relations in Word Embeddings
ABSTRACT: background_label: Derivation is a type of a word-formation process which creates new words from existing ones by adding, changing or deleting affixes.
objective_label: In this paper, we explore the potential of word embeddings to identify properties of word derivations in the morphologically rich Czech language.
method_label: We extract derivational relations between pairs of words from DeriNet, a Czech lexical network, which organizes almost one million Czech lemmata into derivational trees.
method_label: For each such pair, we compute the difference of the embeddings of the two words, and perform unsupervised clustering of the resulting vectors.
result_label: Our results show that these clusters largely match manually annotated semantic categories of the derivational relations (e.g.
result_label: the relation 'bake--baker' belongs to category 'actor', and a correct clustering puts it into the same cluster as 'govern--governor').

===================================
paper_id: 14926846; YEAR: 2013
adju relevance: Similar (+2)
difference: 0; annotator4: 2; annotator3: 2
sources: specter - title_tfidfcbow200
TITLE: A Simple Approach to Unknown Word Processing in Japanese Morphological Analysis
ABSTRACT: background_label: AbstractThis paper presents a simple but effective approach to unknown word processing in Japanese morphological analysis, which handles 1) unknown words that are derived from words in a pre-defined lexicon and 2) unknown onomatopoeias.
method_label: Our approach leverages derivation rules and onomatopoeia patterns, and correctly recognizes certain types of unknown words.
result_label: Experiments revealed that our approach recognized about 4,500 unknown words in 100,000 Web sentences with only 80 harmful side effects and a 6% loss in speed.

===================================
paper_id: 17758148; YEAR: 2006
adju relevance: Similar (+2)
difference: 2; annotator4: 2; annotator3: 0
sources: title_tfidf - abs_tfidf - specter
TITLE: Word-based morphology
ABSTRACT: background_label: This paper examines two contrasting perspectives on morphological analysis, and considers inflectional patterns that bear on the choice between these alternatives.
background_label: On what is termed an ABSTRACTIVE perspective, surface word forms are regarded as basic morphotactic units of a grammatical system, with roots, stems and exponents treated as abstractions over a lexicon of word forms.
background_label: This traditional standpoint is contrasted with the more CONSTRUCTIVE perspective of post-Bloomfieldian models, in which surface word forms are ‘built’ from sub-word units.
background_label: Part of the interest of this contrast is that it cuts across conventional divisions of morphological models.
background_label: Thus, realization-based models are morphosyntactically ‘word-based’ in the sense that they regard words as the minimal meaningful units of a grammatical system.
method_label: Yet morphotactically, these models tend to adopt a constructive ‘root-based’ or ‘stem-based’ perspective.
result_label: An examination of some form-class patterns in Saami, Estonian and Georgian highlights advantages of an abstractive model, and suggests that these advantages derive from the fact that sets of words often predict other word forms and determine a morphotactic analysis of their parts, whereas sets of sub-word units are of limited predictive value and typically do not provide enough information to recover word forms.

===================================
paper_id: 6023976; YEAR: 2013
adju relevance: Similar (+2)
difference: 2; annotator4: 2; annotator3: 0
sources: specter - title_cbow200
TITLE: Supervised Learning of Complete Morphological Paradigms
ABSTRACT: background_label: AbstractWe describe a supervised approach to predicting the set of all inflected forms of a lexical item.
method_label: Our system automatically acquires the orthographic transformation rules of morphological paradigms from labeled examples, and then learns the contexts in which those transformations apply using a discriminative sequence model.
method_label: Because our approach is completely data-driven and the model is trained on examples extracted from Wiktionary, our method can extend to new languages without change.
method_label: Our end-to-end system is able to predict complete paradigms with 86.1% accuracy and individual inflected forms with 94.9% accuracy, averaged across three languages and two parts of speech.

===================================
paper_id: 17410499; YEAR: 2017
adju relevance: Similar (+2)
difference: 2; annotator4: 2; annotator3: 0
sources: specter
TITLE: Producing Unseen Morphological Variants in Statistical Machine Translation
ABSTRACT: background_label: AbstractTranslating into morphologically rich languages is difficult.
background_label: Although the coverage of lemmas may be reasonable, many morphological variants cannot be learned from the training data.
objective_label: We present a statistical translation system that is able to produce these inflected word forms.
method_label: Different from most previous work, we do not separate morphological prediction from lexical choice into two consecutive steps.
method_label: Our approach is novel in that it is integrated in decoding and takes advantage of context information from both the source language and the target language sides.

===================================
paper_id: 6555669; YEAR: 2008
adju relevance: Similar (+2)
difference: 1; annotator4: 2; annotator3: 1
sources: specter - abs_tfidf - title_tfidfcbow200
TITLE: Acquistion of the Morphological Structure of the Lexicon Based on Lexical Similarity and Formal Analogy
ABSTRACT: objective_label: The paper presents a computational model aiming at making the morphological structure of the lexicon emerge from the formal and semantic regularities of the words it contains.
background_label: The model is purely lexeme-based.
method_label: The proposed morphological structure consists of (1) binary relations that connect each headword with words that are morphologically related, and especially with the members of its morphological family and its derivational series, and of (2) the analogies that hold between the words.
result_label: The model has been tested on the lexicon of French using the TLFi machine readable dictionary.

===================================
paper_id: 2731384; YEAR: 1996
adju relevance: Similar (+2)
difference: 1; annotator4: 2; annotator3: 1
sources: title_tfidfcbow200 - title_cbow200
TITLE: Combining Hand-crafted Rules and Unsupervised Learning in Constraint-based Morphological Disambiguation
ABSTRACT: background_label: This paper presents a constraint-based morphological disambiguation approach that is applicable languages with complex morphology--specifically agglutinative languages with productive inflectional and derivational morphological phenomena.
background_label: In certain respects, our approach has been motivated by Brill's recent work, but with the observation that his transformational approach is not directly applicable to languages like Turkish.
method_label: Our system combines corpus independent hand-crafted constraint rules, constraint rules that are learned via unsupervised learning from a training corpus, and additional statistical information from the corpus to be morphologically disambiguated.
method_label: The hand-crafted rules are linguistically motivated and tuned to improve precision without sacrificing recall.
method_label: The unsupervised learning process produces two sets of rules: (i) choose rules which choose morphological parses of a lexical item satisfying constraint effectively discarding other parses, and (ii) delete rules, which delete parses satisfying a constraint.
method_label: Our approach also uses a novel approach to unknown word processing by employing a secondary morphological processor which recovers any relevant inflectional and derivational information from a lexical item whose root is unknown.
result_label: With this approach, well below 1 percent of the tokens remains as unknown in the texts we have experimented with.
result_label: Our results indicate that by combining these hand-crafted,statistical and learned information sources, we can attain a recall of 96 to 97 percent with a corresponding precision of 93 to 94 percent, and ambiguity of 1.02 to 1.03 parses per token.

===================================
paper_id: 59800062; YEAR: 2011
adju relevance: Similar (+2)
difference: 1; annotator4: 2; annotator3: 1
sources: specter
TITLE: Developing Oriya Morphological Analyzer Using Lt-Toolbox
ABSTRACT: objective_label: In this paper we present the work done on developing a Morphological Analyzer (MA) for Oriya language, following the paradigm approach.
background_label: A paradigm defines all the word forms of a given stem, and also provides a feature structure associated with every word.
method_label: It consists of various paradigms under which nouns, adjectives, indeclinables (avyaya) and finite verbs of Oriya are classified.
method_label: Further, we discuss the construction of paradigms and the thought process that goes into their construction.
method_label: The paradigms have been created using an XML based morphological dictionary from the Lt-toolbox package.

===================================
paper_id: 9442505; YEAR: 2008
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidf - abs_tfidfcbow200
TITLE: Distributional Cues to Word Boundaries: Context is Important
ABSTRACT: background_label: Word segmentation, or identifying word boundaries in continuous speech, is one of the first problems that infants must solve as they are acquiring language.
background_label: A number of different weak cues to word boundaries are present in fluent speech, and there is evidence that infants are able to use many of these, including phonotactics (Mattys et al., 1999), allophonic variation (Jusczyk et al., 1999a), metrical (stress) patterns (Morgan et al., 1995; Jusczyk et al., 1999b), effects of coarticulation (Johnson and Jusczyk, 2001), and statistical regularities amongst sequences of syllables (Saffran et al., 1996a).
background_label: The kinds of statistical regularities studied by Saffran et al.
background_label: (1996a) allow for the possibility of language-independent word segmentation strategies, and seem to be used by infants earlier than other kinds of cues (Thiessen and Saffran, 2003).
background_label: These facts have led to the proposal that strategies exploiting the statistical patterns found in sound sequences are a crucial first step in bootstrapping word segmentation (Thiessen and Saffran, 2003), and have provoked a great deal of research into statistical word segmentation using both human subjects and computational models.
result_label: Most previous work on statistical word segmentation is based on the observation that transitions from one syllable or phoneme to the next tend to be less predictable at word boundaries than within words (Harris, 1955; Saffran et al., 1996a).
background_label: This observation has led to proposals that infants use statistics such as transitional probabilities or mutual information in order to segment words from speech.
background_label: A number of models have been developed in an attempt to explain how these kinds of statistics can be used procedurally to identify words or word boundaries.
objective_label: Here, we take a different approach: we seek to identify the assumptions the learner must make about the nature of language in order to correctly segment natural language input.
method_label: Observations about predictability at word boundaries are consistent with two different kinds of assumptions about what constitutes a word: either a word is a unit that is statistically independent of other units, or it is a unit that helps to predict other units (but to a lesser degree than the beginning of a word predicts its end).
method_label: In most artificial language experiments on word segmentation, the first assumption is adopted implicitly by creating stimuli through random concatenation of nonce words.
method_label: In this paper, we use simulations to examine learning from natural, rather than artificial, language input.
result_label: We ask what kinds of words are identified by a learner who assumes that words are statistically independent, or

===================================
paper_id: 6559983; YEAR: 2015
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: specter
TITLE: A Language-Independent Feature Schema for Inflectional Morphology
ABSTRACT: background_label: This paper presents a universal morphological feature schema that represents the finest distinctions in meaning that are expressed by overt, affixal inflectional morphology across languages.
method_label: This schema is used to universalize data extracted from Wiktionary via a robust multidimensional table parsing algorithm and feature mapping algorithms, yielding 883,965 instantiated paradigms in 352 languages.
result_label: These data are shown to be effective for training morphological analyzers, yielding significant accuracy gains when applied to Durrett and DeNero’s (2013) paradigm learning framework.

===================================
paper_id: 11851688; YEAR: 2013
adju relevance: Related (+1)
difference: 1; annotator4: 0; annotator3: 1
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: A hierarchical system for word discovery exploiting DTW-based initialization
ABSTRACT: background_label: Discovering the linguistic structure of a language solely from spoken input asks for two steps: phonetic and lexical discovery.
objective_label: The first is concerned with identifying the categorical subword unit inventory and relating it to the underlying acoustics, while the second aims at discovering words as repeated patterns of subword units.
method_label: The hierarchical approach presented here accounts for classification errors in the first stage by modelling the pronunciation of a word in terms of subword units probabilistically: a hidden Markov model with discrete emission probabilities, emitting the observed subword unit sequences.
method_label: We describe how the system can be learned in a completely unsupervised fashion from spoken input.
method_label: To improve the initialization of the training of the word pronunciations, the output of a dynamic time warping based acoustic pattern discovery system is used, as it is able to discover similar temporal sequences in the input data.
result_label: This improved initialization, using only weak supervision, has led to a 40% reduction in word error rate on a digit recognition task.

===================================
paper_id: 2214232; YEAR: 2004
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: specter
TITLE: Morphosemantic Relations in and across Wordnets: A Study Based on Turkish
ABSTRACT: background_label: Morphological processes in a language can be effectively used to enrich individual wordnets with semantic relations.
background_label: More importantly, morphological pro- cesses in a language can be used to discover less explicit semantic relations in other languages.
method_label: This will both improve the internal connectivity of individual wordnets and also the overlap across different wordnets.
result_label: Using morphology to improve the quality of wordnets and to automatically prepare synset glosses are two other possible appli- cations.

===================================
paper_id: 11791157; YEAR: 1993
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidf - specter
TITLE: Viewing Morphology as an Inference Process
ABSTRACT: background_label: Morphology is the area of linguistics concerned with the internal structure of words.
background_label: Information Retrieval has generally not paid much attention to word structure, other than to account for some of the variability in word forms via the use of stemmers.
objective_label: This paper will describe our experiments to determine the importance of morphology, and the effect that it has on performance.
method_label: We will also describe the role of morphological analysis in word sense disambiguation, and in identifying lexical semantic relationships in a machine-readable dictionary.
method_label: We will first provide a brief overview of morphological phenomena, and then describe the experiments themselves.

===================================
paper_id: 5728098; YEAR: 2012
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: specter
TITLE: Feature-Rich Part-of-speech Tagging for Morphologically Complex Languages: Application to Bulgarian
ABSTRACT: background_label: AbstractWe present experiments with part-ofspeech tagging for Bulgarian, a Slavic language with rich inflectional and derivational morphology.
background_label: Unlike most previous work, which has used a small number of grammatical categories, we work with 680 morpho-syntactic tags.
result_label: We combine a large morphological lexicon with prior linguistic knowledge and guided learning from a POS-annotated corpus, achieving accuracy of 97.98%, which is a significant improvement over the state-of-the-art for Bulgarian.

===================================
paper_id: 6658384; YEAR: 1992
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: cited - title_tfidf - title_tfidfcbow200 - title_cbow200 - specter
TITLE: Two-Level Morphology With Composition
ABSTRACT: background_label: Two-Level Morphology with Composition Lauri Karttunen, Ronald M. Kaplan, and Annie Zaenen Xerox Palo Alto Research Center Center for the Study of language and Information StanJbrd University 1.
background_label: Limitations of "Kimmo" systems The advent of two-level morphology (Koskenniemi [1], Karttunen [2], Antworth [3], Ritchie et al.
background_label: [4]) has made it relatively easy to develop adequate morphological (or at least morphographical) descriptions for natural languages, clearly superior to earlier "cut-and-paste" approaches to mor- phology.
method_label: Most of the existing "Kimmo" systems developed within this paradigm consist of • linked lexicons stored as annotated letter trees • morphological information on the leaf nodes of trees • transducers that encode morphological alternations An analysis of an inflected word form is produced by mapping the input form to a sequence of lexical forms through the transducers and by composing some out- put from the annotations on the leaf nodes of the lexical paths that were traversed.
background_label: Comprehensive morphological descrip- tions of this type have been developed for several languages including Finnish, Swedish, Russian, English, Swahili, and Arabic.
background_label: Although they have several good features, these Kimmo-systems also have some limitations.
background_label: The ones we want to ad- dress in this paper are the following: (1) Lexical representations tend to be arbitrary.
background_label: Because it is difficult to write and test two-level systems that map between pairs of radically dissimilar forms, lexical representations in existing two-level analyzers tend to stay close to the surface forms.
background_label: This is not a problem for morpho- logically simple languages like English because, for most words, inflected forms are very similar to the canonical dictionary entry.
result_label: Except for a small number of irregular verbs and nouns, it is not difficult to create a two-level description for English in which lexical forms coincide with the canonical citation forms found in a dictionary.
background_label: However, current analyzers for mor- phologically more complex languages (Finnish and Russian, for example) are not as satisfying in this respect.
background_label: In these systems, lexical forms typically contain diacritic markers and special symbols; they are not real words in the language.
background_label: For example, in Finnish the lexical counterpart of otin 'I took' might be rendered as otTallln, where T, al, and I1 are an arbitrary encoding of morpho- logical alternations that determine the allomorphs of the stem and the past tense morpheme.
background_label: The canonical citation form ottaa 'to take' is composed from annotations on the leaf nodes of the letter trees that are linked to match the input.
background_label: It is not in any direct way related to the lexical form produced by the transducers.
background_label: (2) Morphological categories are not directly encoded as part of the lexical form.
result_label: Instead of morphemes like Plural or Past, we typically see suffix strings like +s, and +ed, which do not by themselves indi- cate what morpheme they express.
result_label: Different realizations of the same morpho- logical category are often represented as different even on the lexical side.
other_label: These characteristics lead to some un- desirable consequences: ACRES DE COLING-92, NANTES, 23-28 AO~' 1992 1 4 1 PROC.
other_label: OF COLING-92, NA~rr~s, AU6.23-28, 1992

===================================
paper_id: 2575762; YEAR: 2008
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: title_tfidf - specter
TITLE: Learning Morphology with Morfette
ABSTRACT: background_label: AbstractMorfette is a modular, data-driven, probabilistic system which learns to perform joint morphological tagging and lemmatization from morphologically annotated corpora.
method_label: The system is composed of two learning modules which are trained to predict morphological tags and lemmas using the Maximum Entropy classifier.
method_label: The third module dynamically combines the predictions of the Maximum-Entropy models and outputs a probability distribution over tag-lemma pair sequences.
method_label: The lemmatization module exploits the idea of recasting lemmatization as a classification task by using class labels which encode mappings from wordforms to lemmas.
result_label: Experimental evaluation results and error analysis on three morphologically rich languages show that the system achieves high accuracy with no language-specific feature engineering or additional resources.

===================================
paper_id: 14371033; YEAR: 2005
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: title_tfidfcbow200 - specter
TITLE: Morfessor and hutmegs: Unsupervised morpheme segmentation for highlyinflecting and compounding languages
ABSTRACT: background_label: In this work, we announce the Morfessor 1.0 software package, which is a program that takes as input a corpus of raw text and produces a segmentation of the word forms observed in the text.
background_label: The segmentation obtained often resembles a linguistic morpheme segmentation.
method_label: In addition, we briefly describe the Hutmegs package, also publicly available for research purposes.
method_label: Hutmegs contains semi-automatically produced correct, or gold-standard, morpheme segmentations for a large number of Finnish and English word forms.
method_label: One easy way for the reader to familiarize himself with our work is to test the demonstration program on our Internet site.
result_label: The demo shows how Morfessor segments words that the user types in.

===================================
paper_id: 15628229; YEAR: 2004
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidfcbow200
TITLE: A Rule based Approach to Word Lemmatization
ABSTRACT: background_label: Lemmatization is the process of finding the normalized form of a word.
background_label: It is the same as looking for a transformation to apply on a word to get its normalized form.
objective_label: The approach presented in this paper focuses on word endings: what word suffix should be removed and/or added to get the normalized form.
objective_label: This paper compares the results of two word lemmatization algorithms, one based on if-then rules and the other based on ripple down rules induction algorithms.
objective_label: It presents the problem of lemmatization of words from Slovene free text and explains why the Ripple Down Rules (RDR) approach is very well suited for the task.
result_label: When learning from a corpus of lemmatized Slovene words the RDR approach results in easy to understand rules of improved classification accuracy compared to the results of rule learning achieved in previous work.

===================================
paper_id: 12219473; YEAR: 2013
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: A Hybrid Morphological Disambiguation System for Turkish
ABSTRACT: objective_label: AbstractIn this paper, we propose a morphological disambiguation method for Turkish, which is an agglutinative language.
method_label: We use a hybrid method, which combines statistical information with handcrafted rules and learned rules.
method_label: Five different steps are applied for disambiguation.
method_label: In the first step, the most likely tags of words are selected.
method_label: In the second step, we use handcrafted rules to constrain possible parses or select the correct parse.
method_label: Next, the most likely tags are selected for still ambiguous words according to the suffixes of the words that are unseen in the training corpus.
background_label: Then, we use transformation-based rules that are learned by a variation of Brill tagger.
method_label: If the word is still ambiguous, we use some heuristics for the disambiguation.
method_label: We constructed a hand-tagged dataset for training and applied a ten-fold cross validation with this dataset.
method_label: We obtained 93.4% accuracy on the average when whole morphological parses are considered in calculation.
result_label: The accuracy increased to 94.1% when only part-of-speech tags and inflections of last derivations are considered.
result_label: Our accuracy is 96.9% in terms of part-of-speech tagging.

===================================
paper_id: 16049704; YEAR: 2011
adju relevance: Related (+1)
difference: 1; annotator4: 0; annotator3: 1
sources: title_tfidfcbow200
TITLE: Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models
ABSTRACT: background_label: AbstractThis paper describes an unsupervised dynamic graphical model for morphological segmentation and bilingual morpheme alignment for statistical machine translation.
background_label: The model extends Hidden Semi-Markov chain models by using factored output nodes and special structures for its conditional probability distributions.
method_label: It relies on morpho-syntactic and lexical source-side information (part-of-speech, morphological segmentation) while learning a morpheme segmentation over the target language.
result_label: Our model outperforms a competitive word alignment system in alignment quality.
result_label: Used in a monolingual morphological segmentation setting it substantially improves accuracy over previous state-of-the-art models on three Arabic and Hebrew datasets.

===================================
paper_id: 15179271; YEAR: 2005
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: specter - title_tfidf
TITLE: Morphology Induction from Term Clusters
ABSTRACT: background_label: We address the problem of learning a morphological automaton directly from a monolingual text corpus without recourse to additional resources.
background_label: Like previous work in this area, our approach exploits orthographic regularities in a search for possible morphological segmentation points.
method_label: Instead of affixes, however, we search for affix transformation rules that express correspondences between term clusters induced from the data.
method_label: This focuses the system on substrings having syntactic function, and yields cluster-to-cluster transformation rules which enable the system to process unknown morphological forms of known words accurately.
method_label: A stem-weighting algorithm based on Hubs and Authorities is used to clarify ambiguous segmentation points.
result_label: We evaluate our approach using the CELEX database.

===================================
paper_id: 11225110; YEAR: 2006
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: specter
TITLE: Unsupervised morphological parsing of Bengali. Language Resources and Evaluation
ABSTRACT: background_label: Unsupervised morphological analysis is the task of segmenting words into prefixes, suffixes and stems without prior knowledge of language-specific morphotactics and morpho-phonological rules.
objective_label: This paper introduces a simple, yet highly effective algorithm for unsupervised morphological learning for Bengali, an Indo–Aryan language that is highly inflectional in nature.
result_label: When evaluated on a set of 4,110 human-segmented Bengali words, our algorithm achieves an F-score of 83%, substantially outperforming Linguistica, one of the most widely-used unsupervised morphological parsers, by about 23%.

===================================
paper_id: 146253352; YEAR: 1958
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: title_tfidf
TITLE: The Child's Learning of English Morphology
ABSTRACT: background_label: In this study we set out to discover what is learned by children exposed to English morphology.
background_label: To test for knowledge of morphological rules, we use nonsense materials.
background_label: We know that if the subject can supply the correct plural ending, for instance, to a noun we have made up, he has internalized a working system of the plural allomorphs in English, and is able to generalize to new cases and select the right form.
background_label: If a child knows that the plural of witch is witches, he may simply have memorized the plural form.
result_label: If, however, he tells us that the plural of * gutch is * gutches, we have evidence that he actually knows, albeit unconsciously, one of those rules which the descriptive linguist, too, would set forth in his grammar.
result_label: And if children do have knowledge of morphological rules, how does this knowledge evolve?
background_label: Is there a progression from simple, regular rules to the more irregular and qualified rules that are adequate fully to describe English?
background_label: In very general terms, we undertake to discover the psychological status of a certain kind of linguistic description.
background_label: It is evident that the acquisition of language is more than the storing up of rehearsed utterances, since we are all able to say what we have not practiced and what we have never before heard.
objective_label: In bringing descriptive linguistics to the study of language acquisition, we hope to gain knowledge of the systems and patterns used by the speaker.
method_label: In order to test for children's knowledge of this sort, it was necessary to begin with an examination of their actual vocabulary.
method_label: Accordingly, the 1000 most frequent words in the first-grader's vocabulary were selected from Rinsland's listing.
other_label: This listing

===================================
paper_id: 296857; YEAR: 2012
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: title_tfidf - title_tfidfcbow200
TITLE: Unsupervised Morphology Rivals Supervised Morphology for Arabic MT
ABSTRACT: background_label: AbstractIf unsupervised morphological analyzers could approach the effectiveness of supervised ones, they would be a very attractive choice for improving MT performance on low-resource inflected languages.
objective_label: In this paper, we compare performance gains for state-of-the-art supervised vs. unsupervised morphological analyzers, using a state-of-theart Arabic-to-English MT system.
method_label: We apply maximum marginal decoding to the unsupervised analyzer, and show that this yields the best published segmentation accuracy for Arabic, while also making segmentation output more stable.
method_label: Our approach gives an 18% relative BLEU gain for Levantine dialectal Arabic.
result_label: Furthermore, it gives higher gains for Modern Standard Arabic (MSA), as measured on NIST MT-08, than does MADA (Habash and Rambow, 2005) , a leading supervised MSA segmenter.

===================================
paper_id: 2138205; YEAR: 2010
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: specter
TITLE: Morpho Challenge competition 2005-2010 : Evaluations and results
ABSTRACT: background_label: Morpho Challenge is an annual evaluation campaign for unsupervised morpheme analysis.
background_label: In morpheme analysis, words are segmented into smaller meaningful units.
background_label: This is an essential part in processing complex word forms in many large-scale natural language processing applications, such as speech recognition, information retrieval, and machine translation.
background_label: The discovery of morphemes is particularly important for morphologically rich languages where inflection, derivation and composition can produce a huge amount of different word forms.
objective_label: Morpho Challenge aims at language-independent unsupervised learning algorithms that can discover useful morpheme-like units from raw text material.
result_label: In this paper we define the challenge, review proposed algorithms, evaluations and results so far, and point out the questions that are still open.

===================================
paper_id: 5219497; YEAR: 1993
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: specter
TITLE: Morphonology in the Lexicon
ABSTRACT: background_label: In this paper we present a means of defining morphonological phenomena in an inheritance based lexicon.
method_label: We make use of the theory behind the formal language MOLUSC, in which morphological alternations were defined as mappings between sequences of tree-structured syllables.
method_label: We discuss how the alternations can be defined in the inheritance-based lexical representation language DATR, and how the phonological aspects can be built upon to bring it closer to an integrated lexicon with representations which can be used by both the morphology and phonology of a language.

===================================
paper_id: 10209751; YEAR: 2017
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: title_tfidf - title_tfidfcbow200 - specter
TITLE: From Segmentation to Analyses: a Probabilistic Model for Unsupervised Morphology Induction
ABSTRACT: background_label: AbstractA major motivation for unsupervised morphological analysis is to reduce the sparse data problem in under-resourced languages.
background_label: Most previous work focuses on segmenting surface forms into their constituent morphs (e.g., taking: tak +ing), but surface form segmentation does not solve the sparse data problem as the analyses of take and taking are not connected to each other.
method_label: We extend the MorphoChains system (Narasimhan et al., 2015) to provide morphological analyses that can abstract over spelling differences in functionally similar morphs.
method_label: These analyses are not required to use all the orthographic material of a word (stopping: stop +ing), nor are they limited to only that material (acidified: acid +ify +ed).
result_label: On average across six typologically varied languages our system has a similar or better F-score on EMMA (a measure of underlying morpheme accuracy) than three strong baselines; moreover, the total number of distinct morphemes identified by our system is on average 12.8% lower than for Morfessor (Virpioja et al., 2013) , a stateof-the-art surface segmentation system.

===================================
paper_id: 9344827; YEAR: 2006
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidfcbow200
TITLE: A Pragmatic Chinese Word Segmentation Approach Based on Mixing Models
ABSTRACT: background_label: A pragmatic Chinese word segmentation approach is presented in this paper based on mixing language models.
background_label: Chinese word segmentation is composed of several hard sub-tasks, which usually encounter different difficulties.
method_label: The authors apply the corresponding language model to solve each special sub-task, so as to take advantage of each model.
method_label: First, a class-based trigram is adopted in basic word segmentation, which applies the Absolute Discount Smoothing algorithm to overcome data sparseness.
method_label: The Maximum Entropy Model (ME) is also used to identify Named Entities.
method_label: Second, the authors propose the application of rough sets and average mutual information, etc.
method_label: to extract special features.
method_label: Finally, some features are extended through the combination of the word cluster and the thesaurus.
result_label: The authors' system participated in the Second International Chinese Word Segmentation Bakeoff, and achieved 96.7 and 97.2 in F-measure in the PKU and MSRA open tests, respectively.

===================================
paper_id: 2362250; YEAR: 2003
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: title_tfidf - title_tfidfcbow200 - title_cbow200 - specter
TITLE: Unsupervised Learning of Morphology for English and Inuktitut
ABSTRACT: background_label: We describe a simple unsupervised technique for learning morphology by identifying hubs in an automaton.
background_label: For our purposes, a hub is a node in a graph with in-degree greater than one and out-degree greater than one.
method_label: We create a word-trie, transform it into a minimal DFA, then identify hubs.
method_label: Those hubs mark the boundary between root and suffix, achieving similar performance to more complex mixtures of techniques.

===================================
paper_id: 14923637; YEAR: 2008
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: abs_cbow200 - title_tfidf - title_tfidfcbow200 - specter - abs_tfidfcbow200
TITLE: Online Acquisition of Japanese Unknown Morphemes using Morphological Constraints
ABSTRACT: objective_label: We propose a novel lexicon acquirer that works in concert with the morphological analyzer and has the ability to run in online mode.
method_label: Every time a sentence is analyzed, it detects unknown morphemes, enumerates candidates and selects the best candidates by comparing multiple examples kept in the storage.
method_label: When a morpheme is unambiguously selected, the lexicon acquirer updates the dictionary of the analyzer, and it will be used in subsequent analysis.
method_label: We use the constraints of Japanese morphology and effectively reduce the number of examples required to acquire a morpheme.
result_label: Experiments show that unknown morphemes were acquired with high accuracy and improved the quality of morphological analysis.

===================================
paper_id: 144481010; YEAR: 1955
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: cited - title_tfidf - title_tfidfcbow200 - title_cbow200 - specter
TITLE: From Phoneme to Morpheme
ABSTRACT: background_label: The following investigation1 presents a constructional procedure segmenting an utterance in a way which correlates well with word and morpheme boundaries.
method_label: The procedure requires a large set of utterances, elicited in a certain manner from an informant (or found in a very large corpus); and it requires that all the utterances be written in the same phonemic representation, determined without reference to morphemes.
method_label: It then investigates a particular distributional relation among the phonemes in the utterances thus collected; and on the basis of this relation among the phonemes, it indicates particular points of segmentation within one utterance at a time.
other_label: For example, in the utterance /hiyzkwikǝr/ He’s quicker it will indicate segmentation at the points marked by dots: /hiy.
other_label: z. kwik.
result_label: Ər/; and it will do so purely by comparing this phonemic sequence with the phonemic sequences of other utterances.

===================================
paper_id: 879218; YEAR: 2010
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: title_cbow200 - title_tfidf - title_tfidfcbow200 - specter
TITLE: Semi-Supervised Learning of Concatenative Morphology
ABSTRACT: background_label: AbstractWe consider morphology learning in a semi-supervised setting, where a small set of linguistic gold standard analyses is available.
method_label: We extend Morfessor Baseline, which is a method for unsupervised morphological segmentation, to this task.
method_label: We show that known linguistic segmentations can be exploited by adding them into the data likelihood function and optimizing separate weights for unlabeled and labeled data.
method_label: Experiments on English and Finnish are presented with varying amount of labeled data.
result_label: Results of the linguistic evaluation of Morpho Challenge improve rapidly already with small amounts of labeled data, surpassing the state-ofthe-art unsupervised methods at 1000 labeled words for English and at 100 labeled words for Finnish.

===================================
paper_id: 29938854; YEAR: 2004
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: specter
TITLE: Morphological Analysis For Statistical Machine Translation
ABSTRACT: objective_label: We present a novel morphological analysis technique which induces a morphological and syntactic symmetry between two languages with highly asymmetrical morphological structures to improve statistical machine translation qualities.
method_label: The technique pre-supposes fine-grained segmentation of a word in the morphologically rich language into the sequence of prefix(es)-stem-suffix(es) and part-of-speech tagging of the parallel corpus.
method_label: The algorithm identifies morphemes to be merged or deleted in the morphologically rich language to induce the desired morphological and syntactic symmetry.
result_label: The technique improves Arabic-to-English translation qualities significantly when applied to IBM Model 1 and Phrase Translation Models trained on the training corpus size ranging from 3,500 to 3.3 million sentence pairs.

===================================
paper_id: 6395143; YEAR: 2006
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidfcbow200 - abs_tfidf - abs_cbow200 - specter
TITLE: Learning Morphological Disambiguation Rules for Turkish
ABSTRACT: background_label: In this paper, we present a rule based model for morphological disambiguation of Turkish.
background_label: The rules are generated by a novel decision list learning algorithm using supervised training.
background_label: Morphological ambiguity (e.g.
background_label: lives = live+s or life+s) is a challenging problem for agglutinative languages like Turkish where close to half of the words in running text are morphologically ambiguous.
background_label: Furthermore, it is possible for a word to take an unlimited number of suffixes, therefore the number of possible morphological tags is unlimited.
method_label: We attempted to cope with these problems by training a separate model for each of the 126 morphological features recognized by the morphological analyzer.
method_label: The resulting decision lists independently vote on each of the potential parses of a word and the final parse is selected based on our confidence on these votes.
result_label: The accuracy of our model (96%) is slightly above the best previously reported results which use statistical models.
result_label: For comparison, when we train a single decision list on full tags instead of using separate models on each feature we get 91% accuracy.

===================================
paper_id: 21668064; YEAR: 2018
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 2
sources: title_tfidfcbow200 - title_cbow200 - specter
TITLE: Unsupervised Disambiguation of Syncretism in Inflected Lexicons
ABSTRACT: background_label: Lexical ambiguity makes it difficult to compute various useful statistics of a corpus.
background_label: A given word form might represent any of several morphological feature bundles.
background_label: One can, however, use unsupervised learning (as in EM) to fit a model that probabilistically disambiguates word forms.
method_label: We present such an approach, which employs a neural network to smoothly model a prior distribution over feature bundles (even rare ones).
method_label: Although this basic model does not consider a token's context, that very property allows it to operate on a simple list of unigram type counts, partitioning each count among different analyses of that unigram.
result_label: We discuss evaluation metrics for this novel task and report results on 5 languages.

===================================
paper_id: 61278667; YEAR: 2010
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: specter
TITLE: ADX – Agent for Morphologic Analysis of Lexical Entries in a Dictionary
ABSTRACT: background_label: This paper refers to the morphological analysis of words, as an important process in the domain of natural language processing.
method_label: We will present the classical solution, based on the use of inflected paradigms and of an extended data base, containing all roots of the words, and then there are emphasized some of the disadvantages of this method.
method_label: Then we will present an original method, which dynamically generates the roots of words, using phonetic alternances in the context of flexionary rules.
method_label: There are also presented some optimisations of the morphologic analysis algorithm.

===================================
paper_id: 62258158; YEAR: 2002
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 2
sources: title_cbow200 - title_tfidfcbow200 - specter
TITLE: Modeling and Learning Multilingual Inflectional Morphology in a Minimally Supervised Framework
ABSTRACT: background_label: Computational morphology is an important component of most natural language processing tasks including machine translation, information retrieval, wordsense disambiguation, parsing, and text generation.
background_label: Morphological analysis, the process of finding a root form and part-of-speech of an inflected word form, and its inverse, morphological generation, can provide fine-grained part of speech information and help resolve necessary syntactic agreements.
background_label: In addition, morphological analysis can reduce the problem of data sparseness through dimensionality reduction.
method_label: This thesis presents a successful original paradigm for both morphological analysis and generation by treating both tasks in a competitive linkage model based on a combination of diverse inflection-root similarity measures.
method_label: Previous approaches to the machine learning of morphology have been essentially limited to string-based transduction models.
method_label: In contrast, the work presented here integrates both several new noise-robust, trie-based supervised methods for learning these transductions, and also a suite of unsupervised alignment models based on weighted Levenshtein distance, position-weighted contextual similarity, and several models of distributional similarity including expected relative frequency.
method_label: Via iterative bootstrapping the combination of these models yields a full lemmatization analysis competitive with fully supervised approaches but without any direct supervision.
method_label: In addition, this thesis also presents an original translingual projection model for morphology induction, where previously learned morphological analyses in a second language can be robustly projected via bilingual corpora to yield successful analyses in the new target language without any monolingual supervision.
result_label: Collectively these methods outperform previously published algorithms for

===================================
paper_id: 14534244; YEAR: 1998
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidfcbow200 - abs_tfidf - abs_cbow200
TITLE: Extending an Existing Specialized Semantic Lexicon
ABSTRACT: background_label: There is a constant need to extend and tune specialized vocabularies to account for new words and new word usages.
objective_label: This paper addresses the issue of characterizing the semantic class of such words.
objective_label: We test the hypothesis that the analysis of word distribution in a representative corpus, as obtained by robust NLP tools, can help identify words with similar meanings, and to decide on the most likely category for a given word based on the categories of its neighbors.
method_label: We report on an experiment with a moderatesize corpus of patient discharge summaries collected during the MENELAS project, taking as categories the high-level axes of the SNOMED nomenclature, and processing the corpus with the ZELLIG suite of tools.
method_label: We attempt to quantify the extent to which this process succeeds in proposing a correct category for a given word of the corpus while we vary several parameters of the method.
result_label: The percentage of correctly categorized words (precision) ranges between 50 and 75 %, while the best percentage of categorized words (recall) is 37 % for the whole categorization process.
result_label: Categorization results are significantly above chance, but not sufficient for a fully-automated process.
result_label: We discuss possible uses of such a categorization help and identify further directions for improvement.

===================================
paper_id: 9131152; YEAR: 2000
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: specter
TITLE: Language Independent Morphological Analysis
ABSTRACT: background_label: This paper proposes a framework of language independent morphological analysis and mainly concentrate on tokenization, the first process of morphological analysis.
background_label: Although tokenization is usually not regarded as a difficult task in most segmented languages such as English, there are a number of problems in achieving precise treatment of lexical entries.
method_label: We first introduce the concept of morpho-fragments, which are intermediate units between characters and lexical entries.
method_label: We describe our approach to resolve problems arising in tokenization so as to attain a language independent morphological analyzer.

===================================
paper_id: 9614236; YEAR: 2008
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_cbow200
TITLE: An unsupervised Hindi stemmer with heuristic improvements
ABSTRACT: background_label: Stemmers are used to convert inflected words into their root or stem.
background_label: Stem does not necessarily correspond to linguistic root of a word.
background_label: Stemming improve performance by reducing morphologically variants into same words.
objective_label: This paper presents an approach is to develop unsupervised Hindi stemmer.
objective_label: This paper focus on the development of an unsupervised stemmer for Hindi and evaluation of approach using manually segmented words.
result_label: We evaluate our approach on 1000-1000 words randomly extracted words (only) from Hindi WordNet1 data base.
background_label: The training data has been constructed by extracting 106403 words extracted from EMILLE2 corpus.
background_label: The observed accuracy was found to be 89.9% after applying some heuristic measures.
background_label: The F-score was 94.96%.
method_label: As the algorithm does not require any language specific information, it can be applied to other Indian languages as well.
method_label: We also evaluate the effect of stemmer in terms of reducing size of index for Hindi information retrieval task.
result_label: The results have been compared with light weight stemmer [10] and UMass stemmer [17].
result_label: Test run shows that our stemmer outperforms both the stemmer.

===================================
paper_id: 18860664; YEAR: 2017
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: abs_tfidfcbow200 - abs_tfidf
TITLE: Joint Semantic Synthesis and Morphological Analysis of the Derived Word
ABSTRACT: background_label: Much like sentences are composed of words, words themselves are composed of smaller units.
background_label: For example, the English word questionably can be analyzed as question+able+ly.
background_label: However, this structural decomposition of the word does not directly give us a semantic representation of the word's meaning.
background_label: Since morphology obeys the principle of compositionality, the semantics of the word can be systematically derived from the meaning of its parts.
objective_label: In this work, we propose a novel probabilistic model of word formation that captures both the analysis of a word w into its constituents segments and the synthesis of the meaning of w from the meanings of those segments.
method_label: Our model jointly learns to segment words into morphemes and compose distributional semantic vectors of those morphemes.
method_label: We experiment with the model on English CELEX data and German DerivBase (Zeller et al., 2013) data.
method_label: We show that jointly modeling semantics increases both segmentation accuracy and morpheme F1 by between 3% and 5%.
result_label: Additionally, we investigate different models of vector composition, showing that recurrent neural networks yield an improvement over simple additive models.
result_label: Finally, we study the degree to which the representations correspond to a linguist's notion of morphological productivity.

===================================
paper_id: 18122636; YEAR: 2014
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: title_tfidfcbow200 - title_tfidf - title_cbow200
TITLE: Morfessor FlatCat: An HMM-Based Method for Unsupervised and Semi-Supervised Learning of Morphology
ABSTRACT: background_label: AbstractMorfessor is a family of methods for learning morphological segmentations of words based on unannotated data.
method_label: We introduce a new variant of Morfessor, FlatCat, that applies a hidden Markov model structure.
method_label: It builds on previous work on Morfessor, sharing model components with the popular Morfessor Baseline and Categories-MAP variants.
result_label: Our experiments show that while unsupervised FlatCat does not reach the accuracy of Categories-MAP, with semisupervised learning it provides state-of-the-art results in the Morpho Challenge 2010 tasks for English, Finnish, and Turkish.

===================================
paper_id: 11703771; YEAR: 2015
adju relevance: Related (+1)
difference: 1; annotator4: 2; annotator3: 1
sources: title_tfidf - title_cbow200 - specter
TITLE: Paradigm classification in supervised learning of morphology
ABSTRACT: background_label: Supervised morphological paradigm learning by identifying and aligning the longest common subsequence found in inflection tables has recently been proposed as a simple yet competitive way to induce morphological patterns.
method_label: We combine this non-probabilistic strategy of inflection table generalization with a discriminative classifier to permit the reconstruction of complete inflection tables of unseen words.
method_label: Our system learns morphological paradigms from labeled examples of inflection patterns (inflection tables) and then produces inflection tables from unseen lemmas or base forms.
result_label: We evaluate the approach on datasets covering 11 different languages and show that this approach results in consistently higher accuracies vis-` other methods on the same task, thus indicating that the general method is a viable approach to quickly creating highaccuracy morphological resources.

===================================
paper_id: 2253786; YEAR: 2007
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: specter - title_tfidf
TITLE: Generating Complex Morphology for Machine Translation
ABSTRACT: background_label: AbstractWe present a novel method for predicting inflected word forms for generating morphologically rich languages in machine translation.
method_label: We utilize a rich set of syntactic and morphological knowledge sources from both source and target sentences in a probabilistic model, and evaluate their contribution in generating Russian and Arabic sentences.
result_label: Our results show that the proposed model substantially outperforms the commonly used baseline of a trigram target language model; in particular, the use of morphological and syntactic features leads to large gains in prediction accuracy.
result_label: We also show that the proposed method is effective with a relatively small amount of data.

===================================
paper_id: 44119185; YEAR: 2018
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidf - specter
TITLE: Using Morphological Knowledge in Open-Vocabulary Neural Language Models
ABSTRACT: background_label: AbstractLanguages with productive morphology pose problems for language models that generate words from a fixed vocabulary.
background_label: Although character-based models allow any possible word type to be generated, they are linguistically naïve: they must discover that words exist and are delimited by spaces-basic linguistic facts that are built in to the structure of word-based models.
method_label: We introduce an openvocabulary language model that incorporates more sophisticated linguistic knowledge by predicting words using a mixture of three generative processes: (1) by generating words as a sequence of characters, (2) by directly generating full word forms, and (3) by generating words as a sequence of morphemes that are combined using a hand-written morphological analyzer.
result_label: Experiments on Finnish, Turkish, and Russian show that our model outperforms character sequence models and other strong baselines on intrinsic and extrinsic measures.
result_label: Furthermore, we show that our model learns to exploit morphological knowledge encoded in the analyzer, and, as a byproduct, it can perform effective unsupervised morphological disambiguation.

===================================
paper_id: 1766004; YEAR: 2005
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: title_tfidfcbow200 - abs_cbow200 - specter - abs_tfidfcbow200
TITLE: Inducing the Morphological Lexicon of a Natural Language from Unannotated Text
ABSTRACT: background_label: This work presents an algorithm for the unsupervised learning, or induction, of a simple morphology of a natural language.
method_label: A probabilistic maximum a posteriori model is utilized, which builds hierarchical representations for a set of morphs, which are morpheme-like units discovered from unannotated text corpora.
method_label: The induced morph lexicon stores parameters related to both the “meaning” and “form” of the morphs it contains.
method_label: These parameters affect the role of the morphs in words.
method_label: The model is implemented in a task of unsupervised morpheme segmentation of Finnish and English words.
result_label: Very good results are obtained for Finnish and almost as good results are obtained in the English task.

===================================
paper_id: 16326127; YEAR: 2015
adju relevance: Related (+1)
difference: 1; annotator4: 2; annotator3: 1
sources: title_tfidf - specter
TITLE: Unsupervised Morphology Induction Using Word Embeddings
ABSTRACT: method_label: We present a language agnostic, unsupervised method for inducing morphological transformations between words.
method_label: The method relies on certain regularities manifest in highdimensional vector spaces.
method_label: We show that this method is capable of discovering a wide range of morphological rules, which in turn are used to build morphological analyzers.
result_label: We evaluate this method across six different languages and nine datasets, and show significant improvements across all languages.

===================================
paper_id: 14277325; YEAR: 2002
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: title_tfidf - abs_tfidf - specter
TITLE: Unsupervised Learning Of Morphology Using A Novel Directed Search Algorithm: Taking The First Step
ABSTRACT: background_label: This paper describes a system for the unsupervised learning of morphological suffixes and stems from word lists.
method_label: The system is composed of a generative probability model and a novel search algorithm.
method_label: By examining morphologically rich subsets of an input lexicon, the search identifies highly productive paradigms.
result_label: Quantitative results are shown by measuring the accuracy of the morphological relations identified.
result_label: Experiments in English and Polish, as well as comparisons with other recent unsupervised morphology learning algorithms demonstrate the effectiveness of this technique.

===================================
paper_id: 7511759; YEAR: 2016
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 2
sources: abs_tfidf - specter
TITLE: A Joint Model for Word Embedding and Word Morphology
ABSTRACT: objective_label: This paper presents a joint model for performing unsupervised morphological analysis on words, and learning a character-level composition function from morphemes to word embeddings.
method_label: Our model splits individual words into segments, and weights each segment according to its ability to predict context words.
method_label: Our morphological analysis is comparable to dedicated morphological analyzers at the task of morpheme boundary recovery, and also performs better than word-based embedding models at the task of syntactic analogy answering.
result_label: Finally, we show that incorporating morphology explicitly into character-level models help them produce embeddings for unseen words which correlate better with human judgments.

===================================
paper_id: 770625; YEAR: 2006
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: abs_tfidf - title_tfidfcbow200
TITLE: An Unsupervised Morpheme-Based HMM For Hebrew Morphological Disambiguation
ABSTRACT: background_label: Morphological disambiguation is the process of assigning one set of morphological features to each individual word in a text.
background_label: When the word is ambiguous (there are several possible analyses for the word), a disambiguation procedure based on the word context must be applied.
objective_label: This paper deals with morphological disambiguation of the Hebrew language, which combines morphemes into a word in both agglutinative and fusional ways.
method_label: We present an un-supervised stochastic model - the only resource we use is a morphological analyzer-which deals with the data sparseness problem caused by the affixational morphology of the Hebrew language.We present a text encoding method for languages with affixational morphology in which the knowledge of word formation rules (which are quite restricted in Hebrew) helps in the disambiguation.
method_label: We adapt HMM algorithms for learning and searching this text representation, in such a way that segmentation and tagging can be learned in parallel in one step.
result_label: Results on a large scale evaluation indicate that this learning improves disambiguation for complex tag sets.
result_label: Our method is applicable to other languages with affix morphology.

===================================
paper_id: 3004001; YEAR: 2009
adju relevance: Related (+1)
difference: 1; annotator4: 0; annotator3: 1
sources: abs_tfidfcbow200 - abs_tfidf - abs_cbow200
TITLE: Learning better transliterations
ABSTRACT: background_label: We introduce a new probabilistic model for transliteration that performs significantly better than previous approaches, is language-agnostic, requiring no knowledge of the source or target languages, and is capable of both generation (creating the most likely transliteration of a source word) and discovery (selecting the most likely transliteration from a list of candidate words).
background_label: Our experimental results demonstrate improved accuracy over the existing state-of-the-art by more than 10% in Chinese, Hebrew and Russian.
method_label: While past work has commonly made use of fixed-size n-gram features along with more traditional models such as HMM or Perceptron, we utilize an intuitive notion of "productions", where each source word can be segmented into a series of contiguous, non-overlapping substrings of any size, each of which independently transliterates to a substring in the target language with a given probability.
method_label: To learn these parameters, we employ Expectation-Maximization (EM), with the alignment between substrings in the source and target word training pairs as our latent data.
method_label: Despite the size of the parameter space and the 2(|w|-1) possible segmentations to consider for each word, by using dynamic programming each iteration of EM takes O(m^6 * n) time, where m is the length of the longest word in the data and n is the number of word pairs, and is very fast in practice.
method_label: Furthermore, discovering transliterations takes only O(m^4 * w) time, where w is the number of candidate words to choose from, and generating a transliteration takes O(m2 * k2) time, where k is a pruning constant (we used a value of 100).
result_label: Additionally, we are able to obtain training examples in an unsupervised fashion from Wikipedia by using a relatively simple algorithm to filter potential word pairs.

===================================
paper_id: 22627757; YEAR: 1985
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidf - abs_tfidfcbow200
TITLE: Speech Perception, Word Recognition and the Structure of the Lexicon.
ABSTRACT: background_label: This paper reports the results of three projects concerned with auditory word recognition and the structure of the lexicon.
objective_label: The first project was designed to experimentally test several specific predictions derived from MACS, a simulation model of the Cohort Theory of word recognition.
method_label: Using a priming paradigm, evidence was obtained for acoustic-phonetic activation in word recognition in three experiments.
method_label: The second project describes the results of analyses of the structure and distribution of words in the lexicon using a large lexical database.
method_label: Statistics about similarity spaces for high and low frequency words were applied to previously published data on the intelligibility of words presented in noise.
method_label: Differences in identification were shown to be related to structural factors about the specific words and the distribution of similar words in their neighborhoods.
method_label: Finally, the third project describes efforts at developing a new theory of word recognition known as Phonetic Refinement Theory.
method_label: The theory is based on findings from human listeners and was designed to incorporate some of the detailed acoustic-phonetic and phonotactic knowledge that human listeners have about the internal structure of words and the organization of words in the lexicon, and how, they use this knowledge in word recognition.
result_label: Taken together, the results of these projects demonstrate a number of new and important findings about the relation between speech perception and auditory word recognition, two areas of research that have traditionally been approached from quite different perspectives in the past.

===================================
paper_id: 3104165; YEAR: 2016
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200
TITLE: A hybrid approach to Vietnamese word segmentation
ABSTRACT: background_label: Word segmentation is the very first task for Vietnamese language processing.
background_label: Word-segmented text is the input of almost other NLP tasks.
background_label: This task faces some challenges due to specific characteristics of the language.
background_label: As in many other Asian languages such as Japanese, Korean and Chinese, white spaces in Vietnamese are not always used as word separators and a word may contain one or more syllables.
method_label: In this paper, we propose an efficient hybrid approach to detect word boundary for Vietnamese texts using logistic regression as a binary classifier combining with longest matching algorithm.
background_label: First, longest matching algorithm is used to catch words that contain more than two syllables in input sentence.
method_label: Next, the system utilizes the classifier to determine the boundary of 2-syllable words and proper names.
method_label: Then, the predictions having low confidence conducted by the classifier are verified by a dictionary to get the final result.
result_label: Our system can achieve an F-measure of 98.82% which is the most accurate result for Vietnamese word segmentation to the best of our knowledge.
result_label: Moreover, the system also has a high speed.
result_label: It can run word segmentation for nearly 34k tokens per second.

===================================
paper_id: 3261177; YEAR: 1998
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: specter - abs_cbow200 - abs_tfidfcbow200
TITLE: A Freely Available Morphological Analyzer, Disambiguator and Context Sensitive Lemmatizer for German
ABSTRACT: background_label: In this paper we present Morphy, an integrated tool for German morphology, part-of-speech tagging and context-sensitive lemmatization.
background_label: Its large lexicon of more than 320,000 word forms plus its ability to process German compound nouns guarantee a wide morphological coverage.
method_label: Syntactic ambiguities can be resolved with a standard statistical part-of-speech tagger.
method_label: By using the output of the tagger, the lemmatizer can determine the correct root even for ambiguous word forms.
result_label: The complete package is freely available and can be downloaded from the World Wide Web.

===================================
paper_id: 53733849; YEAR: 2018
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: abs_tfidfcbow200 - abs_cbow200
TITLE: Implanting Rational Knowledge into Distributed Representation at Morpheme Level
ABSTRACT: background_label: Previously, researchers paid no attention to the creation of unambiguous morpheme embeddings independent from the corpus, while such information plays an important role in expressing the exact meanings of words for parataxis languages like Chinese.
objective_label: In this paper, after constructing the Chinese lexical and semantic ontology based on word-formation, we propose a novel approach to implanting the structured rational knowledge into distributed representation at morpheme level, naturally avoiding heavy disambiguation in the corpus.
method_label: We design a template to create the instances as pseudo-sentences merely from the pieces of knowledge of morphemes built in the lexicon.
method_label: To exploit hierarchical information and tackle the data sparseness problem, the instance proliferation technique is applied based on similarity to expand the collection of pseudo-sentences.
method_label: The distributed representation for morphemes can then be trained on these pseudo-sentences using word2vec.
result_label: For evaluation, we validate the paradigmatic and syntagmatic relations of morpheme embeddings, and apply the obtained embeddings to word similarity measurement, achieving significant improvements over the classical models by more than 5 Spearman scores or 8 percentage points, which shows very promising prospects for adoption of the new source of knowledge.

===================================
paper_id: 7126582; YEAR: 2003
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: specter
TITLE: Building the Croatian Morphological Lexicon
ABSTRACT: background_label: The paper presents the work being done so far on the building of the Croatian Morphological Lexicon (CML).
background_label: It has been collected since 2002 in the Institute of Linguistics, Faculty of Philosophy, University of Zagreb.
method_label: The CML is planned to have two sub-lexicons: derivative/compositional and inflectional, both produced by a generator.
method_label: The result of generation is lexicon as two distinct lists of generated combinations of morphemes and complete word-forms both with additional data that can be used in further processing.
method_label: The inflectional component is presented more in detail in the second part of the paper.
result_label: At the end, the several possible applications of CML are discussed.

===================================
paper_id: 52943630; YEAR: 2018
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: title_tfidfcbow200
TITLE: Unsupervised Neural Word Segmentation for Chinese via Segmental Language Modeling
ABSTRACT: background_label: Previous traditional approaches to unsupervised Chinese word segmentation (CWS) can be roughly classified into discriminative and generative models.
background_label: The former uses the carefully designed goodness measures for candidate segmentation, while the latter focuses on finding the optimal segmentation of the highest generative probability.
background_label: However, while there exists a trivial way to extend the discriminative models into neural version by using neural language models, those of generative ones are non-trivial.
objective_label: In this paper, we propose the segmental language models (SLMs) for CWS.
method_label: Our approach explicitly focuses on the segmental nature of Chinese, as well as preserves several properties of language models.
method_label: In SLMs, a context encoder encodes the previous context and a segment decoder generates each segment incrementally.
result_label: As far as we know, we are the first to propose a neural model for unsupervised CWS and achieve competitive performance to the state-of-the-art statistical models on four different datasets from SIGHAN 2005 bakeoff.

===================================
paper_id: 17211315; YEAR: 2016
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: title_tfidfcbow200
TITLE: A grapheme-level approach for constructing a Korean morphological analyzer without linguistic knowledge
ABSTRACT: background_label: Morphological analysis is an essential step for processing the Korean language, due to highly agglutinative properties of the language.
objective_label: In this paper, we propose a novel approach for constructing a Korean morphological analyzer that can capture linguistic properties using graphemes as basic processing units.
method_label: Since our model does not utilize prior linguistic knowledge, the model can be applied to other training corpora with ease.
method_label: Our model performs morphological analysis through two consecutive sequence labeling tasks: lexical form recovery and part-of-speech tagging.
method_label: In the lexical form recovery step, morphological changes of an input sentence are restored to the original form.
method_label: Then in the part-of-speech step, corresponding part-of-speech tags are attached to the recovered form.
result_label: Experimental results show that our model outperforms previous models which are constructed without prior knowledge.

===================================
paper_id: 18811442; YEAR: 2010
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: abs_tfidf - specter
TITLE: A Sequence Labeling Approach to Morphological Analyzer for Tamil Language
ABSTRACT: background_label: Morphological analysis is the basic process for any Natural Language Processing task.
background_label: Morphology is the study of internal structure of the word.
background_label: Morphological analysis retrieves the grammatical features and properties of a morphologically inflected word.
background_label: Capturing the agglutinative structure of Tamil words by an automatic system is a challenging job.
background_label: Generally rule based approaches are used for building morphological analyzer.
objective_label: In this paper we propose a novel approach to solve the morphological analyzer problem using machine learning methodology.
method_label: Here morphological analyzer problem is redefined as classification problem.
method_label: This approach is based on sequence labeling and training by kernel methods that captures the non linear relationships of the morphological features from training data samples in a better and simpler way.
other_label: Keywordsmorphology; morphological analyzer; machine learning; sequence labeling.

===================================
paper_id: 121844001; YEAR: 2006
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 2
sources: cited - title_tfidf - title_tfidfcbow200 - title_cbow200 - specter
TITLE: A-Morphous Morphology
ABSTRACT: background_label: The central claims of the A-Morphous Morphology framework for the description of word structure are described and motivated.
background_label: A variety of reasons for rejecting the traditional notion of the morpheme are explored.
result_label: The consequences of (one form of) the Lexicalist Hypothesis for the relation between morphology and syntax are drawn, and arguments are given that an analysis of words into morphemes does not provide the appropriate informational interface between these two components of grammar.

===================================
paper_id: 6063298; YEAR: 2007
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: specter
TITLE: High-Performance, Language-Independent Morphological Segmentation
ABSTRACT: background_label: AbstractThis paper introduces an unsupervised morphological segmentation algorithm that shows robust performance for four languages with different levels of morphological complexity.
method_label: In particular, our algorithm outperforms Goldsmith's Linguistica and Creutz and Lagus's Morphessor for English and Bengali, and achieves performance that is comparable to the best results for all three PASCAL evaluation datasets.
result_label: Improvements arise from (1) the use of relative corpus frequency and suffix level similarity for detecting incorrect morpheme attachments and (2) the induction of orthographic rules and allomorphs for segmenting words where roots exhibit spelling changes during morpheme attachments.

===================================
paper_id: 14630989; YEAR: 2015
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: specter
TITLE: Robust Morphological Tagging with Word Representations
ABSTRACT: background_label: We present a comparative investigation of word representations for part-of-speech (POS) and morphological tagging, focusing on scenarios with considerable differences between training and test data where a robust approach is necessary.
objective_label: Instead of adapting the model towards a specific domain we aim to build a robust model across domains.
method_label: We developed a test suite for robust tagging consisting of six languages and different domains.
result_label: We find that representations similar to Brown clusters perform best for POS tagging and that word representations based on linguistic morphological analyzers perform best for morphological tagging.

===================================
paper_id: 13044552; YEAR: 1999
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: specter - abs_tfidf
TITLE: Memory-Based Morphological Analysis
ABSTRACT: background_label: We present a general architecture for efficient and deterministic morphological analysis based on memory-based learning, and apply it to morphological analysis of Dutch.
method_label: The system makes direct mappings from letters in context to rich categories that encode morphological boundaries, syntactic class labels, and spelling changes.
result_label: Both precision and recall of labeled morphemes are over 84% on held-out dictionary test words and estimated to be over 93% in free text.

===================================
paper_id: 1974029; YEAR: 2008
adju relevance: Related (+1)
difference: 1; annotator4: 0; annotator3: 1
sources: title_tfidfcbow200
TITLE: On the Meaning of Words and Dinosaur Bones: Lexical Knowledge Without a Lexicon
ABSTRACT: background_label: Although for many years a sharp distinction has been made in language research between rules and words-with primary interest on rules-this distinction is now blurred in many theories.
background_label: If anything, the focus of attention has shifted in recent years in favor of words.
background_label: Results from many different areas of language research suggest that the lexicon is representationally rich, that it is the source of much productive behavior, and that lexically-specific information plays a critical and early role in the interpretation of grammatical structure.
background_label: But how much information can or should be placed in the lexicon?
objective_label: This is the question I address here.
method_label: I review a set of studies whose results indicate that event knowledge plays a significant role in early stages of sentence processing and structural analysis.
result_label: This poses a conundrum for traditional views of the lexicon.
method_label: Either the lexicon must be expanded to include factors that do not plausibly seem to belong there; or else virtually all information about word meaning is removed, leaving the lexicon impoverished.
result_label: I suggest a third alternative, which provides a way to account for lexical knowledge without a mental lexicon.

===================================
paper_id: 9671238; YEAR: 2000
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: specter
TITLE: Robust, Applied Morphological Generation
ABSTRACT: background_label: In practical natural language generation systems it is often advantageous to have a separate component that deals purely with morphological processing.
method_label: We present such a component: a fast and robust morphological generator for English based on finite-state techniques that generates a word form given a specification of the lemma, part-of-speech, and the type of inflection required.
method_label: We describe how this morphological generator is used in a prototype system for automatic simplification of English newspaper text, and discuss practical morphological and orthographic issues we have encountered in generation of unrestricted text within this application.

===================================
paper_id: 2687347; YEAR: 2007
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: abs_tfidfcbow200
TITLE: Chinese Segmentation with a Word-Based Perceptron Algorithm
ABSTRACT: background_label: AbstractStandard approaches to Chinese word segmentation treat the problem as a tagging task, assigning labels to the characters in the sequence indicating whether the character marks a word boundary.
background_label: Discriminatively trained models based on local character features are used to make the tagging decisions, with Viterbi decoding finding the highest scoring segmentation.
objective_label: In this paper we propose an alternative, word-based segmentor, which uses features based on complete words and word sequences.
method_label: The generalized perceptron algorithm is used for discriminative training, and we use a beamsearch decoder.
result_label: Closed tests on the first and second SIGHAN bakeoffs show that our system is competitive with the best in the literature, achieving the highest reported F-scores for a number of corpora.

===================================
paper_id: 206467; YEAR: 2011
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: specter
TITLE: Modeling Syntactic Context Improves Morphological Segmentation
ABSTRACT: background_label: AbstractThe connection between part-of-speech (POS) categories and morphological properties is well-documented in linguistics but underutilized in text processing systems.
objective_label: This paper proposes a novel model for morphological segmentation that is driven by this connection.
method_label: Our model learns that words with common affixes are likely to be in the same syntactic category and uses learned syntactic categories to refine the segmentation boundaries of words.
result_label: Our results demonstrate that incorporating POS categorization yields substantial performance gains on morphological segmentation of Arabic.

===================================
paper_id: 16639831; YEAR: 2004
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: specter
TITLE: Multilingual Noise-Robust Supervised Morphological Analysis Using The WordFrame Model
ABSTRACT: background_label: This paper presents the WordFrame model, a noise-robust supervised algorithm capable of inducing morphological analyses for languages which exhibit prefixation, suffixation, and internal vowel shifts.
method_label: In combination with a naive approach to suffix-based morphology, this algorithm is shown to be remarkably effective across a broad range of languages, including those exhibiting infixation and partial reduplication.
result_label: Results are presented for over 30 languages with a median accuracy of 97.5% on test sets including both regular and irregular verbal inflections.
result_label: Because the proposed method trains extremely well under conditions of high noise, it is an ideal candidate for use in co-training with unsupervised algorithms.

===================================
paper_id: 8819802; YEAR: 2007
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: title_tfidf - title_tfidfcbow200 - title_cbow200 - specter
TITLE: Unsupervised models for morpheme segmentation and morphology learning
ABSTRACT: background_label: We present a model family called Morfessor for the unsupervised induction of a simple morphology from raw text data.
method_label: The model is formulated in a probabilistic maximum a posteriori framework.
method_label: Morfessor can handle highly inflecting and compounding languages where words can consist of lengthy sequences of morphemes.
method_label: A lexicon of word segments, called morphs, is induced from the data.
method_label: The lexicon stores information about both the usage and form of the morphs.
result_label: Several instances of the model are evaluated quantitatively in a morpheme segmentation task on different sized sets of Finnish as well as English data.
result_label: Morfessor is shown to perform very well compared to a widely known benchmark algorithm, in particular on Finnish data.

===================================
paper_id: 1968269; YEAR: 2001
adju relevance: Related (+1)
difference: 1; annotator4: 0; annotator3: 1
sources: cited - title_tfidf - title_tfidfcbow200 - title_cbow200 - specter
TITLE: Unsupervised Learning Of The Morphology Of A Natural Language
ABSTRACT: background_label: This study reports the results of using minimum description length (MDL) analysis to model unsupervised learning of the morphological segmentation of European languages, using corpora ranging in size from 5,000 words to 500,000 words.
method_label: We develop a set of heuristics that rapidly develop a probabilistic morphological grammar, and use MDL as our primary tool to determine whether the modifications proposed by the heuristics will be adopted or not.
method_label: The resulting grammar matches well the analysis that would be developed by a human morphologist.
result_label: In the final section, we discuss the relationship of this style of MDL grammatical analysis to the notion of evaluation metric in early generative grammar.

===================================
paper_id: 6286444; YEAR: 2011
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: title_tfidf - title_cbow200 - specter
TITLE: Unsupervised Learning of Morphology
ABSTRACT: objective_label: This article surveys work on Unsupervised Learning of Morphology.
objective_label: We define Unsupervised Learning of Morphology as the problem of inducing a description (of some kind, even if only morpheme-segmentation) of how orthographic words are built up given only raw text data of a language.
method_label: We briefly go through the history and motivation of the this problem.
method_label: Next, over 200 items of work are listed with a brief characterization, and the most important ideas in the field are critically discussed.
result_label: We summarize the achievements so far and give pointers for future developments.

===================================
paper_id: 5244724; YEAR: 2014
adju relevance: Related (+1)
difference: 1; annotator4: 0; annotator3: 1
sources: abs_tfidf
TITLE: Learning Effective Word Embedding using Morphological Word Similarity
ABSTRACT: other_label: Abstract.
background_label: Deep learning techniques aim at obtaining high-quality distributed representations of words, i.e., word embeddings, to address text mining and natural language processing tasks.
background_label: Recently, efficient methods have been proposed to learn word embeddings from context that captures both semantic and syntactic relationships between words.
background_label: However, it is challenging to handle unseen words or rare words with insufficient context.
objective_label: In this paper, inspired by the study on word recognition process in cognitive psychology, we propose to take advantage of seemingly less obvious but essentially important morphological word similarity to address these challenges.
method_label: In particular, we introduce a novel neural network architecture that leverages both contextual information and morphological word similarity to learn word embeddings.
method_label: Meanwhile, the learning architecture is also able to refine the pre-defined morphological knowledge and obtain more accurate word similarity.
result_label: Experiments on an analogical reasoning task and a word similarity task both demonstrate that the proposed method can greatly enhance the effectiveness of word embeddings.

===================================
paper_id: 2547808; YEAR: 2001
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: specter
TITLE: Knowledge-Free Induction of Inflectional Morphologies
ABSTRACT: objective_label: We propose an algorithm to automatically induce the morphology of inflectional languages using only text corpora and no human input.
method_label: Our algorithm combines cues from orthography, semantics, and syntactic distributions to induce morphological relationships in German, Dutch, and English.
result_label: Using CELEX as a gold standard for evaluation, we show our algorithm to be an improvement over any knowledge-free algorithm yet proposed.

===================================
paper_id: 15276369; YEAR: 2008
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: title_tfidfcbow200 - specter
TITLE: Unsupervised Word Segmentation for Sesotho Using Adaptor Grammars
ABSTRACT: background_label: This paper describes a variety of non-parametric Bayesian models of word segmentation based on Adaptor Grammars that model different aspects of the input and incorporate different kinds of prior knowledge, and applies them to the Bantu language Sesotho.
background_label: While we find overall word segmentation accuracies lower than these models achieve on English, we also find some interesting differences in which factors contribute to better word segmentation.
result_label: Specifically, we found little improvement to word segmentation accuracy when we modeled contextual dependencies, while modeling morphological structure did improve segmentation accuracy.

===================================
paper_id: 15237215; YEAR: 2016
adju relevance: Related (+1)
difference: 1; annotator4: 0; annotator3: 1
sources: title_tfidfcbow200 - title_cbow200
TITLE: Corpus analysis without prior linguistic knowledge - unsupervised mining of phrases and subphrase structure
ABSTRACT: background_label: When looking at the structure of natural language,"phrases"and"words"are central notions.
objective_label: We consider the problem of identifying such"meaningful subparts"of language of any length and underlying composition principles in a completely corpus-based and language-independent way without using any kind of prior linguistic knowledge.
method_label: Unsupervised methods for identifying"phrases", mining subphrase structure and finding words in a fully automated way are described.
method_label: This can be considered as a step towards automatically computing a"general dictionary and grammar of the corpus".
method_label: We hope that in the long run variants of our approach turn out to be useful for other kind of sequence data as well, such as, e.g., speech, genom sequences, or music annotation.
result_label: Even if we are not primarily interested in immediate applications, results obtained for a variety of languages show that our methods are interesting for many practical tasks in text mining, terminology extraction and lexicography, search engine technology, and related fields.

===================================
paper_id: 52284208; YEAR: 2018
adju relevance: Related (+1)
difference: 1; annotator4: 0; annotator3: 1
sources: abs_cbow200
TITLE: Phoneme Based Embedded Segmental K-Means for Unsupervised Term Discovery
ABSTRACT: background_label: Identifying and grouping the frequently occurring word-like patterns from raw acoustic waveforms is an important task in the zero resource speech processing.
background_label: Embedded segmental K-means (ES-KMeans) discovers both the word boundaries and the word types from raw data.
method_label: Starting from an initial set of subword boundaries, the ES-Kmeans iteratively eliminates some of the boundaries to arrive at frequently occurring longer word patterns.
method_label: Notice that the initial word boundaries will not be adjusted during the process.
result_label: As a result, the performance of the ES-Kmeans critically depends on the initial subword boundaries.
result_label: Originally, syllable boundaries were used to initialize ES-Kmeans.
objective_label: In this paper, we propose to use a phoneme segmentation method that produces boundaries closer to true boundaries for ES-KMeans initialization.
method_label: The use of shorter units increases the number of initial boundaries which leads to a significant increment in the computational complexity.
method_label: To reduce the computational cost, we extract compact lower dimensional embeddings from an auto-encoder.
method_label: The proposed algorithm is benchmarked on Zero Resource 2017 challenge, which consists of 70 hours of unlabeled data across three languages, viz.
method_label: English, French, and Mandarin.
result_label: The proposed algorithm outperforms the baseline system without any language-specific parameter tuning.

===================================
paper_id: 18379868; YEAR: 2010
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: abs_tfidf - specter
TITLE: Morphonette: a morphological network of French
ABSTRACT: background_label: This paper describes in details the first version of Morphonette, a new French morphological resource and a new radically lexeme-based method of morphological analysis.
method_label: This research is grounded in a paradigmatic conception of derivational morphology where the morphological structure is a structure of the entire lexicon and not one of the individual words it contains.
result_label: The discovery of this structure relies on a measure of morphological similarity between words, on formal analogy and on the properties of two morphological paradigms:

===================================
paper_id: 5133576; YEAR: 2002
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: title_tfidf - specter
TITLE: Unsupervised Discovery of Morphemes
ABSTRACT: method_label: We present two methods for unsupervised segmentation of words into morpheme-like units.
method_label: The model utilized is especially suited for languages with a rich morphology, such as Finnish.
method_label: The first method is based on the Minimum Description Length (MDL) principle and works online.
method_label: In the second method, Maximum Likelihood (ML) optimization is used.
method_label: The quality of the segmentations is measured using an evaluation method that compares the segmentations produced to an existing morphological analysis.
result_label: Experiments on both Finnish and English corpora show that the presented methods perform well compared to a current state-of-the-art system.

===================================
paper_id: 52874710; YEAR: 2003
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 2
sources: specter
TITLE: Unsupervised Segmentation of Words Using Prior Distributions of Morph Length and Frequency
ABSTRACT: background_label: We present a language-independent and unsupervised algorithm for the segmentation of words into morphs.
method_label: The algorithm is based on a new generative probabilistic model, which makes use of relevant prior information on the length and frequency distributions of morphs in a language.
result_label: Our algorithm is shown to outperform two competing algorithms, when evaluated on data from a language with agglutinative morphology (Finnish), and to perform well also on English data.

===================================
paper_id: 13926706; YEAR: 1997
adju relevance: Irrelevant (0)
difference: 1; annotator4: 0; annotator3: 1
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: Pronunciation by Analogy: Impact of Implementational Choices on Performance
ABSTRACT: background_label: Pronunciation by analogy (PbA) is an emerging, data-driven technique with potential application in text-to-speech (TTS) systems, as well as being an influential psychological model of reading aloud.
background_label: The underlying idea is that a pronunciation for an unknown word (i.e., one not in the dictionary, or lexicon, of the human or machine reader) is assembled by matching substrings of the input to substrings of known, lexical words, hypothesizing a partial pronunciation for each matched substring from the lexical knowledge of the reader, and concatenating the partial pronunciations.
objective_label: This paper assesses the capability of PbA to derive pronunciations for unknown words of English.
method_label: As a psychological model, PbA is under-specified, that is, the implementor of a simulation of the process faces detailed choices which can only be resolved by trial and error.
objective_label: One goal for this paper is to explore the impact of certain basic implementational choices on the performance of PbA systems.
background_label: The variables studied are the specific lexical database used as the basis of the analogy process, the way of ranking/scoring candidate pronunciations, and the effect of manual versus automatic alignment of letters and phonemes.
background_label: When tested with short (monosyllabic) pseudowords previously used in experimental psychology studies, the lowest error rate achieved is 14.3% (for a test set of size 70).
background_label: We conclude that current PbA systems are at best poor models of pseudoword pronunciation by humans.
method_label: To assess their suitability for use in a TTS application, in which multisyllabic words will be encountered, the implementations have also been tested with lexical words temporarily removed from the dictionary.
result_label: The best performance obtained was 93.5% phonemes correct (corresponding to 67.9% words correct) for a 16,280-word dictionary.
result_label: This is vastly superior to the 25.7% words correct obtained using a set of popular letter-to-sound rules, indicating considerable scope for analogy methods to be exploited in fu

===================================
paper_id: 174800654; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Measuring the perceptual availability of phonological features during language acquisition using unsupervised binary stochastic autoencoders
ABSTRACT: background_label: AbstractIn this paper, we deploy binary stochastic neural autoencoder networks as models of infant language learning in two typologically unrelated languages (Xitsonga and English).
background_label: We show that the drive to model auditory percepts leads to latent clusters that partially align with theory-driven phonemic categories.
other_label: We further evaluate the degree to which theorydriven phonological features are encoded in the latent bit patterns, finding that some (e.g.
other_label: [±approximant]), are well represented by the network in both languages, while others (e.g.
method_label: [±spread glottis]) are less so.
result_label: Together, these findings suggest that many reliable cues to phonemic structure are immediately available to infants from bottom-up perceptual characteristics alone, but that these cues must eventually be supplemented by top-down lexical and phonotactic information to achieve adult-like phone discrimination.
result_label: Our results also suggest differences in degree of perceptual availability between features, yielding testable predictions as to which features might depend more or less heavily on top-down cues during child language acquisition.

===================================
paper_id: 29427154; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Learning without Prejudice: Avoiding Bias in Webly-Supervised Action Recognition
ABSTRACT: background_label: Webly-supervised learning has recently emerged as an alternative paradigm to traditional supervised learning based on large-scale datasets with manual annotations.
background_label: The key idea is that models such as CNNs can be learned from the noisy visual data available on the web.
objective_label: In this work we aim to exploit web data for video understanding tasks such as action recognition and detection.
objective_label: One of the main problems in webly-supervised learning is cleaning the noisy labeled data from the web.
method_label: The state-of-the-art paradigm relies on training a first classifier on noisy data that is then used to clean the remaining dataset.
method_label: Our key insight is that this procedure biases the second classifier towards samples that the first one understands.
method_label: Here we train two independent CNNs, a RGB network on web images and video frames and a second network using temporal information from optical flow.
method_label: We show that training the networks independently is vastly superior to selecting the frames for the flow classifier by using our RGB network.
result_label: Moreover, we show benefits in enriching the training set with different data sources from heterogeneous public web databases.
result_label: We demonstrate that our framework outperforms all other webly-supervised methods on two public benchmarks, UCF-101 and Thumos'14.

===================================
paper_id: 2479254; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf
TITLE: Ending-based Strategies for Part-of-speech Tagging
ABSTRACT: background_label: Probabilistic approaches to part-of-speech tagging rely primarily on whole-word statistics about word/tag combinations as well as contextual information.
background_label: But experience shows about 4 per cent of tokens encountered in test sets are unknown even when the training set is as large as a million words.
background_label: Unseen words are tagged using secondary strategies that exploit word features such as endings, capitalizations and punctuation marks.
objective_label: In this work, word-ending statistics are primary and whole-word statistics are secondary.
method_label: First, a tagger was trained and tested on word endings only.
method_label: Subsequent experiments added back whole-word statistics for the words occurring most frequently in the training set.
method_label: As grew larger, performance was expected to improve, in the limit performing the same as word-based taggers.
result_label: Surprisingly, the ending-based tagger initially performed nearly as well as the word-based tagger; in the best case, its performance significantly exceeded that of the word-based tagger.
result_label: Lastly, and unexpectedly, an effect of negative returns was observed - as grew larger, performance generally improved and then declined.
result_label: By varying factors such as ending length and tag-list strategy, we achieved a success rate of 97.5 percent.

===================================
paper_id: 118988729; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: A Microphotonic Astrocomb
ABSTRACT: background_label: One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers.
background_label: It has already been realized that the large number of exactly known optical frequencies provided by laser frequency combs ('astrocombs') can significantly surpass conventionally used hollow-cathode lamps as calibration light sources.
background_label: A remaining challenge, however, is generation of frequency combs with lines resolvable by astronomical spectrometers.
method_label: Here we demonstrate an astrocomb generated via soliton formation in an on-chip microphotonic resonator ('microresonator') with a resolvable line spacing of 23.7 GHz.
method_label: This comb is providing wavelength calibration on the 10 cm/s radial velocity level on the GIANO-B high-resolution near-infrared spectrometer.
result_label: As such, microresonator frequency combs have the potential of providing broadband wavelength calibration for the next-generation of astronomical instruments in planet-hunting and cosmological research.

===================================
paper_id: 4901129; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: The EcoLexicon Semantic Sketch Grammar: from Knowledge Patterns to Word Sketches
ABSTRACT: background_label: Many projects have applied knowledge patterns (KPs) to the retrieval of specialized information.
background_label: Yet terminologists still rely on manual analysis of concordance lines to extract semantic information, since there are no user-friendly publicly available applications enabling them to find knowledge rich contexts (KRCs).
method_label: To fill this void, we have created the KP-based EcoLexicon Semantic SketchGrammar (ESSG) in the well-known corpus query system Sketch Engine.
method_label: For the first time, the ESSG is now publicly available inSketch Engine to query the EcoLexicon English Corpus.
method_label: Additionally, reusing the ESSG in any English corpus uploaded by the user enables Sketch Engine to extract KRCs codifying generic-specific, part-whole, location, cause and function relations, because most of the KPs are domain-independent.
result_label: The information is displayed in the form of summary lists (word sketches) containing the pairs of terms linked by a given semantic relation.
objective_label: This paper describes the process of building a KP-based sketch grammar with special focus on the last stage, namely, the evaluation with refinement purposes.
method_label: We conducted an initial shallow precision and recall evaluation of the 64 English sketch grammar rules created so far for hyponymy, meronymy and causality.
method_label: Precision was measured based on a random sample of concordances extracted from each word sketch type.
method_label: Recall was assessed based on a random sample of concordances where known term pairs are found.
result_label: The results are necessary for the improvement and refinement of the ESSG.
result_label: The noise of false positives helped to further specify the rules, whereas the silence of false negatives allows us to find useful new patterns.

===================================
paper_id: 119309930; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf
TITLE: The power word problem
ABSTRACT: background_label: In this work we introduce a new succinct variant of the word problem in a finitely generated group $G$, which we call the power word problem: the input word may contain powers $p^x$, where $p$ is a finite word over generators of $G$ and $x$ is a binary encoded integer.
background_label: The power word problem is a restriction of the compressed word problem, where the input word is represented by a straight-line program (i.e., an algebraic circuit over $G$).
method_label: The main result of the paper states that the power word problem for a finitely generated free group $F$ is AC$^0$-Turing-reducible to the word problem for $F$.
result_label: Moreover, the following hardness result is shown: For a wreath product $G \wr \mathbb{Z}$, where $G$ is either free of rank at least two or finite non-solvable, the power word problem is complete for coNP.
result_label: This contrasts with the situation where $G$ is abelian: then the power word problem is shown to be in TC$^0$.

===================================
paper_id: 127598; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidf
TITLE: Controlled Experiments for Word Embeddings
ABSTRACT: objective_label: An experimental approach to studying the properties of word embeddings is proposed.
background_label: Controlled experiments, achieved through modifications of the training corpus, permit the demonstration of direct relations between word properties and word vector direction and length.
method_label: The approach is demonstrated using the word2vec CBOW model with experiments that independently vary word frequency and word co-occurrence noise.
method_label: The experiments reveal that word vector length depends more or less linearly on both word frequency and the level of noise in the co-occurrence distribution of the word.
result_label: The coefficients of linearity depend upon the word.
result_label: The special point in feature space, defined by the (artificial) word with pure noise in its co-occurrence distribution, is found to be small but non-zero.

===================================
paper_id: 21730518; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: Expanding Abbreviations in a Strongly Inflected Language: Are Morphosyntactic Tags Sufficient?
ABSTRACT: background_label: In this paper, the problem of recovery of morphological information lost in abbreviated forms is addressed with a focus on highly inflected languages.
background_label: Evidence is presented that the correct inflected form of an expanded abbreviation can in many cases be deduced solely from the morphosyntactic tags of the context.
method_label: The prediction model is a deep bidirectional LSTM network with tag embedding.
method_label: The training and evaluation data are gathered by finding the words which could have been abbreviated and using their corresponding morphosyntactic tags as the labels, while the tags of the context words are used as the input features for classification.
method_label: The network is trained on over 10 million words from the Polish Sejm Corpus and achieves 74.2% prediction accuracy on a smaller, but more general National Corpus of Polish.
result_label: The analysis of errors suggests that performance in this task may improve if some prior knowledge about the abbreviated word is incorporated into the model.

===================================
paper_id: 118719120; YEAR: 1985
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: A Framework for Computational Morphology
ABSTRACT: objective_label: This paper outlines a new methodology for describing the “internal structure” (or “skeleton”) of planar point sets.
method_label: The methodology, which is based on parameterized measures of neighbourliness, gives rise to a spectrum of possible internal shapes.
method_label: Applications to the analysis of both point set and network patterns are described.

===================================
paper_id: 625189; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: Knowledge-based Word Sense Disambiguation using Topic Models
ABSTRACT: background_label: Word Sense Disambiguation is an open problem in Natural Language Processing which is particularly challenging and useful in the unsupervised setting where all the words in any given text need to be disambiguated without using any labeled data.
background_label: Typically WSD systems use the sentence or a small window of words around the target word as the context for disambiguation because their computational complexity scales exponentially with the size of the context.
method_label: In this paper, we leverage the formalism of topic model to design a WSD system that scales linearly with the number of words in the context.
method_label: As a result, our system is able to utilize the whole document as the context for a word to be disambiguated.
method_label: The proposed method is a variant of Latent Dirichlet Allocation in which the topic proportions for a document are replaced by synset proportions.
method_label: We further utilize the information in the WordNet by assigning a non-uniform prior to synset distribution over words and a logistic-normal prior for document distribution over synsets.
result_label: We evaluate the proposed method on Senseval-2, Senseval-3, SemEval-2007, SemEval-2013 and SemEval-2015 English All-Word WSD datasets and show that it outperforms the state-of-the-art unsupervised knowledge-based WSD system by a significant margin.

===================================
paper_id: 8902599; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf
TITLE: Part-of-Speech Relevance Weights for Learning Word Embeddings
ABSTRACT: objective_label: This paper proposes a model to learn word embeddings with weighted contexts based on part-of-speech (POS) relevance weights.
background_label: POS is a fundamental element in natural language.
background_label: However, state-of-the-art word embedding models fail to consider it.
objective_label: This paper proposes to use position-dependent POS relevance weighting matrices to model the inherent syntactic relationship among words within a context window.
method_label: We utilize the POS relevance weights to model each word-context pairs during the word embedding training process.
method_label: The model proposed in this paper paper jointly optimizes word vectors and the POS relevance matrices.
result_label: Experiments conducted on popular word analogy and word similarity tasks all demonstrated the effectiveness of the proposed method.

===================================
paper_id: 82456167; YEAR: 2007
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: Janeway's Immunobiology
ABSTRACT: background_label: Part I An Introduction to Immunobiology and Innate Immunity 1.
background_label: Basic Concepts in Immunology 2.
background_label: Innate Immunity Part II The Recognition of Antigen 3.
background_label: Antigen Recognition by B-cell and T-cell Receptors 4.
method_label: The Generation of Lymphocyte Antigen Receptors 5.
method_label: Antigen Presentation to T Lymphocytes Part III The Development of Mature Lymphocyte Receptor Repertoires 6.
method_label: Signaling Through Immune System Receptors 7.
result_label: The Development and Survival of Lymphocytes Part IV The Adaptive Immune Response 8.
background_label: T Cell-Mediated Immunity 9.
background_label: The Humoral Immune Response 10.
background_label: Dynamics of Adaptive Immunity 11.
other_label: The Mucosal Immune System Part V The Immune System in Health and Disease 12.
background_label: Failures of Host Defense Mechanism 13.
other_label: Allergy and Hypersensitivity 14.
other_label: Autoimmunity and Transplantation 15.
other_label: Manipulation of the Immune Response Part VI The Origins of Immune Responses 16.
other_label: Evolution of the Immune System Appendix I Immunologists' Toolbox Appendix II CD Antigens Appendix III Cytokines and their Receptors Appendix IV Chemokines and their Receptors Appendix V Immunological Constants

===================================
paper_id: 14179137; YEAR: 2010
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200
TITLE: An Unsupervised Snippet-Based Sentiment Classification Method for Chinese Unknown Phrases without Using Reference Word Pairs
ABSTRACT: background_label: This work presents an unsupervised snippet-based sentiment classification method for Chinese unknown sentiment phrases, which is also applicable to other languages theoretically.
method_label: Unlike existing Semantic Orientation (SO) methods, our proposed method does not require any Reference Word Pairs (RWPs) for predicting the sentiments of phrases.
result_label: The results of preliminary experiments show that our proposed method is highly effective and achieves over 80% accuracy and F-measures with relatively fewer queries.
result_label: An experiment of opinion extraction using a public Chinese UGC corpus also shows promising results.

===================================
paper_id: 15190020; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: Semantic Composition and Decomposition: From Recognition to Generation
ABSTRACT: background_label: Semantic composition is the task of understanding the meaning of text by composing the meanings of the individual words in the text.
background_label: Semantic decomposition is the task of understanding the meaning of an individual word by decomposing it into various aspects (factors, constituents, components) that are latent in the meaning of the word.
background_label: We take a distributional approach to semantics, in which a word is represented by a context vector.
background_label: Much recent work has considered the problem of recognizing compositions and decompositions, but we tackle the more difficult generation problem.
method_label: For simplicity, we focus on noun-modifier bigrams and noun unigrams.
method_label: A test for semantic composition is, given context vectors for the noun and modifier in a noun-modifier bigram ("red salmon"), generate a noun unigram that is synonymous with the given bigram ("sockeye").
background_label: A test for semantic decomposition is, given a context vector for a noun unigram ("snifter"), generate a noun-modifier bigram that is synonymous with the given unigram ("brandy glass").
background_label: With a vocabulary of about 73,000 unigrams from WordNet, there are 73,000 candidate unigram compositions for a bigram and 5,300,000,000 (73,000 squared) candidate bigram decompositions for a unigram.
method_label: We generate ranked lists of potential solutions in two passes.
method_label: A fast unsupervised learning algorithm generates an initial list of candidates and then a slower supervised learning algorithm refines the list.
result_label: We evaluate the candidate solutions by comparing them to WordNet synonym sets.
result_label: For decomposition (unigram to bigram), the top 100 most highly ranked bigrams include a WordNet synonym of the given unigram 50.7% of the time.
result_label: For composition (bigram to unigram), the top 100 most highly ranked unigrams include a WordNet synonym of the given bigram 77.8% of the time.

===================================
paper_id: 11711192; YEAR: 2004
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf
TITLE: Using language structure for adaptive multimodal language acquisition
ABSTRACT: background_label: In human spoken communication, language structure plays a vital role in providing a framework for humans to understand each other.
background_label: Using language rules, words are combined into meaningful sentences to represent knowledge.
background_label: Speech enabled systems based on pre-programmed Rule Grammar suffer from constraints on vocabulary and sentence structures.
objective_label: To address this problem, in this paper, we discuss a language acquisition system that is capable of learning new words and their corresponding semantic meaning by initiating an adaptive dialog with the user.
method_label: Thus, the vocabulary of the system can be increased in real time by the user.
method_label: The language acquisition system is provided knowledge about language structure and is capable of accepting multimodal user inputs that includes speech, touch, pen-tablet, mouse, and keyboard.
method_label: We discuss the efficiency of learning new concepts and the ease with which users can teach the system new concepts.
method_label: The multimodal language acquisition system is capable of acquiring, in real time, new words that pertain to objects, actions or attributes and their corresponding meanings.
method_label: The first step in this process is to detect unknown words in the spoken utterance.
result_label: Any new word that is detected is classified into one of the above mentioned categories.
objective_label: The second step is to learn from the user the meaning of the word and add it to the semantic database.
background_label: An unknown word is flagged whenever an utterance is not consistent with the pre-programmed Rule Grammar.
background_label: Because the system can acquire words pertaining to objects, actions or attributes, we are interested in words that are nouns, verbs or adjectives.
method_label: We use a transformation based part-of-speech tagger that is capable of annotating English words with their part-of-speech to identify words in the utterance that are nouns, verbs and adjectives.
method_label: These words are searched in the semantic database and unknown words are identified.
method_label: The system then initiates an adaptive dialog with the user, requesting the user to provide the meaning of the unknown word.
method_label: When the user has provided the relevant meaning using any of the input modalities, the system checks whether the meaning given corresponds to the category of the word, i.e.
method_label: if the unknown word is a noun then the user can associate only an object with it or if the unknown word is a verb then only an action can be associated with the word.
method_label: Thus, the system uses the knowledge of the occurrence of the word in the sentence to determine what kind of meaning can be associated with the word.
result_label: The language structure thus gives the system a basic knowledge of the unknown word.

===================================
paper_id: 5314099; YEAR: 2010
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200
TITLE: Finding Cognate Groups Using Phylogenies
ABSTRACT: background_label: AbstractA central problem in historical linguistics is the identification of historically related cognate words.
objective_label: We present a generative phylogenetic model for automatically inducing cognate group structure from unaligned word lists.
method_label: Our model represents the process of transformation and transmission from ancestor word to daughter word, as well as the alignment between the words lists of the observed languages.
method_label: We also present a novel method for simplifying complex weighted automata created during inference to counteract the otherwise exponential growth of message sizes.
method_label: On the task of identifying cognates in a dataset of Romance words, our model significantly outperforms a baseline approach, increasing accuracy by as much as 80%.
result_label: Finally, we demonstrate that our automatically induced groups can be used to successfully reconstruct ancestral words.

===================================
paper_id: 16661699; YEAR: 2009
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: title_tfidfcbow200
TITLE: Subjectivity Recognition on Word Senses via Semi-supervised Mincuts
ABSTRACT: background_label: We supplement WordNet entries with information on the subjectivity of its word senses.
background_label: Supervised classifiers that operate on word sense definitions in the same way that text classifiers operate on web or newspaper texts need large amounts of training data.
background_label: The resulting data sparseness problem is aggravated by the fact that dictionary definitions are very short.
objective_label: We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure.
result_label: The experimental results show that it outperforms supervised minimum cut as well as standard supervised, non-graph classification, reducing the error rate by 40%.
result_label: In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data.

===================================
paper_id: 7996820; YEAR: 2012
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200
TITLE: Word learning emerges from the interaction of online referent selection and slow associative learning.
ABSTRACT: background_label: Classic approaches to word learning emphasize referential ambiguity: In naming situations, a novel word could refer to many possible objects, properties, actions, and so forth.
background_label: To solve this, researchers have posited constraints, and inference strategies, but assume that determining the referent of a novel word is isomorphic to learning.
objective_label: We present an alternative in which referent selection is an online process and independent of long-term learning.
method_label: We illustrate this theoretical approach with a dynamic associative model in which referent selection emerges from real-time competition between referents and learning is associative (Hebbian).
method_label: This model accounts for a range of findings including the differences in expressive and receptive vocabulary, cross-situational learning under high degrees of ambiguity, accelerating (vocabulary explosion) and decelerating (power law) learning, fast mapping by mutual exclusivity (and differences in bilinguals), improvements in familiar word recognition with development, and correlations between speed of processing and learning.
result_label: Together it suggests that (a) association learning buttressed by dynamic competition can account for much of the literature; (b) familiar word recognition is subserved by the same processes that identify the referents of novel words (fast mapping); (c) online competition may allow the children to leverage information available in the task to augment performance despite slow learning; (d) in complex systems, associative learning is highly multifaceted; and (e) learning and referent selection, though logically distinct, can be subtly related.
result_label: It suggests more sophisticated ways of describing the interaction between situation- and developmental-time processes and points to the need for considering such interactions as a primary determinant of development.

===================================
paper_id: 14323869; YEAR: 1979
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200
TITLE: Considerations in applying clustering techniques to speaker-independent word recognition.
ABSTRACT: background_label: Recent work at Bell Laboratories has demonstrated the utility of applying sophisticated pattern recognition techniques to obtain a set of speaker-independent word templates for an isolated word recognition system [Levinson et al.,IEEE Trans.
background_label: Acoust.
background_label: Speech Signal Process.
other_label: ASSP-27 (2), 134--141 (1979); Rabiner et al., IEEE Trans.
other_label: Acoust.
method_label: Speech Signal Process.
background_label: (in press)].
method_label: In these studies, it was shown that a careful experimenter could guide the clustering algorithms to choose a small set of templates that were representative of a large number of replications for each word in the vocabulary.
result_label: Subsequent word recognition tests verified that the templates chosen were indeed representative of a fairly large population of talkers.
objective_label: Given the success of this approach, the next important step is to investigate fully automatic techniques for clustering multiple versions of a single word into a set of speaker-independent word templates.
method_label: Two such techniques are described in this paper.
method_label: The first method uses distance data (between replications of a word) to segment the population into stable clusters.
method_label: The word template is obtained as either the cluster minimax, or as an averaged version of all the elements in the cluster.
other_label: The second method is a variation of the one described by Rabiner [IEEE Trans.
other_label: Acoust.
method_label: Speech Signal Process.
method_label: ASSP-26 (3), 34--42 (1978)] in which averaging techniques are directly combined with the nearest neighbor rule to simultaneously define both the word template (i.e., the cluster center) and the elements in the cluster.
result_label: Experimental data show the first method to be superior to the second method when three or more clusters per word are used in the recognition task.

===================================
paper_id: 7404747; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Discovery of Brainwide Neural-Behavioral Maps via Multiscale Unsupervised Structure Learning
ABSTRACT: background_label: A single nervous system can generate many distinct motor patterns.
background_label: Identifying which neurons and circuits control which behaviors has been a laborious piecemeal process, usually for one observer-defined behavior at a time.
objective_label: We present a fundamentally different approach to neuron-behavior mapping.
method_label: We optogenetically activated 1054 identified neuron lines in Drosophila larvae and tracked the behavioral responses from 37,780 animals.
method_label: Application of multiscale unsupervised structure learning methods to the behavioral data enabled us to identify 29 discrete, statistically distinguishable, observer-unbiased behavioral phenotypes.
method_label: Mapping the neural lines to the behavior(s) they evoke provides a behavioral reference atlas for neuron subsets covering a large fraction of larval neurons.
result_label: This atlas is a starting point for connectivity- and activity-mapping studies to further investigate the mechanisms by which neurons mediate diverse behaviors.

===================================
paper_id: 15762346; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: The Forest Convolutional Network: Compositional Distributional Semantics with a Neural Chart and without Binarization
ABSTRACT: background_label: According to the principle of compositionality, the meaning of a sentence is computed from the meaning of its parts and the way they are syntactically combined.
background_label: In practice, however, the syntactic structure is computed by automatic parsers which are far-from-perfect and not tuned to the specifics of the task.
method_label: Current recursive neural network (RNN) approaches for computing sentence meaning therefore run into a number of practical difficulties, including the need to carefully select a parser appropriate for the task, deciding how and to what extent syntactic context modifies the semantic composition function, as well as on how to transform parse trees to conform to the branching settings (typically, binary branching) of the RNN.
method_label: This paper introduces a new model, the Forest Convolutional Network, that avoids all of these challenges, by taking a parse forest as input, rather than a single tree, and by allowing arbitrary branching factors.
result_label: We report improvements over the state-of-the-art in sentiment analysis and question classification.

===================================
paper_id: 3733062; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Inferencing Based on Unsupervised Learning of Disentangled Representations
ABSTRACT: background_label: Combining Generative Adversarial Networks (GANs) with encoders that learn to encode data points has shown promising results in learning data representations in an unsupervised way.
objective_label: We propose a framework that combines an encoder and a generator to learn disentangled representations which encode meaningful information about the data distribution without the need for any labels.
method_label: While current approaches focus mostly on the generative aspects of GANs, our framework can be used to perform inference on both real and generated data points.
result_label: Experiments on several data sets show that the encoder learns interpretable, disentangled representations which encode descriptive properties and can be used to sample images that exhibit specific characteristics.

===================================
paper_id: 12402693; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Detection of Slang Words in e-Data using semi-Supervised Learning
ABSTRACT: objective_label: The proposed algorithmic approach deals with finding the sense of a word in an electronic data.
background_label: Now a day,in different communication mediums like internet, mobile services etc.
background_label: people use few words, which are slang in nature.
method_label: This approach detects those abusive words using supervised learning procedure.
method_label: But in the real life scenario, the slang words are not used in complete word forms always.
method_label: Most of the times, those words are used in different abbreviated forms like sounds alike forms, taboo morphemes etc.
method_label: This proposed approach can detect those abbreviated forms also using semi supervised learning procedure.
result_label: Using the synset and concept analysis of the text, the probability of a suspicious word to be a slang word is also evaluated.

===================================
paper_id: 7363293; YEAR: 1998
adju relevance: Irrelevant (0)
difference: 1; annotator4: 0; annotator3: 1
sources: title_tfidf - specter
TITLE: Morphemes as Necessary Concept for Structures Discovery from Untagged Corpora
ABSTRACT: objective_label: This paper describes an overview of a method which allows discovery of syntactic structures from untagged corpora.
method_label: It is composed of three main steps: the discovery of the grammatical morphemes of the language.
method_label: Then the construction of the chunks which are a multilingual conceptual level allowing the bypass of the limping notion of words.
method_label: And Finally the discovery of the relations between chunks.
method_label: We give an overview of the different procedures realized and we especially describe the discovery of morphemes.
method_label: This operation is divided into three steps: the discovery of the most frequent morphemes of the language.
method_label: Then the discovery of the other morphemes, and finally the segmentation of the words of the corpus.
method_label: We concluded with the procedure of correction which required the chunk level.
result_label: The concepts and algorithms were tested on a twenty natural languages like English, German, Turkish, Vietnamese, Swahili, Finnish, Latin, Indonesian.

===================================
paper_id: 189998801; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Improving Unsupervised Subword Modeling via Disentangled Speech Representation Learning and Transformation
ABSTRACT: background_label: This study tackles unsupervised subword modeling in the zero-resource scenario, learning frame-level speech representation that is phonetically discriminative and speaker-invariant, using only untranscribed speech for target languages.
background_label: Frame label acquisition is an essential step in solving this problem.
background_label: High quality frame labels should be in good consistency with golden transcriptions and robust to speaker variation.
method_label: We propose to improve frame label acquisition in our previously adopted deep neural network-bottleneck feature (DNN-BNF) architecture by applying the factorized hierarchical variational autoencoder (FHVAE).
method_label: FHVAEs learn to disentangle linguistic content and speaker identity information encoded in speech.
method_label: By discarding or unifying speaker information, speaker-invariant features are learned and fed as inputs to DPGMM frame clustering and DNN-BNF training.
result_label: Experiments conducted on ZeroSpeech 2017 show that our proposed approaches achieve $2.4\%$ and $0.6\%$ absolute ABX error rate reductions in across- and within-speaker conditions, comparing to the baseline DNN-BNF system without applying FHVAEs.
result_label: Our proposed approaches significantly outperform vocal tract length normalization in improving frame labeling and subword modeling.

===================================
paper_id: 11346875; YEAR: 2012
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: A Coarse-to-Fine Approach for Handwritten Word Spotting in Large Scale Historical Documents Collection
ABSTRACT: background_label: In this paper we propose an approach for word spotting in handwritten document images.
background_label: We state the problem from a focused retrieval perspective, i.e.
objective_label: locating instances of a query word in a large scale dataset of digitized manuscripts.
method_label: We combine two approaches, namely one based on word segmentation and another one segmentation-free.
method_label: The first approach uses a hashing strategy to coarsely prune word images that are unlikely to be instances of the query word.
background_label: This process is fast but has a low precision due to the errors introduced in the segmentation step.
background_label: The regions containing candidate words are sent to the second process based on a state of the art technique from the visual object detection field.
method_label: This discriminative model represents the appearance of the query word and computes a similarity score.
method_label: In this way we propose a coarse-to-fine approach achieving a compromise between efficiency and accuracy.
method_label: The validation of the model is shown using a collection of old handwritten manuscripts.
result_label: We appreciate a substantial improvement in terms of precision regarding the previous proposed method with a low computational cost increase.

===================================
paper_id: 17389364; YEAR: 2010
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: The Development of Polysemy and Frequency Use in English Second Language Speakers.
ABSTRACT: background_label: Spoken language data were collected from six adult second language (L2) English learners over a year-long period in order to explore the development of word polysemy and frequency use.
background_label: The data were analyzed both quantitatively and qualitatively.
method_label: In the first analysis, the growth of WordNet polysemy values and CELEX word frequency values were examined.
method_label: For both indexes, significant growth was demonstrated from the 2nd to the 16th week of observation, after which values remained stable.
method_label: Growth in word polysemy values also correlated with changes in word frequency, supporting the notion that frequency and polysemy effects in word use are related.
method_label: A second analysis used the WordNet dictionary to explore qualitative changes in word sense use concerning six frequent lexical items in the learner corpus (think, know, place, work, play, and name).
method_label: A qualitative analysis compared normalized frequencies for each word sense in the first trimester of the study to the later trimesters.
result_label: Differences in the number of word senses used across trimesters were found for all six words.
result_label: Analyses 1 and 2, taken together, support the notion that L2 learners begin to use words that have the potential for more senses during the first 4 months; learners then begin to extend the core meanings of these polysemous words.
result_label: These findings provide further insights into the development of lexical proficiency in L2 learners and the growth of lexical networks.

===================================
paper_id: 9191188; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: Meta-Unsupervised-Learning: A supervised approach to unsupervised learning
ABSTRACT: background_label: We introduce a new paradigm to investigate unsupervised learning, reducing unsupervised learning to supervised learning.
method_label: Specifically, we mitigate the subjectivity in unsupervised decision-making by leveraging knowledge acquired from prior, possibly heterogeneous, supervised learning tasks.
method_label: We demonstrate the versatility of our framework via comprehensive expositions and detailed experiments on several unsupervised problems such as (a) clustering, (b) outlier detection, and (c) similarity prediction under a common umbrella of meta-unsupervised-learning.
result_label: We also provide rigorous PAC-agnostic bounds to establish the theoretical foundations of our framework, and show that our framing of meta-clustering circumvents Kleinberg's impossibility theorem for clustering.

===================================
paper_id: 63244878; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: Morphological, syntactic and diacritics rules for automatic diacritization of Arabic sentences
ABSTRACT: background_label: The diacritical marks of Arabic language are characters other than letters and are in the majority of cases absent from Arab writings.
objective_label: This paper presents a hybrid system for automatic diacritization of Arabic sentences combining linguistic rules and statistical treatments.
method_label: The used approach is based on four stages.
method_label: The first phase consists of a morphological analysis using the second version of the morphological analyzer Alkhalil Morpho Sys.
method_label: Morphosyntactic outputs from this step are used in the second phase to eliminate invalid word transitions according to the syntactic rules.
method_label: Then, the system used in the third stage is a discrete hidden Markov model and Viterbi algorithm to determine the most probable diacritized sentence.
method_label: The unseen transitions in the training corpus are processed using smoothing techniques.
method_label: Finally, the last step deals with words not analyzed by Alkhalil analyzer, for which we use statistical treatments based on the letters.
result_label: The word error rate of our system is around 2.58% if we ignore the diacritic of the last letter of the word and around 6.28% when this diacritic is taken into account.

===================================
paper_id: 4553465; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200
TITLE: Completely Unsupervised Phoneme Recognition by Adversarially Learning Mapping Relationships from Audio Embeddings
ABSTRACT: background_label: Unsupervised discovery of acoustic tokens from audio corpora without annotation and learning vector representations for these tokens have been widely studied.
background_label: Although these techniques have been shown successful in some applications such as query-by-example Spoken Term Detection (STD), the lack of mapping relationships between these discovered tokens and real phonemes have limited the down-stream applications.
objective_label: This paper represents probably the first attempt towards the goal of completely unsupervised phoneme recognition, or mapping audio signals to phoneme sequences without phoneme-labeled audio data.
method_label: The basic idea is to cluster the embedded acoustic tokens and learn the mapping between the cluster sequences and the unknown phoneme sequences with a Generative Adversarial Network (GAN).
result_label: An unsupervised phoneme recognition accuracy of 36% was achieved in the preliminary experiments.

===================================
paper_id: 3331747; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200
TITLE: Visual Localisation and Individual Identification of Holstein Friesian Cattle via Deep Learning
ABSTRACT: background_label: In this paper, we demonstrate that computer vision pipelines utilising deep neural architectures are well-suited to perform automated Holstein Friesian cattle detection as well as individual identification in agriculturally relevant setups.
objective_label: To the best of our knowledge, this work is the first to apply deep learning to the task of automated visual bovine identification.
method_label: We show that off-the-shelf networks can perform end-to-end identification of individuals in top-down still imagery acquired from fixed cameras.
method_label: We then introduce a video processing pipeline composed of standard components to efficiently process dynamic herd footage filmed by Unmanned Aerial Vehicles (UAVs).
result_label: We report on these setups, as well as the context, training and evaluation of their components.
background_label: We publish alongside new datasets: FriesianCattle2017 of in-barn top-down imagery, and AerialCattle2017 of outdoor cattle footage filmed by a DJI Inspire MkI UAV.
background_label: We show that Friesian cattle detection and localisation can be performed robustly with an accuracy of 99.3% on this data.
method_label: We evaluate individual identification exploiting coat uniqueness on 940 RGB stills taken after milking in-barn (89 individuals, accuracy = 86.1%).
result_label: We also evaluate identification via a video processing pipeline on 46,430 frames originating from 34 clips (approx.
method_label: 20 s length each) of UAV footage taken during grazing (23 individuals, accuracy = 98.1%).
result_label: These tests suggest that, particularly when videoing small herds in uncluttered environments, an application of marker-less Friesian cattle identification is not only feasible using standard deep learning components - it appears robust enough to assist existing tagging methods.

===================================
paper_id: 21106538; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Hybrid Part of Speech tagger for Sinhala Language
ABSTRACT: background_label: This research presents a hybrid Part of Speech tagging approach which utilizes both rule based and stochastic tagging approaches for Sinhala Language.
method_label: In the first phase, Hidden Markov Model based stochastic tagger is constructed which is based on bi-gram probabilities.
method_label: A stemmer is used in the tagging process to enhance the accuracy of the tagger.
method_label: An experiment on three POS tag set versions is carried out to come up with the best tag set which leads towards a meaningful and precise tagging process for Sinhala Language.
method_label: Since Sinhala is a morphologically rich language, rules based on morphological features are used to predict the relevant tag for words which do not present in the training set.
method_label: Further, an experiment is carried out to find out whether the implemented hybrid tagger can be used to enhance the size of the data set.
result_label: The implemented hybrid tagger is successful in achieving an overall accuracy of 72% when the average unknown word percentage is 20%.

===================================
paper_id: 329483; YEAR: 1995
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200
TITLE: Unsupervised Learning Of Disambiguation Rules For Part Of Speech Tagging
ABSTRACT: background_label: In this paper we describe an unsupervised learning algorithm for automatically training a rule-based part of speech tagger without using a manually tagged corpus.
method_label: We compare this algorithm to the Baum-Welch algorithm, used for unsupervised training of stochastic taggers.
method_label: Next, we show a method for combining unsupervised and supervised rule-based training algorithms to create a highly accurate tagger using only a small amount of manually tagged text1.

===================================
paper_id: 5354393; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200
TITLE: Context-Based Chinese Word Segmentation using SVM Machine-Learning Algorithm without Dictionary Support
ABSTRACT: background_label: AbstractThis paper presents a new machine-learning Chinese word segmentation (CWS) approach, which defines CWS as a break-point classification problem; the break point is the boundary of two subsequent words.
method_label: Further, this paper exploits a support vector machine (SVM) classifier, which learns the segmentation rules of the Chinese language from a context model of break points in a corpus.
method_label: Additionally, we have designed an effective feature set for building the context model, and a systematic approach for creating the positive and negative samples used for training the classifier.
method_label: Unlike the traditional approach, which requires the assistance of large-scale known information sources such as dictionaries or linguistic tagging, the proposed approach selects the most frequent words in the corpus as the learning sources.
method_label: In this way, CWS is able to execute in any novel corpus without proper assistance sources.
result_label: According to our experimental results, the proposed approach can achieve a competitive result compared with the Chinese knowledge and information processing (CKIP) system from Academia Sinica.

===================================
paper_id: 14252086; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200
TITLE: Towards Unsupervised and Language-independent Compound Splitting using Inflectional Morphological Transformations
ABSTRACT: objective_label: AbstractIn this paper, we address the task of languageindependent, knowledge-lean and unsupervised compound splitting, which is an essential component for many natural language processing tasks such as machine translation.
background_label: Previous methods on statistical compound splitting either include language-specific knowledge (e.g., linking elements) or rely on parallel data, which results in limited applicability.
objective_label: We aim to overcome these limitations by learning compounding morphology from inflectional information derived from lemmatized monolingual corpora.
result_label: In experiments for Germanic languages, we show that our approach significantly outperforms language-dependent stateof-the-art methods in finding the correct split point and that word inflection is a good approximation for compounding morphology.

===================================
paper_id: 67143344; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: Text Classification Research with Attention-based Recurrent Neural Networks
ABSTRACT: background_label: Text classification is one of the principal tasks of machine learning.
objective_label: It aims to design proper algorithms to enable computers to extract features and classify texts automatically.
method_label: In the past, this has been mainly based on the classification of keywords and neural network semantic synthesis classification.
method_label: The former emphasizes the role of keywords, while the latter focuses on the combination of words between roles.
method_label: The method proposed in this paper considers the advantages of both methods.
method_label: It uses an attention mechanism to learn weighting for each word.
background_label: Under the setting, key words will have a higher weight, and common words will have lower weight.
background_label: Therefore, the representation of texts not only considers all words, but also pays more attention to key words.
method_label: Then we feed the feature vector to a softmax classifier.
method_label: At last, we conduct experiments on two news classification datasets published by NLPCC2014 and Reuters, respectively.
result_label: The proposed model achieves F-values by 88.5% and 51.8% on the two datasets.
result_label: The experimental results show that our method outperforms all the traditional baseline systems.

===================================
paper_id: 35957621; YEAR: 2007
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf
TITLE: Preview benefit and parafoveal-on-foveal effects from word n + 2.
ABSTRACT: background_label: Using the gaze-contingent boundary paradigm with the boundary placed after word n, the experiment manipulated preview of word n + 2 for fixations on word n. There was no preview benefit for 1st-pass reading on word n + 2, replicating the results of K. Rayner, B. J. Juhasz, and S. J.
result_label: Brown (2007), but there was a preview benefit on the 3-letter word n + 1, that is, after the boundary but before word n + 2.
result_label: Additionally, both word n + 1 and word n + 2 exhibited parafoveal-on-foveal effects on word n. Thus, during a fixation on word n and given a short word n + 1, some information is extracted from word n + 2, supporting the hypothesis of distributed processing in the perceptual span.

===================================
paper_id: 64163481; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: Unsupervised Learning Algorithms
ABSTRACT: background_label: This book summarizes the state-of-the-art in unsupervised learning.
background_label: The contributors discuss how withthe proliferation of massive amounts of unlabeled data, unsupervised learning algorithms, which can automatically discover interesting and useful patterns in such data, have gained popularity among researchers and practitioners.
background_label: The authors outline how these algorithms have found numerous applications including pattern recognition, market basket analysis, web mining, social network analysis, information retrieval, recommender systems, market research, intrusion detection, and fraud detection.
method_label: They present how the difficulty of developing theoretically sound approaches that are amenable to objective evaluation have resulted in the proposal of numerous unsupervised learning algorithms over the past half-century.
method_label: The intended audience includes researchers and practitioners who are increasingly using unsupervised learning algorithms to analyze their data.
method_label: Topics of interest include anomaly detection, clustering, feature extraction, and applications of unsupervised learning.
result_label: Each chapter is contributed by a leading expert in the field.

===================================
paper_id: 8977153; YEAR: 2006
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf
TITLE: Unsupervised Part-Of-Speech Tagging Employing Efficient Graph Clustering
ABSTRACT: background_label: An unsupervised part-of-speech (POS) tagging system that relies on graph clustering methods is described.
background_label: Unlike in current state-of-the-art approaches, the kind and number of different tags is generated by the method itself.
method_label: We compute and merge two partitionings of word graphs: one based on context similarity of high frequency words, another on log-likelihood statistics for words of lower frequencies.
method_label: Using the resulting word clusters as a lexicon, a Viterbi POS tagger is trained, which is refined by a morphological component.
result_label: The approach is evaluated on three different languages by measuring agreement with existing taggers.

===================================
paper_id: 17779450; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: Point morphology
ABSTRACT: background_label: We introduce a complete morphological analysis framework for 3D point clouds.
method_label: Starting from an unorganized point set sampling a surface, we propose morphological operators in the form of projections, allowing to sample erosions, dilations, closings and openings of an object without any explicit mesh structure.
method_label: Our framework supports structuring elements with arbitrary shape, accounts robustly for geometric and morphological sharp features, remains efficient at large scales and comes together with a specific adaptive sampler.
method_label: Based on this meshless framework, we propose applications which benefit from the non-linear nature of morphological analysis and can be expressed as simple sequences of our operators, including medial axis sampling, hysteresis shape filtering and geometry-preserving topological simplification.

===================================
paper_id: 16170186; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: Unveiling the relationship between complex networks metrics and word senses
ABSTRACT: background_label: The automatic disambiguation of word senses (i.e., the identification of which of the meanings is used in a given context for a word that has multiple meanings) is essential for such applications as machine translation and information retrieval, and represents a key step for developing the so-called Semantic Web.
background_label: Humans disambiguate words in a straightforward fashion, but this does not apply to computers.
objective_label: In this paper we address the problem of Word Sense Disambiguation (WSD) by treating texts as complex networks, and show that word senses can be distinguished upon characterizing the local structure around ambiguous words.
objective_label: Our goal was not to obtain the best possible disambiguation system, but we nevertheless found that in half of the cases our approach outperforms traditional shallow methods.
method_label: We show that the hierarchical connectivity and clustering of words are usually the most relevant features for WSD.
method_label: The results reported here shine light on the relationship between semantic and structural parameters of complex networks.
result_label: They also indicate that when combined with traditional techniques the complex network approach may be useful to enhance the discrimination of senses in large texts

===================================
paper_id: 60329865; YEAR: 1992
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: title_tfidf - abs_tfidf - specter
TITLE: Morphology and Computation
ABSTRACT: background_label: Part 1 Applications of computational morphology: natural language applications speech applications word processing document retrieval.
background_label: Part 2 The nature of morphology: functions of morphology what is combined, and how?
background_label: morphemes, the structure of words, and word-formation rules morphotactics - the order of morphemes phonology psycholinguistic evidence.
method_label: Part 3 Computational morphology: computational mechanisms an overview of URKIMMO augments to the KIMMO approach the computational complexity of two-level morphology other ways of doing computational morphology a prospectus - what is left to do.
result_label: Part 4 Some peripheral issues: morphological acquisition compound nominals and related constructions.

===================================
paper_id: 6081625; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: Corpus specificity in LSA and Word2vec: the role of out-of-domain documents
ABSTRACT: background_label: Latent Semantic Analysis (LSA) and Word2vec are some of the most widely used word embeddings.
background_label: Despite the popularity of these techniques, the precise mechanisms by which they acquire new semantic relations between words remain unclear.
objective_label: In the present article we investigate whether LSA and Word2vec capacity to identify relevant semantic dimensions increases with size of corpus.
method_label: One intuitive hypothesis is that the capacity to identify relevant dimensions should increase as the amount of data increases.
result_label: However, if corpus size grow in topics which are not specific to the domain of interest, signal to noise ratio may weaken.
background_label: Here we set to examine and distinguish these alternative hypothesis.
method_label: To investigate the effect of corpus specificity and size in word-embeddings we study two ways for progressive elimination of documents: the elimination of random documents vs. the elimination of documents unrelated to a specific task.
method_label: We show that Word2vec can take advantage of all the documents, obtaining its best performance when it is trained with the whole corpus.
method_label: On the contrary, the specialization (removal of out-of-domain documents) of the training corpus, accompanied by a decrease of dimensionality, can increase LSA word-representation quality while speeding up the processing time.
result_label: Furthermore, we show that the specialization without the decrease in LSA dimensionality can produce a strong performance reduction in specific tasks.
result_label: From a cognitive-modeling point of view, we point out that LSA's word-knowledge acquisitions may not be efficiently exploiting higher-order co-occurrences and global relations, whereas Word2vec does.

===================================
paper_id: 9427281; YEAR: 2010
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200
TITLE: Towards weakly supervised semantic segmentation by means of multiple instance and multitask learning
ABSTRACT: objective_label: We address the task of learning a semantic segmentation from weakly supervised data.
objective_label: Our aim is to devise a system that predicts an object label for each pixel by making use of only image level labels during training – the information whether a certain object is present or not in the image.
background_label: Such coarse tagging of images is faster and easier to obtain as opposed to the tedious task of pixelwise labeling required in state of the art systems.
method_label: We cast this task naturally as a multiple instance learning (MIL) problem.
method_label: We use Semantic Texton Forest (STF) as the basic framework and extend it for the MIL setting.
method_label: We make use of multitask learning (MTL) to regularize our solution.
method_label: Here, an external task of geometric context estimation is used to improve on the task of semantic segmentation.
result_label: We report experimental results on the MSRC21 and the very challenging VOC2007 datasets.
result_label: On MSRC21 dataset we are able, by using 276 weakly labeled images, to achieve the performance of a supervised STF trained on pixelwise labeled training set of 56 images, which is a significant reduction in supervision needed.

===================================
paper_id: 146534647; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: Use of bound morphemes (noun particles) in word segmentation by Japanese-learning infants
ABSTRACT: background_label: Abstract Recent studies have shown that English-, French-, and German-learning infants begin to use determiners to segment adjacent nouns before their first birthday.
objective_label: The present research extended the investigation to a typologically different language, Japanese, focusing on infants’ use of a high-frequency particle ga, a subject-marker.
background_label: In Japanese, a particle follows, rather than precedes, the noun, and is usually followed by a predicate verb; thus particles rarely occur at utterance edges.
method_label: Furthermore, a particle is not a free morpheme (like a determiner) but a bound morpheme.
method_label: Although particles are frequently omitted in colloquial speech, the frequency of their occurrence is comparable to that of high-frequency determiners in a previously studied language, French.
result_label: The results demonstrated that Japanese-learning infants used particles for word segmentation not at 10 and 12 months but at 15 months, which is later than the age at which infants begin to use determiners in previously studied languages.
result_label: The reason for this delay was discussed in light of the properties of Japanese particles.

===================================
paper_id: 43927675; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: How much does a word weigh? Weighting word embeddings for word sense induction
ABSTRACT: background_label: The paper describes our participation in the first shared task on word sense induction and disambiguation for the Russian language RUSSE'2018 (Panchenko et al., 2018).
method_label: For each of several dozens of ambiguous words, the participants were asked to group text fragments containing it according to the senses of this word, which were not provided beforehand, therefore the"induction"part of the task.
method_label: For instance, a word"bank"and a set of text fragments (also known as"contexts") in which this word occurs, e.g.
method_label: "bank is a financial institution that accepts deposits"and"river bank is a slope beside a body of water"were given.
method_label: A participant was asked to cluster such contexts in the unknown in advance number of clusters corresponding to, in this case, the"company"and the"area"senses of the word"bank".
method_label: The organizers proposed three evaluation datasets of varying complexity and text genres based respectively on texts of Wikipedia, Web pages, and a dictionary of the Russian language.
method_label: We present two experiments: a positive and a negative one, based respectively on clustering of contexts represented as a weighted average of word embeddings and on machine translation using two state-of-the-art production neural machine translation systems.
method_label: Our team showed the second best result on two datasets and the third best result on the remaining one dataset among 18 participating teams.
result_label: We managed to substantially outperform competitive state-of-the-art baselines from the previous years based on sense embeddings.

===================================
paper_id: 1392319; YEAR: 2004
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Rapid Processing and Unsupervised Learning in a Model of the Cortical Macrocolumn
ABSTRACT: background_label: We study a model of the cortical macrocolumn consisting of a collection of inhibitorily coupled minicolumns.
background_label: The proposed system overcomes several severe deficits of systems based on single neurons as cerebral functional units, notably limited robustness to damage and unrealistically large computation time.
method_label: Motivated by neuroanatomical and neurophysiological findings, the utilized dynamics is based on a simple model of a spiking neuron with refractory period, fixed random excitatory interconnection within minicolumns, and instantaneous inhibition within one macrocolumn.
method_label: A stability analysis of the system's dynamical equations shows that minicolumns can act as monolithic functional units for purposes of critical, fast decisions and learning.
result_label: Oscillating inhibition (in the gamma frequency range) leads to a phase-coupled population rate code and high sensitivity to small imbalances in minicolumn inputs.
method_label: Minicolumns are shown to be able to organize their collective inputs without supervision by Hebbian plasticity into selective receptive field shapes, thereby becoming classifiers for input patterns.
result_label: Using the bars test, we critically compare our system's performance with that of others and demonstrate its ability for distributed neural coding.

===================================
paper_id: 3333630; YEAR: 1995
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf
TITLE: Segmentation-free word recognition with application to Arabic
ABSTRACT: background_label: This paper describes the design and implementation of a system that recognizes machine-printed Arabic words without prior segmentation.
method_label: The technique is based on describing symbols in terms of shape primitives.
method_label: At recognition time, the primitives are detected on a word image using mathematical morphology operations.
method_label: The system then matches the detected primitives with symbol models.
method_label: This leads to a spatial arrangement of matched symbol models.
method_label: The system conducts a search in the space of spatial arrangements of models and outputs the arrangement with the highest posterior probability as the recognition of the word.
method_label: The advantage of using this whole word approach versus a segmentation approach is that the result of recognition is optimized with regard to the whole word.
result_label: Results of preliminary experiments using a lexicon of 42,000 words show a recognition rate of 99.4% for noise-free text and 73% for scanned text.

===================================
paper_id: 60441247; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200
TITLE: RespNet: A deep learning model for extraction of respiration from photoplethysmogram
ABSTRACT: background_label: Respiratory ailments afflict a wide range of people and manifests itself through conditions like asthma and sleep apnea.
background_label: Continuous monitoring of chronic respiratory ailments is seldom used outside the intensive care ward due to the large size and cost of the monitoring system.
background_label: While Electrocardiogram (ECG) based respiration extraction is a validated approach, its adoption is limited by access to a suitable continuous ECG monitor.
background_label: Recently, due to the widespread adoption of wearable smartwatches with in-built Photoplethysmogram (PPG) sensor, it is being considered as a viable candidate for continuous and unobtrusive respiration monitoring.
background_label: Research in this domain, however, has been predominantly focussed on estimating respiration rate from PPG.
method_label: In this work, a novel end-to-end deep learning network called RespNet is proposed to perform the task of extracting the respiration signal from a given input PPG as opposed to extracting respiration rate.
background_label: The proposed network was trained and tested on two different datasets utilizing different modalities of reference respiration signal recordings.
method_label: Also, the similarity and performance of the proposed network against two conventional signal processing approaches for extracting respiration signal were studied.
method_label: The proposed method was tested on two independent datasets with a Mean Squared Error of 0.262 and 0.145.
method_label: The Cross-Correlation coefficient of the respective datasets were found to be 0.933 and 0.931.
result_label: The reported errors and similarity was found to be better than conventional approaches.
result_label: The proposed approach would aid clinicians to provide comprehensive evaluation of sleep-related respiratory conditions and chronic respiratory ailments while being comfortable and inexpensive for the patient.

===================================
paper_id: 663225; YEAR: 2007
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: An efficient algorithm for building a distributional thesaurus (and other Sketch Engine developments)
ABSTRACT: background_label: Gorman and Curran (2006) argue that thesaurus generation for billion+-word corpora is problematic as the full computation takes many days.
background_label: We present an algorithm with which the computation takes under two hours.
method_label: We have created, and made publicly available, thesauruses based on large corpora for (at time of writing) seven major world languages.
method_label: The development is implemented in the Sketch Engine (Kilgarriff et al., 2004).
method_label: Another innovative development in the same tool is the presentation of the grammatical behaviour of a word against the background of how all other words of the same word class behave.
result_label: Thus, the English noun constraint occurs 75% in the plural.
other_label: Is this a salient lexical fact?
method_label: To form a judgement, we need to know the distribution for all nouns.
result_label: We use histograms to present the distribution in a way that is easy to grasp.

===================================
paper_id: 54436148; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: Comparing Neural- and N-Gram-Based Language Models for Word Segmentation
ABSTRACT: background_label: Word segmentation is the task of inserting or deleting word boundary characters in order to separate character sequences that correspond to words in some language.
objective_label: In this article we propose an approach based on a beam search algorithm and a language model working at the byte/character level, the latter component implemented either as an n-gram model or a recurrent neural network.
method_label: The resulting system analyzes the text input with no word boundaries one token at a time, which can be a character or a byte, and uses the information gathered by the language model to determine if a boundary must be placed in the current position or not.
objective_label: Our aim is to use this system in a preprocessing step for a microtext normalization system.
method_label: This means that it needs to effectively cope with the data sparsity present on this kind of texts.
method_label: We also strove to surpass the performance of two readily available word segmentation systems: The well-known and accessible Word Breaker by Microsoft, and the Python module WordSegment by Grant Jenks.
result_label: The results show that we have met our objectives, and we hope to continue to improve both the precision and the efficiency of our system in the future.

===================================
paper_id: 14777455; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: Lexicon reduction for printed Farsi subwords using pictorial and textual dictionaries
ABSTRACT: method_label: In this paper, we present a method to reduce the lexicon size of printed Farsi subwords, which utilizes the holistic shape along with the key character information to dynamically reduce the size of lexicon organized accordingly in two approaches: (1) based on the global shape description (to build a pictorial dictionary) and (2) based on constitutive character information (to build a textual dictionary).
method_label: Given an input word image, the reduction procedure is accomplished in two successive stages.
method_label: First, characteristic loci features are extracted and compared with the pictorial dictionary to select the candidate subwords based on their shapes similarity.
method_label: The lexicon is further reduced in the second stage by determining the key character in the input image and comparing it with the textual dictionary.
method_label: The key characters are defined as the ones which can be segmented and recognized rather easily and also, together with global descriptors, characterize the word image efficiently.
method_label: A method for optimal selection of key characters is also proposed which is based on the mutual information of pictorial and textual dictionaries.
method_label: The final candidate subwords are those sharing the same key character with the input image.
result_label: The performance of the proposed method was studied experimentally on a set of 5,000 subword samples.
result_label: The results obtained show a reduction rate of 97.83 % on a lexicon of 6,900 printed Farsi subwords.

===================================
paper_id: 23375227; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Automatic Proposition Extraction from Dependency Trees: Helping Early Prediction of Alzheimer's Disease from Narratives
ABSTRACT: background_label: Idea Density (ID) was originally proposed as a way of measuring the memory load of narratives, by representing the underlying content of the text as a series of semantic units, called propositions or ideas.
background_label: From a clinical perspective, this notion has been shown to correlate with several cognitive aspects, such as memory, readability, aging, and dementia onset and progress.
background_label: Traditionally, propositions are extracted manually from texts.
method_label: There is a tool that can automate ID extraction [1], but it uses shallow information as input, and doesn't produce the propositions themselves as output.
method_label: We propose a novel approach to obtaining the ID automatically from a text.
method_label: Our method is an automation of Chand et al.
method_label: 's ID manual [2], and consists of a rule-based system acting upon dependency trees.
method_label: Initially, for each sentence in a text, a dependency parser is used to elicit the dependency relations between words.
method_label: Then, a set of rules is recursively applied in order to process these relations to yield the corresponding propositions.
result_label: We analyze preliminary results of our system using a well-formed journalistic text, and speech transcriptions of dementia patients.

===================================
paper_id: 37344085; YEAR: 1986
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf
TITLE: A segmental k-means training procedure for connected word recognition
ABSTRACT: background_label: Algorithms for recognizing strings of connected words from whole-word patterns have become highly efficient and accurate, although computation rates remain high.
background_label: Even the most ambitious connected-word recognition task is practical with today's integrated circuit technology, but extracting reliable, robust whole-word reference patterns still is difficult.
background_label: In the past, connected-word recognizers relied on isolated-word reference patterns or patterns derived from a limited context (e.g., the middle digit from strings of three digits).
background_label: These whole-word patterns were adequate for slow rates of articulated speech, but not for strings of words spoken at high rates (e.g., about 200 to 300 words per minute).
method_label: To alleviate this difficulty, a segmental k-means training procedure was used to extract whole-word patterns from naturally spoken word strings.
method_label: The segmented words are then used to create a set of word reference patterns for recognition.
result_label: Recognition string accuracies were 98 to 99 percent for digits in variable length strings and 90 to 98 percent for sentences from an airline reservation task.
result_label: These performance scores represent significant improvements over previous connected-word recognizers.

===================================
paper_id: 642450; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Unsupervised Learning of Prototypical Fillers for Implicit Semantic Role Labeling
ABSTRACT: background_label: Gold annotations for supervised implicit semantic role labeling are extremely sparse and costly.
method_label: As a lightweight alternative, this paper describes an approach based on unsupervised parsing which can do without iSRL-specific training data: We induce prototypical roles from large amounts of explicit SRL annotations paired with their distributed word representations.
result_label: An evaluation shows competitive performance with supervised methods on the SemEval 2010 data, and our method can easily be applied to predicates (or languages) for which no training annotations are available.

===================================
paper_id: 17272416; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200
TITLE: Syntax-based Simultaneous Translation through Prediction of Unseen Syntactic Constituents
ABSTRACT: background_label: Simultaneous translation is a method to reduce the latency of communication through machine translation (MT) by dividing the input into short segments before performing translation.
background_label: However, short segments pose problems for syntaxbased translation methods, as it is difficult to generate accurate parse trees for sub-sentential segments.
method_label: In this paper, we perform the first experiments applying syntax-based SMT to simultaneous translation, and propose two methods to prevent degradations in accuracy: a method to predict unseen syntactic constituents that help generate complete parse trees, and a method that waits for more input when the current utterance is not enough to generate a fluent translation.
result_label: Experiments on English-Japanese translation show that the proposed methods allow for improvements in accuracy, particularly with regards to word order of the target sentences.

===================================
paper_id: 26376992; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf - abs_tfidfcbow200
TITLE: Temporal Word Analogies: Identifying Lexical Replacement with Diachronic Word Embeddings
ABSTRACT: background_label: AbstractThis paper introduces the concept of temporal word analogies: pairs of words which occupy the same semantic space at different points in time.
background_label: One well-known property of word embeddings is that they are able to effectively model traditional word analogies ("word w 1 is to word w 2 as word w 3 is to word w 4 ") through vector addition.
method_label: Here, I show that temporal word analogies ("word w 1 at time t α is like word w 2 at time t β ") can effectively be modeled with diachronic word embeddings, provided that the independent embedding spaces from each time period are appropriately transformed into a common vector space.
method_label: When applied to a diachronic corpus of news articles, this method is able to identify temporal word analogies such as "Ronald Reagan in 1987 is like Bill Clinton in 1997", or "Walkman in 1987 is like iPod in 2007".
method_label: BackgroundThe meanings of utterances change over time, due both to changes within the linguistic system and to changes in the state of the world.
method_label: For example, the meaning of the word awful has changed over the past few centuries from something like "aweinspiring" to something more like "very bad", due to a process of semantic drift.
method_label: On the other hand, the phrase president of the United States has meant different things at different points in time due to the fact that different people have occupied that same position at different times.
method_label: These are very different types of changes, and the latter may not even be considered a linguistic phenomenon, but both types of change are relevant to the concept of temporal word analogies.I define a temporal word analogy (TWA) as a pair of words which occupy a similar semantic space at different points in time.
result_label: For example, assuming that there is a semantic space associated with "President of the USA", this space was occupied by Ronald Reagan in the 1980s, and by Bill Clinton in the 1990s.
background_label: So a temporal analogy holds: "Ronald Reagan in 1987 is like Bill Clinton in 1997".Distributional semantics methods, particularly vector-space models of word meanings, have been employed to study both semantic change and word analogies, and as such are well-suited for the task of identifying TWAs.
background_label: The principle behind these models, that the meaning of words can be captured by looking at the contexts in which they appear (i.e.
background_label: other words), is not a recent idea, and is generally attributed to Harris (1954 ) or Firth (1957 .
background_label: The modern era of applying this principle algorithmically began with latent semantic analysis (LSA) (Landauer and Dumais, 1997), and the recent explosion in popularity of word embeddings is largely due to the very effective word2vec neural network approach to computing word embeddings (Mikolov et al., 2013a) .
background_label: In these types of vector space models (VSMs), the meaning of a word is represented as a multi-dimensional vector, and semantically-related words tend to have vectors that relate to one another in regular ways (e.g.
background_label: by occupying nearby points in the vector space).
background_label: One factor in word embeddings' recent popularity is their eye-catching ability to model word analogies using vector addition, as in the well-known example king + man − woman = queen (Mikolov et al., 2013b) .
other_label: Sagi et al.
result_label: (2011) were the first to advocate the use of distributional semantics methods (specifically LSA) to automate and quantify large-scale studies of semantic change, in contrast to a more traditional approach in which a researcher inspects 448

===================================
paper_id: 18610536; YEAR: 2011
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: Morpho-Semantic Features for Rule-based Tamil Enconversion
ABSTRACT: background_label: This paper discusses the UNL Enconversion of Tamil sentences.
background_label: The rich morphology of Tamil enables the Enconversion process to be based on morpho-semantic features of the words and their preceding and succeeding context.
method_label: The use of case relation indicating morphological suffixes, POS tag and word level semantics allows the rule based Enconversion to be independent of the syntactic structure of the sentence.
method_label: These UNL graphs are used to build a conceptual level index.
result_label: General Terms Natural Language Processing, Knowledge representation, Information Extraction

===================================
paper_id: 36117198; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: DeepMind_Commentary
ABSTRACT: background_label: We agree with Lake and colleagues on their list of key ingredients for building humanlike intelligence, including the idea that model-based reasoning is essential.
background_label: However, we favor an approach that centers on one additional ingredient: autonomy.
objective_label: In particular, we aim toward agents that can both build and exploit their own internal models, with minimal human hand-engineering.
method_label: We believe an approach centered on autonomous learning has the greatest chance of success as we scale toward real-world complexity, tackling domains for which ready-made formal models are not available.
result_label: Here we survey several important examples of the progress that has been made toward building autonomous agents with humanlike abilities, and highlight some outstanding challenges.

===================================
paper_id: 24372068; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Phonological feature-based speech recognition system for pronunciation training in non-native language learning.
ABSTRACT: background_label: The authors address the question whether phonological features can be used effectively in an automatic speech recognition (ASR) system for pronunciation training in non-native language (L2) learning.
background_label: Computer-aided pronunciation training consists of two essential tasks-detecting mispronunciations and providing corrective feedback, usually either on the basis of full words or phonemes.
method_label: Phonemes, however, can be further disassembled into phonological features, which in turn define groups of phonemes.
method_label: A phonological feature-based ASR system allows the authors to perform a sub-phonemic analysis at feature level, providing a more effective feedback to reach the acoustic goal and perceptual constancy.
method_label: Furthermore, phonological features provide a structured way for analysing the types of errors a learner makes, and can readily convey which pronunciations need improvement.
method_label: This paper presents the authors implementation of such an ASR system using deep neural networks as an acoustic model, and its use for detecting mispronunciations, analysing errors, and rendering corrective feedback.
method_label: Quantitative as well as qualitative evaluations are carried out for German and Italian learners of English.
result_label: In addition to achieving high accuracy of mispronunciation detection, the system also provides accurate diagnosis of errors.

===================================
paper_id: 6527365; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Dependency and AMR Embeddings for Drug-Drug Interaction Extraction from Biomedical Literature
ABSTRACT: background_label: Drug-drug interaction (DDI) is an unexpected change in a drug's effect on the human body when the drug and a second drug are co-prescribed and taken together.
background_label: As many DDIs are frequently reported in biomedical literature, it is important to mine DDI information from literature to keep DDI knowledge up to date.
objective_label: One of the SemEval challenges in the year 2011 and 2013 was designed to tackle the task where the best system achieved an F1 score of 0.80.
method_label: In this paper, we propose to utilize dependency embeddings and Abstract Meaning Representation (AMR) embeddings as features for extracting DDIs.
objective_label: Our contribution is two-fold.
method_label: First, we employed dependency embeddings, previously shown effective for sentence classification, for DDI extraction.
background_label: The dependency embeddings incorporated structural syntactic contexts into the embeddings, which were not present in the conventional word embeddings.
objective_label: Second, we proposed a novel syntactic embedding approach using AMR.
method_label: AMR aims to abstract away from syntactic idiosyncrasies and attempts to capture only the core meaning of a sentence, which could potentially improve DDI extraction from sentences.
method_label: Two classifiers (Support Vector Machine and Random Forest) taking these embedding features as input were evaluated on the DDIExtraction 2013 challenge corpus.
result_label: The experimental results show the effectiveness of dependency and AMR embeddings in the DDI extraction task.
result_label: The best performance was obtained by combining word, dependency and AMR embeddings (F1 score=0.84).

===================================
paper_id: 8140588; YEAR: 2007
adju relevance: Irrelevant (0)
difference: 1; annotator4: 0; annotator3: 1
sources: abs_cbow200
TITLE: 13 Adapting Morphology for Arabic Information Retrieval*
ABSTRACT: background_label: This chapter presents an adaptation of existing techniques in Arabic morphology by leveraging corpus statistics to make them suitable for Information Retrieval (IR).
method_label: The adaptation resulted in the development of Sebawai, an shallow Arabic morphological analyzer, and Al-Stem, an Arabic light stemmer.
method_label: Both were used to produce Arabic index terms for Arabic IR experimentation.
method_label: Sebawai is concerned with generating possible roots and stems of given Arabic word along with probability estimates of deriving the word from each of the possible roots.
method_label: The probability estimates were used a guide to determine which prefixes and suffixes should be used to build the light stemmer Al-Stem.
result_label: The use of the Sebawai generated roots and stems as index terms along with the stems from Al-Stem are evaluated in an information retrieval application and the results are compared.

===================================
paper_id: 119425731; YEAR: 1972
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: Unzerlegbare Darstellungen I
ABSTRACT: background_label: LetK be the structure got by forgetting the composition law of morphisms in a given category.
background_label: A linear representation ofK is given by a map V associating with any morphism ϕ: a→e ofK a linear vector space map V(ϕ): V(a)→V(e).
method_label: We classify thoseK having only finitely many isomorphy classes of indecomposable linear representations.
other_label: This classification is related to an old paper by Yoshii [3].

===================================
paper_id: 60684609; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: Galaxy morphology - an unsupervised machine learning approach
ABSTRACT: background_label: Structural properties posses valuable information about the formation and evolution of galaxies, and are important for understanding the past, present, and future universe.
method_label: Here we use unsupervised machine learning methodology to analyze a network of similarities between galaxy morphological types, and automatically deduce a morphological sequence of galaxies.
method_label: Application of the method to the EFIGI catalog show that the morphological scheme produced by the algorithm is largely in agreement with the De Vaucouleurs system, demonstrating the ability of computer vision and machine learning methods to automatically profile galaxy morphological sequences.
method_label: The unsupervised analysis method is based on comprehensive computer vision techniques that compute the visual similarities between the different morphological types.
method_label: Rather than relying on human cognition, the proposed system deduces the similarities between sets of galaxy images in an automatic manner, and is therefore not limited by the number of galaxies being analyzed.
result_label: The source code of the method is publicly available, and the protocol of the experiment is included in the paper so that the experiment can be replicated, and the method can be used to analyze user-defined datasets of galaxy images.

===================================
paper_id: 17065535; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Neural Network Models for Implicit Discourse Relation Classification in English and Chinese without Surface Features
ABSTRACT: background_label: Inferring implicit discourse relations in natural language text is the most difficult subtask in discourse parsing.
background_label: Surface features achieve good performance, but they are not readily applicable to other languages without semantic lexicons.
background_label: Previous neural models require parses, surface features, or a small label set to work well.
objective_label: Here, we propose neural network models that are based on feedforward and long-short term memory architecture without any surface features.
method_label: To our surprise, our best configured feedforward architecture outperforms LSTM-based model in most cases despite thorough tuning.
result_label: Under various fine-grained label sets and a cross-linguistic setting, our feedforward models perform consistently better or at least just as well as systems that require hand-crafted surface features.
result_label: Our models present the first neural Chinese discourse parser in the style of Chinese Discourse Treebank, showing that our results hold cross-linguistically.

===================================
paper_id: 63767187; YEAR: 1994
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Spoken Language Translator: First-Year Report
ABSTRACT: background_label: This document is the first-year report for a project whose long-term goal is the construction of a practically useful system capable of translating continuous spoken language within a restricted domain.
background_label: The main deliverable resulting from the first year is a prototype, the Spoken Language Translator (SLT), which can translate queries from spoken English to spoken Swedish in the domain of air travel planning.
method_label: The system was developed by SRI International, the Swedish Institute of Computer Science, and Telia Research AB.
method_label: Most of it is constructed from previously existing pieces of software, which have been adapted for use in the speech translation task with as few changes as possible.
method_label: The main components are connected together in a pipelined sequence as follows.
method_label: The input signal is processed by SRI''s DECIPHER(TM), a speaker-independent continuous speech recognition system.
method_label: It produces a set of speech hypotheses which is passed to the English-language processor, the SRI Core Language Engine (CLE), a general natural- language processing system.
method_label: The CLE grammar associates each speech hypothesis with a set of possible logical-form-like representations, typically producing 5 to 50 logical forms per hypothesis.
method_label: A preference component is then used to give each of them a numerical score reflecting its linguistic plausibility.
background_label: When the preference component has made its choice, the highest-scoring logical form is passed to the transfer component, which uses a set of simple non-deterministic recursive pattern-matching rules to rewrite it into a set of possible corresponding Swedish representations.
background_label: The preference component is now invoked again, to select the most plausible transferred logical form.
method_label: The result is fed to a second copy of the CLE, which uses a Swedish- language grammar and lexicon developed at SICS to convert the form into a Swedish string and an associated syntax tree.
method_label: Finally, the string and tree are passed to the Telia Prophon speech synthesizer, which utilizes polyphone synthesis to produce the spoken Swedish utterance.
method_label: The system''s current performance figures, measured on previously unseen test data, are as follows.
method_label: For sentences of length 12 words and under, 65% of all utterances are such that the top-scoring speech hypothesis is an acceptable one.
result_label: If the speech hypothesis is correct, then a translation is produced in 80% of the cases; and 90% of all translations produced are acceptable.
background_label: Nearly all incorrect translations are incorrect due to their containing errors in grammar or naturalness of expression, with errors due to divergence in meaning between the source and target sentences accounting for less than 1% of all translations.
result_label: Making fairly conservative extrapolations from the current SLT prototype, we believe that simply continuing the basic development strategy could within three to five years produce an enhanced version, which recognized about 90% of the short sentences (12 words or less) in a specific domain, and produced accepta

===================================
paper_id: 966293; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Cosegmentation and Cosketch by Unsupervised Learning
ABSTRACT: background_label: Co segmentation refers to the problem of segmenting multiple images simultaneously by exploiting the similarities between the foreground and background regions in these images.
background_label: The key issue in co segmentation is to align common objects between these images.
objective_label: To address this issue, we propose an unsupervised learning framework for co segmentation, by coupling co segmentation with what we call ``co sketch''.
objective_label: The goal of co sketch is to automatically discover a codebook of deformable shape templates shared by the input images.
method_label: These shape templates capture distinct image patterns and each template is matched to similar image patches in different images.
method_label: Thus the co sketch of the images helps to align foreground objects, thereby providing crucial information for co segmentation.
method_label: We present a statistical model whose energy function couples co sketch and co segmentation.
method_label: We then present an unsupervised learning algorithm that performs co sketch and co segmentation by energy minimization.
result_label: Experiments show that our method outperforms state of the art methods for co segmentation on the challenging MSRC and iciest datasets.
result_label: We also illustrate our method on a new dataset called Coseg-Rep where co segmentation can be performed within a single image with repetitive patterns.

===================================
paper_id: 18051931; YEAR: 2012
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: Arabic Keyphrase Extraction using Linguistic knowledge and Machine Learning Techniques
ABSTRACT: background_label: In this paper, a supervised learning technique for extracting keyphrases of Arabic documents is presented.
background_label: The extractor is supplied with linguistic knowledge to enhance its efficiency instead of relying only on statistical information such as term frequency and distance.
method_label: During analysis, an annotated Arabic corpus is used to extract the required lexical features of the document words.
method_label: The knowledge also includes syntactic rules based on part of speech tags and allowed word sequences to extract the candidate keyphrases.
method_label: In this work, the abstract form of Arabic words is used instead of its stem form to represent the candidate terms.
method_label: The Abstract form hides most of the inflections found in Arabic words.
method_label: The paper introduces new features of keyphrases based on linguistic knowledge, to capture titles and subtitles of a document.
method_label: A simple ANOVA test is used to evaluate the validity of selected features.
method_label: Then, the learning model is built using the LDA - Linear Discriminant Analysis - and training documents.
result_label: Although, the presented system is trained using documents in the IT domain, experiments carried out show that it has a significantly better performance than the existing Arabic extractor systems, where precision and recall values reach double their corresponding values in the other systems especially for lengthy and non-scientific articles.

===================================
paper_id: 16746162; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Use of Transformation-Based Learning in Annotation Pipeline of Igbo, an African Language
ABSTRACT: background_label: The accuracy of an annotated corpus can be increased through evaluation and re- vision of the annotation scheme, and through adjudication of the disagreements found.
objective_label: In this paper, we describe a novel process that has been applied to improve a part-of-speech (POS) tagged corpus for the African language Igbo.
objective_label: An inter-annotation agreement (IAA) exercise was undertaken to iteratively revise the tagset used in the creation of the initial tagged corpus, with the aim of refining the tagset and maximizing annotator performance.
method_label: The tagset revisions and other corrections were efficiently propagated to the overall corpus in a semi-automated manner using transformation-based learning (TBL) to identify candidates for cor- rection and to propose possible tag corrections.
method_label: The affected word-tag pairs in the corpus were inspected to ensure a high quality end-product with an accuracy that would not be achieved through a purely automated process.
result_label: The results show that the tagging accuracy increases from 88% to 94%.
result_label: The tagged corpus is potentially re-usable for other dialects of the language.

===================================
paper_id: 14952993; YEAR: 2005
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Measuring Non-Native Speakers' Proficiency Of English By Using A Test With Automatically-Generated Fill-In-The-Blank Questions
ABSTRACT: objective_label: This paper proposes the automatic generation of Fill-in-the-Blank Questions (FBQs) together with testing based on Item Response Theory (IRT) to measure English proficiency.
method_label: First, the proposal generates an FBQ from a given sentence in English.
method_label: The position of a blank in the sentence is determined, and the word at that position is considered as the correct choice.
method_label: The candidates for incorrect choices for the blank are hypothesized through a thesaurus.
method_label: Then, each of the candidates is verified by using the Web.
method_label: Finally, the blanked sentence, the correct choice and the incorrect choices surviving the verification are together laid out to form the FBQ.
method_label: Second, the proficiency of non-native speakers who took the test consisting of such FBQs is estimated through IRT.
result_label: Our experimental results suggest that: (1) the generated questions plus IRT estimate the non-native speakers' English proficiency; (2) while on the other hand, the test can be completed almost perfectly by English native speakers; and (3) the number of questions can be reduced by using item information in IRT.
result_label: The proposed method provides teachers and testers with a tool that reduces time and expenditure for testing English proficiency.

===================================
paper_id: 2612947; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: Context Sensitive Lemmatization Using Two Successive Bidirectional Gated Recurrent Networks
ABSTRACT: background_label: AbstractWe introduce a composite deep neural network architecture for supervised and language independent context sensitive lemmatization.
method_label: The proposed method considers the task as to identify the correct edit tree representing the transformation between a word-lemma pair.
method_label: To find the lemma of a surface word, we exploit two successive bidirectional gated recurrent structures -the first one is used to extract the character level dependencies and the next one captures the contextual information of the given word.
method_label: The key advantages of our model compared to the state-of-the-art lemmatizers such as Lemming and Morfette are -(i) it is independent of human decided features (ii) except the gold lemma, no other expensive morphological attribute is required for joint learning.
result_label: We evaluate the lemmatizer on nine languages -Bengali, Catalan, Dutch, Hindi, Hungarian, Italian, Latin, Romanian and Spanish.
result_label: It is found that except Bengali, the proposed method outperforms Lemming and Morfette on the other languages.
result_label: To train the model on Bengali, we develop a gold lemma annotated dataset 1 (having 1, 702 sentences with a total of 20, 257 word tokens), which is an additional contribution of this work.

===================================
paper_id: 59339352; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: The Natural Selection of Words: Finding the Features of Fitness
ABSTRACT: background_label: We introduce a dataset for studying the evolution of words, constructed from WordNet and the Google Books Ngram Corpus.
background_label: The dataset tracks the evolution of 4,000 synonym sets (synsets), containing 9,000 English words, from 1800 AD to 2000 AD.
method_label: We present a supervised learning algorithm that is able to predict the future leader of a synset: the word in the synset that will have the highest frequency.
method_label: The algorithm uses features based on a word's length, the characters in the word, and the historical frequencies of the word.
method_label: It can predict change of leadership (including the identity of the new leader) fifty years in the future, with an F-score considerably above random guessing.
method_label: Analysis of the learned models provides insight into the causes of change in the leader of a synset.
method_label: The algorithm confirms observations linguists have made, such as the trend to replace the -ise suffix with -ize, the rivalry between the -ity and -ness suffixes, and the struggle between economy (shorter words are easier to remember and to write) and clarity (longer words are more distinctive and less likely to be confused with one another).
result_label: The results indicate that integration of the Google Books Ngram Corpus with WordNet has significant potential for improving our understanding of how language evolves.

===================================
paper_id: 1962889; YEAR: 2012
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: Unsupervised modeling of cell morphology dynamics for time-lapse microscopy
ABSTRACT: background_label: Analysis of cellular phenotypes in large imaging data sets conventionally involves supervised statistical methods, which require user-annotated training data.
method_label: This paper introduces an unsupervised learning method, based on temporally constrained combinatorial clustering, for automatic prediction of cell morphology classes in time-resolved images.
method_label: We applied the unsupervised method to diverse fluorescent markers and screening data and validated accurate classification of human cell phenotypes, demonstrating fully objective data labeling in image-based systems biology.

===================================
paper_id: 6137951; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200
TITLE: Epithelium-Stroma Classification via Convolutional Neural Networks and Unsupervised Domain Adaptation in Histopathological Images
ABSTRACT: background_label: Epithelium-stroma classification is a necessary preprocessing step in histopathological image analysis.
background_label: Current deep learning based recognition methods for histology data require collection of large volumes of labeled data in order to train a new neural network when there are changes to the image acquisition procedure.
background_label: However, it is extremely expensive for pathologists to manually label sufficient volumes of data for each pathology study in a professional manner, which results in limitations in real-world applications.
method_label: A very simple but effective deep learning method, that introduces the concept of unsupervised domain adaptation to a simple convolutional neural network (CNN), has been proposed in this paper.
method_label: Inspired by transfer learning, our paper assumes that the training data and testing data follow different distributions, and there is an adaptation operation to more accurately estimate the kernels in CNN in feature extraction, in order to enhance performance by transferring knowledge from labeled data in source domain to unlabeled data in target domain.
method_label: The model has been evaluated using three independent public epithelium-stroma datasets by cross-dataset validations.
result_label: The experimental results demonstrate that for epithelium-stroma classification, the proposed framework outperforms the state-of-the-art deep neural network model, and it also achieves better performance than other existing deep domain adaptation methods.
result_label: The proposed model can be considered to be a better option for real-world applications in histopathological image analysis, since there is no longer a requirement for large-scale labeled data in each specified domain.

===================================
paper_id: 31731936; YEAR: 1997
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf
TITLE: Handwritten word recognition using lexicon free and lexicon directed word recognition algorithms
ABSTRACT: background_label: The paper discusses the relative merits and complexities of two word recognition algorithms: lexicon directed and lexicon free techniques.
background_label: This algorithm operates on a pre-segmented word image and yields the optimum concatenation of the image segments for each word in the lexicon.
background_label: However, the computational complexity of this algorithm is quite high, as the optimum concatenation is required for every word in the lexicon.
method_label: In the lexicon free word matching process, the character likelihood for all the letters are calculated and the maximum likelihood value and the associated letter are determined.
method_label: In this approach an optimum string results from the concatenation process.
method_label: The word matching process is applied only once for an input word image.
result_label: Comparative results with regard to accuracy, speed and size of lexicon are presented.

===================================
paper_id: 52113103; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200
TITLE: Unsupervised Learning of Syntactic Structure with Invertible Neural Projections
ABSTRACT: background_label: Unsupervised learning of syntactic structure is typically performed using generative models with discrete latent variables and multinomial parameters.
background_label: In most cases, these models have not leveraged continuous word representations.
objective_label: In this work, we propose a novel generative model that jointly learns discrete syntactic structure and continuous word representations in an unsupervised fashion by cascading an invertible neural network with a structured generative prior.
method_label: We show that the invertibility condition allows for efficient exact inference and marginal likelihood computation in our model so long as the prior is well-behaved.
method_label: In experiments we instantiate our approach with both Markov and tree-structured priors, evaluating on two tasks: part-of-speech (POS) induction, and unsupervised dependency parsing without gold POS annotation.
result_label: On the Penn Treebank, our Markov-structured model surpasses state-of-the-art results on POS induction.
result_label: Similarly, we find that our tree-structured model achieves state-of-the-art performance on unsupervised dependency parsing for the difficult training condition where neither gold POS annotation nor punctuation-based constraints are available.

===================================
paper_id: 118163639; YEAR: 1970
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited - title_tfidf - title_tfidfcbow200 - title_cbow200 - specter
TITLE: Morpheme Boundaries within Words: Report on a Computer Test
ABSTRACT: background_label: For the science of linguistics we seek objective and formally describable operations with which to analyze language.
method_label: The phonemes of a language can be determined by means of an explicit behavioral test (the pair test, involving two speakers of the language) and distributional simplifications, i. e. the defining of symbols which express the way in which the outcomes of that test occur in respect to each other in sentences of the language.
method_label: The syntax, and most of the morphology, of a language is discovered by seeing how the morphemes occur in respect to each other in sentences.
result_label: As a bridge between these two sets of methods we need a test for determining what are the morphemes of a language, or at least a test that would tentatively segment a phonemic sequence (as a sentence) into morphemes, leaving it for a distributional criterion to decide which of these tentative segments are to be accepted as morphemes.

===================================
paper_id: 19127468; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Question-Answering with Grammatically-Interpretable Representations
ABSTRACT: background_label: We introduce an architecture, the Tensor Product Recurrent Network (TPRN).
background_label: In our application of TPRN, internal representations learned by end-to-end optimization in a deep neural network performing a textual question-answering (QA) task can be interpreted using basic concepts from linguistic theory.
method_label: No performance penalty need be paid for this increased interpretability: the proposed model performs comparably to a state-of-the-art system on the SQuAD QA task.
method_label: The internal representation which is interpreted is a Tensor Product Representation: for each input word, the model selects a symbol to encode the word, and a role in which to place the symbol, and binds the two together.
method_label: The selection is via soft attention.
background_label: The overall interpretation is built from interpretations of the symbols, as recruited by the trained model, and interpretations of the roles as used by the model.
background_label: We find support for our initial hypothesis that symbols can be interpreted as lexical-semantic word meanings, while roles can be interpreted as approximations of grammatical roles (or categories) such as subject, wh-word, determiner, etc.
other_label: Fine-grained analysis reveals specific correspondences between the learned roles and parts of speech as assigned by a standard tagger (Toutanova et al.
result_label: 2003), and finds several discrepancies in the model's favor.
result_label: In this sense, the model learns significant aspects of grammar, after having been exposed solely to linguistically unannotated text, questions, and answers: no prior linguistic knowledge is given to the model.
result_label: What is given is the means to build representations using symbols and roles, with an inductive bias favoring use of these in an approximately discrete manner.

===================================
paper_id: 53408371; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf - title_tfidfcbow200 - title_cbow200
TITLE: An automatic taxonomy of galaxy morphology using unsupervised machine learning
ABSTRACT: background_label: We present an unsupervised machine learning technique that automatically segments and labels galaxies in astronomical imaging surveys using only pixel data.
background_label: Distinct from previous unsupervised machine learning approaches used in astronomy we use no pre-selection or pre-filtering of target galaxy type to identify galaxies that are similar.
method_label: We demonstrate the technique on the HST Frontier Fields.
method_label: By training the algorithm using galaxies from one field (Abell 2744) and applying the result to another (MACS0416.1-2403), we show how the algorithm can cleanly separate early and late type galaxies without any form of pre-directed training for what an 'early' or 'late' type galaxy is.
method_label: We then apply the technique to the HST CANDELS fields, creating a catalogue of approximately 60,000 classifications.
method_label: We show how the automatic classification groups galaxies of similar morphological (and photometric) type, and make the classifications public via a catalogue, a visual catalogue and galaxy similarity search.
result_label: We compare the CANDELS machine-based classifications to human-based classifications from the Galaxy Zoo: CANDELS project.
result_label: Although there is not a direct mapping between Galaxy Zoo and our hierarchical labelling, we demonstrate a good level of concordance between human and machine classifications.
result_label: Finally, we show how the technique can be used to identify rarer objects and present new lensed galaxy candidates from the CANDELS imaging.

===================================
paper_id: 134987; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200
TITLE: Unsupervised Incremental Learning and Prediction of Music Signals
ABSTRACT: background_label: A system is presented that segments, clusters and predicts musical audio in an unsupervised manner, adjusting the number of (timbre) clusters instantaneously to the audio input.
method_label: A sequence learning algorithm adapts its structure to a dynamically changing clustering tree.
background_label: The flow of the system is as follows: 1) segmentation by onset detection, 2) timbre representation of each segment by Mel frequency cepstrum coefficients, 3) discretization by incremental clustering, yielding a tree of different sound classes (e.g.
method_label: instruments) that can grow or shrink on the fly driven by the instantaneous sound events, resulting in a discrete symbol sequence, 4) extraction of statistical regularities of the symbol sequence, using hierarchical N-grams and the newly introduced conceptual Boltzmann machine, and 5) prediction of the next sound event in the sequence.
method_label: The system's robustness is assessed with respect to complexity and noisiness of the signal.
result_label: Clustering in isolation yields an adjusted Rand index (ARI) of 82.7% / 85.7% for data sets of singing voice and drums.
result_label: Onset detection jointly with clustering achieve an ARI of 81.3% / 76.3% and the prediction of the entire system yields an ARI of 27.2% / 39.2%.

===================================
paper_id: 3688126; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200
TITLE: Calculated attributes of synonym sets
ABSTRACT: objective_label: The goal of formalization, proposed in this paper, is to bring together, as near as possible, the theoretic linguistic problem of synonym conception and the computer linguistic methods based generally on empirical intuitive unjustified factors.
method_label: Using the word vector representation we have proposed the geometric approach to mathematical modeling of synonym set (synset).
method_label: The word embedding is based on the neural networks (Skip-gram, CBOW), developed and realized as word2vec program by T. Mikolov.
method_label: The standard cosine similarity is used as the distance between word-vectors.
method_label: Several geometric characteristics of the synset words are introduced: the interior of synset, the synset word rank and centrality.
method_label: These notions are intended to select the most significant synset words, i.e.
method_label: the words which senses are the nearest to the sense of a synset.
method_label: Some experiments with proposed notions, based on RusVectores resources, are represented.
other_label: A brief description of this work can be viewed in slides https://goo.gl/K82Fei

===================================
paper_id: 14968002; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Unsupervised learning of rhetorical structure with un-topic models
ABSTRACT: background_label: AbstractIn this paper we investigate whether unsupervised models can be used to induce conventional aspects of rhetorical language in scientific writing.
background_label: We rely on the intuition that the rhetorical language used in a document is general in nature and independent of the document's topic.
method_label: We describe a Bayesian latent-variable model that implements this intuition.
result_label: In two empirical evaluations based on the task of argumentative zoning (AZ), we demonstrate that our generality hypothesis is crucial for distinguishing between rhetorical and topical language and that features provided by our unsupervised model trained on a large corpus can improve the performance of a supervised AZ classifier.

===================================
paper_id: 14727924; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_tfidf
TITLE: Detection, segmentation and classification of 3D urban objects using mathematical morphology and supervised learning
ABSTRACT: objective_label: We propose an automatic and robust approach to detect, segment and classify urban objects from 3D point clouds.
method_label: Processing is carried out using elevation images and the result is reprojected onto the 3D point cloud.
method_label: First, the ground is segmented and objects are detected as discontinuities on the ground.
method_label: Then, connected objects are segmented using a watershed approach.
method_label: Finally, objects are classified using SVM with geometrical and contextual features.
method_label: Our methodology is evaluated on databases from Ohio (USA) and Paris (France).
result_label: In the former, our method detects 98% of the objects, 78% of them are correctly segmented and 82% of the well-segmented objects are correctly classified.
result_label: In the latter, our method leads to an improvement of about 15% on the classification step with respect to previous works.
result_label: Quantitative results prove that our method not only provides a good performance but is also faster than other works reported in the literature.

===================================
paper_id: 15445897; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: PLSA enhanced with a long-distance bigram language model for speech recognition
ABSTRACT: objective_label: We propose a language modeling (LM) approach using background n-grams and interpolated distanced n-grams for speech recognition using an enhanced probabilistic latent semantic analysis (EPLSA) derivation.
background_label: PLSA is a bag-of-words model that exploits the topic information at the document level, which is inconsistent for the language modeling in speech recognition.
objective_label: In this paper, we consider the word sequence in modeling the EPLSA model.
method_label: Here, the predicted word of an n-gram event is drawn from a topic that is chosen from the topic distribution of the (n-1) history words.
method_label: The EPLSA model cannot capture the long-range topic information from outside of the n-gram event.
method_label: The distanced n-grams are incorporated into interpolated form (IEPLSA) to cover the long-range information.
method_label: A cache-based LM that models the re-occurring words is also incorporated through unigram scaling to the EPLSA and IEPLSA models, which models the topical words.
result_label: We have seen that our proposed approaches yield significant reductions in perplexity and word error rate (WER) over a PLSA based LM approach using the Wall Street Journal (WSJ) corpus.

===================================
paper_id: 775942; YEAR: 2006
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: Unsupervised classification of ventricular extrasystoles using bounded clustering algorithms and morphology matching
ABSTRACT: background_label: Ventricular extrasystoles (VE) are ectopic heartbeats involving irregularities in the heart rhythm.
background_label: VEs arise in response to impulses generated in some part of the heart different from the sinoatrial node.
background_label: These are caused by the premature discharge of a ventricular ectopic focus.
background_label: VEs after myocardial infarction are associated with increased mortality.
background_label: Screening of VEs is typically a manual and time consuming task that involves analysis of the heartbeat morphology, QRS duration, and variations of the RR intervals using long-term electrocardiograms.
method_label: We describe a novel algorithm to perform automatic classification of VEs and report the results of our validation study.
method_label: The proposed algorithm makes use of bounded clustering algorithms, morphology matching, and RR interval length to perform automatic VE classification without prior knowledge of the number of classes and heartbeat features.
result_label: Additionally, the proposed algorithm does not need a training set.

===================================
paper_id: 119009054; YEAR: 1995
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200
TITLE: AUTOMATED MORPHOLOGICAL CLASSIFICATION OF APM GALAXIES BY SUPERVISED ARTIFICIAL NEURAL NETWORKS
ABSTRACT: background_label: We train Artificial Neural Networks to classify galaxies based solely on the morphology of the galaxy images as they appear on blue survey plates.
method_label: The images are reduced and morphological features such as bulge size and the number of arms are extracted, all in a fully automated manner.
method_label: The galaxy sample was first classified by 6 independent experts.
method_label: We use several definitions for the mean type of each galaxy, based on those classifications.
method_label: We then train and test the network on these features.
result_label: We find that the rms error of the network classifications, as compared with the mean types of the expert classifications, is 1.8 Revised Hubble Types.
result_label: This is comparable to the overall rms dispersion between the experts.
result_label: This result is robust and almost completely independent of the network architecture used.

===================================
paper_id: 84845408; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidf
TITLE: ToyArchitecture: Unsupervised Learning of Interpretable Models of the World
ABSTRACT: background_label: Research in Artificial Intelligence (AI) has focused mostly on two extremes: either on small improvements in narrow AI domains, or on universal theoretical frameworks which are usually uncomputable, incompatible with theories of biological intelligence, or lack practical implementations.
objective_label: The goal of this work is to combine the main advantages of the two: to follow a big picture view, while providing a particular theory and its implementation.
background_label: In contrast with purely theoretical approaches, the resulting architecture should be usable in realistic settings, but also form the core of a framework containing all the basic mechanisms, into which it should be easier to integrate additional required functionality.
method_label: In this paper, we present a novel, purposely simple, and interpretable hierarchical architecture which combines multiple different mechanisms into one system: unsupervised learning of a model of the world, learning the influence of one's own actions on the world, model-based reinforcement learning, hierarchical planning and plan execution, and symbolic/sub-symbolic integration in general.
method_label: The learned model is stored in the form of hierarchical representations with the following properties: 1) they are increasingly more abstract, but can retain details when needed, and 2) they are easy to manipulate in their local and symbolic-like form, thus also allowing one to observe the learning process at each level of abstraction.
method_label: On all levels of the system, the representation of the data can be interpreted in both a symbolic and a sub-symbolic manner.
method_label: This enables the architecture to learn efficiently using sub-symbolic methods and to employ symbolic inference.

===================================
paper_id: 6336747; YEAR: 2011
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: specter
TITLE: Exploiting Morphology in Turkish Named Entity Recognition System
ABSTRACT: background_label: AbstractTurkish is an agglutinative language with complex morphological structures, therefore using only word forms is not enough for many computational tasks.
objective_label: In this paper we analyze the effect of morphology in a Named Entity Recognition system for Turkish.
method_label: We start with the standard word-level representation and incrementally explore the effect of capturing syntactic and contextual properties of tokens.
method_label: Furthermore, we also explore a new representation in which roots and morphological features are represented as separate tokens instead of representing only words as tokens.
result_label: Using syntactic and contextual properties with the new representation provide an 7.6% relative improvement over the baseline.

===================================
paper_id: 6313308; YEAR: 2007
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Automatic Pattern Classification by Unsupervised Learning Using Dimensionality Reduction of Data with Mirroring Neural Networks
ABSTRACT: background_label: This paper proposes an unsupervised learning technique by using Multi-layer Mirroring Neural Network and Forgy's clustering algorithm.
method_label: Multi-layer Mirroring Neural Network is a neural network that can be trained with generalized data inputs (different categories of image patterns) to perform non-linear dimensionality reduction and the resultant low-dimensional code is used for unsupervised pattern classification using Forgy's algorithm.
method_label: By adapting the non-linear activation function (modified sigmoidal function) and initializing the weights and bias terms to small random values, mirroring of the input pattern is initiated.
method_label: In training, the weights and bias terms are changed in such a way that the input presented is reproduced at the output by back propagating the error.
method_label: The mirroring neural network is capable of reducing the input vector to a great degree (approximately 1/30th the original size) and also able to reconstruct the input pattern at the output layer from this reduced code units.
method_label: The feature set (output of central hidden layer) extracted from this network is fed to Forgy's algorithm, which classify input data patterns into distinguishable classes.
method_label: In the implementation of Forgy's algorithm, initial seed points are selected in such a way that they are distant enough to be perfectly grouped into different categories.
method_label: Thus a new method of unsupervised learning is formulated and demonstrated in this paper.
result_label: This method gave impressive results when applied to classification of different image patterns.

===================================
paper_id: 586686; YEAR: 2011
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: specter - abs_tfidfcbow200
TITLE: A Novel Approach to Morphological Disambiguation for Turkish
ABSTRACT: other_label: Abstract.
background_label: In this paper, we propose a classification based approach to the morphological disambiguation for Turkish language.
background_label: Due to complex morphology in Turkish, any word can get unlimited number of affixes resulting very large tag sets.
objective_label: The problem is defined as choosing one of parses of a word not taking the existing root word into consideration.
method_label: We trained our model with well-known classifiers using WEKA toolkit and tested on a common test set.
result_label: The best performance achieved is 95.61% by J48 Tree classifier.

===================================
paper_id: 6326861; YEAR: 2011
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Unsupervised learning of stochastic AND-OR templates for object modeling
ABSTRACT: background_label: This paper presents a framework for unsupervised learning of a hierarchical generative image model called ANDOR Template (AOT) for visual objects.
background_label: The AOT includes: (1) hierarchical composition as “AND” nodes, (2) deformation of parts as continuous “OR” nodes, and (3) multiple ways of composition as discrete “OR” nodes.
method_label: These AND/OR nodes form the hierarchical visual dictionary.
method_label: We show that both the structure and parameters of the AOT model can be learned in an unsupervised way from example images using an information projection principle.
method_label: The learning algorithm consists two steps: i) a recursive Block-Pursuit procedure to learn the hierarchical dictionary of primitives, parts and objects, which form leaf nodes, AND nodes and structural OR nodes and ii) a Graph-Compression operation to minimize model structure for better generalizability, which produce additional OR nodes across the compositional hierarchy.
method_label: We investigate the conditions under which the learning algorithm can identify, (i.e.
result_label: recover) an underlying AOT that generates the data, and evaluate the performance of our learning algorithm through both artificial and real examples.

===================================
paper_id: 9148295; YEAR: 2007
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_cbow200 - title_tfidf - title_tfidfcbow200 - specter - abs_tfidfcbow200
TITLE: Morphology-aware statistical machine translation based on morphs induced in an unsupervised manner
ABSTRACT: method_label: In this paper, we apply a method of unsupervised morphology learning to a state-of-the-art phrase-based statistical ma chine translation (SMT) system.
background_label: In SMT, words are traditionally used as the smallest units of translation.
background_label: Such a system generalizes poorl y to word forms that do not occur in the training data.
background_label: In particular, this is problematic for languages that are highly compounding, highly inflecting, or both.
method_label: An alternative way is to use sub-word units, such as morphemes.
method_label: We use the Morfessor algorithm to find statistical mo rphemelike units (called morphs) that can be used to reduce the size of the lexicon and improve the ability to generalize.
method_label: Transl ation and language models are trained directly on morphs instead of words.
method_label: The approach is tested on three Nordic languages (Danish, Finnish, and Swedish) that are included in the Europarl corpus consisting of the Proceedings of the European Parliament.
result_label: However, in our experiments we did not obtain higher BLEU scores for the morph model than for the standard word-based approach.
result_label: Nonetheless, the proposed morph-based solution has clear benefits, as morpho logically well motivated structures (phrases) are learned , and the proportion of words left untranslated is clearly reduced.

===================================
paper_id: 3217932; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: UMCC_DLSI: Reinforcing a Ranking Algorithm with Sense Frequencies and Multidimensional Semantic Resources to solve Multilingual Word Sense Disambiguation
ABSTRACT: objective_label: AbstractThis work introduces a new unsupervised approach to multilingual word sense disambiguation.
objective_label: Its main purpose is to automatically choose the intended sense (meaning) of a word in a particular context for different languages.
method_label: It does so by selecting the correct Babel synset for the word and the various Wiki Page titles that mention the word.
method_label: BabelNet contains all the output information that our system needs, in its Babel synset.
method_label: Through Babel synset, we find all the possible Synsets for the word in WordNet.
method_label: Using these Synsets, we apply the disambiguation method Ppr+Freq to find what we need.
method_label: To facilitate the work with WordNet, we use the ISR-WN which offers the integration of different resources to WordNet.
result_label: Our system, recognized as the best in the competition, obtains results around 69% of Recall.

===================================
paper_id: 8254342; YEAR: 2012
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Detecting Grammatical Errors with Treebank-Induced , Probabilistic Parsers
ABSTRACT: background_label: Today's grammar checkers often use hand-crafted rule systems that define acceptable language.
background_label: The development of such rule systems is labour-intensive and has to be repeated for each language.
background_label: At the same time, grammars automatically induced from syntactically annotated corpora (treebanks) are successfully employed in other applications, for example text understanding and machine translation.
background_label: At first glance, treebank-induced grammars seem to be unsuitable for grammar checking as they massively over-generate and fail to reject ungrammatical input due to their high robustness.
method_label: We present three new methods for judging the grammaticality of a sentence with probabilistic, treebank-induced grammars, demonstrating that such grammars can be successfully applied to automatically judge the grammaticality of an input string.
result_label: Our best-performing method exploits the differences between parse results for grammars trained on grammatical and ungrammatical treebanks.
background_label: The second approach builds an estimator of the probability of the most likely parse using grammatical training data that has previously been parsed and annotated with parse probabilities.
background_label: If the estimated probability of an input sentence (whose grammaticality is to be judged by the system) is higher by a certain amount than the actual parse probability, the sentence is flagged as ungrammatical.
method_label: The third approach extracts discriminative parse tree fragments in the form of CFG rules from parsed grammatical and ungrammatical corpora and trains a binary classifier to distinguish grammatical from ungrammatical sentences.
method_label: The three approaches are evaluated on a large test set of grammatical and ungrammatical sentences.
method_label: The ungrammatical test set is generated automatically by inserting common grammatical errors into the British National Corpus.
result_label: The results are compared to two traditional approaches, one that uses a hand-crafted, discriminative grammar, the XLE ParGram English LFG, and one based on part-of-speech n-grams.
result_label: In addition, the baseline methods and the new methods are combined in a machine learning-based framework, yielding further improvements.

===================================
paper_id: 4554680; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: Learning Unsupervised Learning Rules
ABSTRACT: objective_label: AbstractA major goal of unsupervised learning is to discover data representations that are useful for subsequent tasks, without access to supervised labels during training.
objective_label: Typically, this goal is approached by minimizing a surrogate objective, such as the negative log likelihood of a generative model, with the hope that representations useful for subsequent tasks will arise incidentally.
objective_label: In this work, we propose instead to directly target a later desired task by meta-learning an unsupervised learning rule, which leads to representations useful for that task.
method_label: Here, our desired task (meta-objective) is the performance of the representation on semi-supervised classification, and we meta-learn an algorithm -an unsupervised weight update rule -that produces representations that perform well under this meta-objective.
method_label: Additionally, we constrain our unsupervised update rule to a be a biologicallymotivated, neuron-local function, which enables it to generalize to novel neural network architectures.
method_label: We show that the meta-learned update rule produces useful features and sometimes outperforms existing unsupervised learning techniques.
method_label: We further show that the meta-learned unsupervised update rule generalizes to train networks with different widths, depths, and nonlinearities.
result_label: It also generalizes to train on data with randomly permuted input dimensions and even generalizes from image datasets to a text task.

===================================
paper_id: 1358160; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200
TITLE: Incremental Learning of Object Detectors without Catastrophic Forgetting
ABSTRACT: background_label: Despite their success for object detection, convolutional neural networks are ill-equipped for incremental learning, i.e., adapting the original model trained on a set of classes to additionally detect objects of new classes, in the absence of the initial training data.
background_label: They suffer from"catastrophic forgetting"- an abrupt degradation of performance on the original set of classes, when the training objective is adapted to the new classes.
method_label: We present a method to address this issue, and learn object detectors incrementally, when neither the original training data nor annotations for the original classes in the new training set are available.
method_label: The core of our proposed solution is a loss function to balance the interplay between predictions on the new classes and a new distillation loss which minimizes the discrepancy between responses for old classes from the original and the updated networks.
method_label: This incremental learning can be performed multiple times, for a new set of classes in each step, with a moderate drop in performance compared to the baseline network trained on the ensemble of data.
result_label: We present object detection results on the PASCAL VOC 2007 and COCO datasets, along with a detailed empirical analysis of the approach.

===================================
paper_id: 6352278; YEAR: 1996
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf
TITLE: Automatic Extraction Of New Words From Japanese Texts Using Generalized Forward-Backward Search
ABSTRACT: background_label: AbstractWe present a novel new word extraction method from Japanese texts based on expected word frequencies.
method_label: First, we compute expected word frequencies from Japanese texts using a robust stochastic N-best word segmenter.
method_label: We then extract new words by filtering out erroneous word hypotheses whose expected word frequencies are lower than the predefined threshold.
method_label: The method is derived from an approximation of the generalized version of the Forward-Backward algorithm.
result_label: When the Japanese word segmenter is trained on a 4.7 million word segmented corpus and tested on 1000 sentences whose out-of-vocabulary rate is 2.1%, the accuracy of the new word extraction method is 43.7% recall and 52.3% precision.

===================================
paper_id: 3866444; YEAR: 2010
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf
TITLE: Niche as a determinant of word fate in online groups
ABSTRACT: background_label: Patterns of word use both reflect and influence a myriad of human activities and interactions.
background_label: Like other entities that are reproduced and evolve, words rise or decline depending upon a complex interplay between {their intrinsic properties and the environments in which they function}.
method_label: Using Internet discussion communities as model systems, we define the concept of a word niche as the relationship between the word and the characteristic features of the environments in which it is used.
method_label: We develop a method to quantify two important aspects of the size of the word niche: the range of individuals using the word and the range of topics it is used to discuss.
method_label: Controlling for word frequency, we show that these aspects of the word niche are strong determinants of changes in word frequency.
method_label: Previous studies have already indicated that word frequency itself is a correlate of word success at historical time scales.
result_label: Our analysis of changes in word frequencies over time reveals that the relative sizes of word niches are far more important than word frequencies in the dynamics of the entire vocabulary at shorter time scales, as the language adapts to new concepts and social groupings.
method_label: We also distinguish endogenous versus exogenous factors as additional contributors to the fates of words, and demonstrate the force of this distinction in the rise of novel words.
result_label: Our results indicate that short-term nonstationarity in word statistics is strongly driven by individual proclivities, including inclinations to provide novel information and to project a distinctive social identity.

===================================
paper_id: 18087715; YEAR: 2008
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_cbow200
TITLE: Tagging Icelandic text: A linguistic rule-based approach
ABSTRACT: background_label: The Icelandic language is a morphologically complex language, for which a large tagset has been created.
objective_label: This paper describes the design of a linguistic rulebased system for part-of-speech tagging Icelandic text.
method_label: The system contains two main components: a disambiguator, IceTagger, and an unknown word guesser, IceMorphy.
method_label: IceTagger uses a small number of local elimination rules along with a global heuristics component.
method_label: The heuristics guess the functional roles of the words in a sentence, mark prepositional phrases, and use the acquired knowledge to force feature agreement where appropriate.
method_label: IceMorphy is used for guessing the tag profile for unknown words and for automatically filling tag profile gaps in the lexicon.
result_label: Evaluation shows that IceTagger achieves 91.54% accuracy, a substantial improvement on the highest accuracy, 90.44%, obtained using three state-of-the-art data-driven taggers.
result_label: Furthermore, the accuracy increases to 92.95% by using IceTagger along with two data-driven taggers in a simple voting scheme.
result_label: The development time of the tagging system was only 7 man-months, which can be considered a short development period for a linguistic rule-based system.

===================================
paper_id: 61801813; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200
TITLE: Simplicity of Kmeans Versus Deepness of Deep Learning: A Case of Unsupervised Feature Learning with Limited Data
ABSTRACT: background_label: We study a bio-detection application as a case study to demonstrate that Kmeans -- based unsupervised feature learning can be a simple yet effective alternative to deep learning techniques for small data sets with limited intra-as well as inter-class diversity.
objective_label: We investigate the effect on the classifier performance of data augmentation as well as feature extraction with multiple patch sizes and at different image scales.
method_label: Our data set includes 1833 images from four different classes of bacteria, each bacterial culture captured at three different wavelengths and overall data collected during a three-day period.
method_label: The limited number and diversity of images present, potential random effects across multiple days, and the multi-mode nature of class distributions pose a challenging setting for representation learning.
result_label: Using images collected on the first day for training, on the second day for validation, and on the third day for testing Kmeans -- based representation learning achieves 97% classification accuracy on the test data.
result_label: This compares very favorably to 56% accuracy achieved by deep learning and 74% accuracy achieved by handcrafted features.
result_label: Our results suggest that data augmentation or dropping connections between units offers little help for deep-learning algorithms, whereas significant boost can be achieved by Kmeans -- based representation learning by augmenting data and by concatenating features obtained at multiple patch sizes or image scales.

===================================
paper_id: 735269; YEAR: 2012
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Korean Treebank Transformation for Parser Training
ABSTRACT: background_label: AbstractKorean is a morphologically rich language in which grammatical functions are marked by inflections and affixes, and they can indicate grammatical relations such as subject, object, predicate, etc.
background_label: A Korean sentence could be thought as a sequence of eojeols.
background_label: An eojeol is a word or its variant word form agglutinated with grammatical affixes, and eojeols are separated by white space as in English written texts.
method_label: Korean treebanks (Choi et al., 1994; Han et al., 2002; Korean Language Institute, 2012) use eojeol as their fundamental unit of analysis, thus representing an eojeol as a prepreterminal phrase inside the constituent tree.
method_label: This eojeol-based annotating schema introduces various complexity to train the parser, for example an entity represented by a sequence of nouns will be annotated as two or more different noun phrases, depending on the number of spaces used.
method_label: In this paper, we propose methods to transform eojeol-based Korean treebanks into entity-based Korean treebanks.
method_label: The methods are applied to Sejong treebank, which is the largest constituent treebank in Korean, and the transformed treebank is used to train and test various probabilistic CFG parsers.
result_label: The experimental result shows that the proposed transformation methods reduce ambiguity in the training corpus, increasing the overall F1 score up to about 9 %.

===================================
paper_id: 121138711; YEAR: 1995
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited - title_tfidf - title_tfidfcbow200 - title_cbow200 - specter
TITLE: Learnable classes of categorial grammars
ABSTRACT: background_label: The dissertation investigates learnability of various classes of classical categorial grammars within the Gold paradigm of identification in the limit from positive data.
background_label: Both learning from functor-argument structures and learning from flat strings are considered.
background_label: The class of rigid grammars, the class of k-valued grammars (k = 2, 3, ...), the class of least-valued grammars, and the class of least-cardinality grammars are shown to be learnable from structures, and the class of rigid grammars and the class of k-valued grammars (k = 2, 3, ...) are also shown to be learnable from strings.
background_label: An interesting class that is not learnable even from structures is treated as well.
method_label: In proving learnability results, I make essential use of the concept known as finite elasticity, which is a property of language classes.
method_label: I prove that finite elasticity is preserved under the inverse image of a finite-valued relation, extending results of Wright's and of Moriyama and Sato's.
method_label: I use this theorem to 'transfer' finite elasticity from the class of rigid structure languages to the class of k-valued structure languages, and then to the class of k-valued string languages.
method_label: The learning algorithms used incorporate Buszkowski and Penn's algorithms for determining categorial grammars from input consisting of functor-argument structures.
method_label: Some of the learnability results are extended to such loosely 'categorial' formalisms as combinatory grammar and Montague grammar.
result_label: The appendix presents Prolog implementations of some of the learning algorithms used in the dissertation.

===================================
paper_id: 121370737; YEAR: 1994
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: Optimal unsupervised learning
ABSTRACT: background_label: We introduce an inferential approach to unsupervised learning which allows us to define an optimal learning strategy.
method_label: Applying these ideas to a simple, previously studied model, we show that it is impossible to detect structure in data until a critical number of examples have been presented-an effect which will be observed in all problems with certain underlying symmetries.
method_label: Thereafter, the advantage of optimal learning over previously studied learning algorithms depends critically upon the distribution of patterns; optimal learning may be exponentially faster.
result_label: Models with more subtle correlations are harder to analyse, but in a simple limit of one such problem we calculate exactly the efficacy of an algorithm similar to some used in practice, and compare it to that of the optimal prescription.

===================================
paper_id: 60925141; YEAR: 1986
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Alignment of phonemes with their corresponding orthography
ABSTRACT: background_label: A technique has been developed for aligning the phonemes in a phonemic transcription of a word with the graphemes in its orthographic representation.
background_label: For example, creationism /kri:eizam/ can be alighed ascreationism/kri:[email protected]!nizam/ Tables of phonemic-to-orthographic correspondences are given together with the frequency of occurrence of each correspondence in the texts constituting the Lancaster-Oslo/Bergen Corpus (LOB) (1978).
objective_label: The initial motivation for this algorithm was as an aid to the production of a computerized dictionary of English pronunciation.
method_label: It has been used subsequently to extract from the dictionary all possible pronunciations of English prefixes, and is being used as part of the inferential process in the building of rules for the automated phonemic transcription of English.
method_label: Uses are also seen in automatic speech recognition as part of the process of decoding the phonetic information extracted from the analysis of the acoustic signal.
method_label: The algorithm has been checked using a phonemically tagged version of the LOB Corpus.
result_label: The accent of English used in this work is Received Pronunciation (RP), see Gimson (1980).

===================================
paper_id: 61203024; YEAR: 1985
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: Isolated word recognition using hidden Markov models
ABSTRACT: background_label: In this paper we investigated smoothing techniques for alleviating the problem due to insufficient amount of training data.
background_label: Hidden Markov Models (HMM) require a large amount of training data to obtain reliable probability estimates.
background_label: But for isolated word recognition (100 words or more), we can not expect a user to speak each word more than several times.
background_label: We found that the confusion matrix between a pair of label prototypes was particularly effective for the problem.
method_label: We investigated two ways of computing the confusion matrix.
method_label: One is based on distance among labels, and the other is based on the correspondence of labels in several utterances of the same word.
method_label: Performance of these techniques was tested by using 100 Japanese city names spoken in an isolated word mode by three speakers.
result_label: It was found that the smoothing technique reduced recognition errors from 1% to 0.1%.
result_label: To visualize such performance improvement, we used, together with recognition rate, "two-dimensional score plot," which shows the distribution of the best score of the true word and that of the remaining false ones in the vocabulary.

===================================
paper_id: 58981651; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidf
TITLE: Enhancing Semantic Word Representations by Embedding Deeper Word Relationships
ABSTRACT: background_label: Word representations are created using analogy context-based statistics and lexical relations on words.
background_label: Word representations are inputs for the learning models in Natural Language Understanding (NLU) tasks.
background_label: However, to understand language, knowing only the context is not sufficient.
background_label: Reading between the lines is a key component of NLU.
method_label: Embedding deeper word relationships which are not represented in the context enhances the word representation.
method_label: This paper presents a word embedding which combines an analogy, context-based statistics using Word2Vec, and deeper word relationships using Conceptnet, to create an expanded word representation.
method_label: In order to fine-tune the word representation, Self-Organizing Map is used to optimize it.
method_label: The proposed word representation is compared with semantic word representations using Simlex 999.
method_label: Furthermore, the use of 3D visual representations has shown to be capable of representing the similarity and association between words.
result_label: The proposed word representation shows a Spearman correlation score of 0.886 and provided the best results when compared to the current state-of-the-art methods, and exceed the human performance of 0.78.

===================================
paper_id: 52012943; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: A New Approach to Animacy Detection
ABSTRACT: background_label: AbstractAnimacy is a necessary property for a referent to be an agent, and thus animacy detection is useful for a variety of natural language processing tasks, including word sense disambiguation, co-reference resolution, semantic role labeling, and others.
background_label: Prior work treated animacy as a word-level property, and has developed statistical classifiers to classify words as either animate or inanimate.
method_label: We discuss why this approach to the problem is ill-posed, and present a new approach based on classifying the animacy of co-reference chains.
method_label: We show that simple voting approaches to inferring the animacy of a chain from its constituent words perform relatively poorly, and then present a hybrid system merging supervised machine learning (ML) and a small number of handbuilt rules to compute the animacy of referring expressions and co-reference chains.
method_label: This method achieves state of the art performance.
method_label: The supervised ML component leverages features such as word embeddings over referring expressions, parts of speech, and grammatical and semantic roles.
background_label: The rules take into consideration parts of speech and the hypernymy structure encoded in WordNet.
background_label: The system achieves an F 1 of 0.88 for classifying the animacy of referring expressions, which is comparable to state of the art results for classifying the animacy of words, and achieves an F 1 of 0.75 for classifying the animacy of coreference chains themselves.
method_label: We release our training and test dataset, which includes 142 texts (all narratives) comprising 156,154 words, 34,698 referring expressions, and 10,941 co-reference chains.
method_label: We test the method on a subset of the OntoNotes dataset, showing using manual sampling that animacy classification is 90%±2% accurate for coreference chains, and 92%±1% for referring expressions.
method_label: The data also contains 46 folktales, which present an interesting challenge because they often involve characters who are members of traditionally inanimate classes (e.g., stoves that walk, trees that talk).
result_label: We show that our system is able to detect the animacy of these unusual referents with an F 1 of 0.95.

===================================
paper_id: 35102549; YEAR: 2001
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: specter
TITLE: A Bayesian Model For Morpheme And Paradigm Identification
ABSTRACT: background_label: This paper describes a system for unsupervised learning of morphological affixes from texts or word lists.
method_label: The system is composed of a generative probability model and a search algorithm.
method_label: Experiments on the Wall Street Journal and the Hansard Corpus (French and English) demonstrate the effectiveness of this approach.
result_label: The results suggest that more integrated systems for learning both affixes and morphographemic adjustment rules may be feasible.
method_label: In addition, several definitions and a theorem are developed so that our search algorithm can be formalized in terms of the lattice formed by subsets of suffixes under inclusion.
result_label: This formalism is expected to be useful for investigating alternative search strategies over the same morphological hypothesis space.

===================================
paper_id: 146981046; YEAR: 1992
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: cited - title_tfidf - title_tfidfcbow200 - title_cbow200 - specter
TITLE: PC-KIMMO: A Two-Level Processor for Morphological Analysis
ABSTRACT: background_label: This package is a faithful implementation of Koskenniemi's two-level morphology, KIMMO (Koskenniemi 1983) .
background_label: As with previous tools from the Summer Institute of Linguistics (e.g., Weber, Black, and McConnel 1988) , the package is primarily aimed at field linguists with little computational expertise, but for whom morphological analysis tools are useful.
background_label: The book that accompanies the software is extremely good: among other things, it incorporates what is in my opinion the best available introduction to the theory and practice of two-level morphology.
method_label: The package is therefore not only useful to the audience for which it is primarily intended: it will also be of use to those wishing to learn something about computational morphology, and would be an excellent teaching aid for a computational linguistics course.The book is divided into seven chapters and three appendices.
method_label: Chapter 1 traces the history of computational morphology.
result_label: As is typical of such overviews, the scope is somewhat narrow, discussing only a few Finnish projects (besides the work of Koskenniemi and his apostles), and work done at the Summer Institute of Linguistics.
background_label: That criticism aside, the rest of the chapter is an accurate overview of the positive and negative aspects of KIMMO.
background_label: On the negative side, Antworth cites the well-known fact that KIMMO cannot elegantly handle nonconcatenative morphology such as infixation or reduplication.
background_label: He also notes limitations more particular to the PC-KIMMO implementation, such as the lack of a rule compiler and the lack of a sensible (i.e., non-finite-state) model of morphotactics; both of these issues have received treatment within the two-level school (Dalrymple et al.
background_label: 1987 , Bear 1986 , inter alia), though with respect to rule compilation, Antworth is justified in noting (p. 11) that "it remains to be seen if such a project is feasible for small computers.
method_label: "Chapter 2 is an introduction to PC-KIMMO, and gives a sample session of its use.
method_label: Chapters 3 through 5 describe in detail how finite state transducers (FSTs) work, 1 The software runs on IBM PCs and compatibles running MS-DOS or PC-DOS version 2.0 or higher; minimum requirements are 256K memory (will use up to 640K); executable is about 97.5K.
result_label: Versions are available for the Macintosh, and the system has been compiled and tested under Unix System V and 4.2 BSD Unix.

===================================
paper_id: 8237688; YEAR: 1999
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: A Knowledge-Free Method For Capitalized Word Disambiguation
ABSTRACT: background_label: In this paper we present an approach to the disambiguation of capitalized words when they are used in the positions where capitalization is expected, such as the first word in a sentence or after a period, quotes, etc..
background_label: Such words can act as proper names or can be just capitalized variants of common words.
method_label: The main feature of our approach is that it uses a minimum of prebuilt resources and tires to dynamically infer the disambiguation clues from the entire document.
result_label: The approach was thoroughly tested and achieved about 98.5% accuracy on unseen texts from The New York Times 1996 corpus.

===================================
paper_id: 7838719; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Extending hybrid word-character neural machine translation with multi-task learning of morphological analysis
ABSTRACT: background_label: AbstractThis article describes the Aalto University entry to the English-to-Finnish news translation shared task in WMT 2017.
background_label: Our system is an open vocabulary neural machine translation (NMT) system, adapted to the needs of a morphologically complex target language.
result_label: The main contributions of this paper are 1) implicitly incorporating morphological information to NMT through multi-task learning, 2) adding an attention mechanism to the character-level decoder, combined with character segmentation of names, and 3) a new overattending penalty to beam search.

===================================
paper_id: 1157497; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf
TITLE: Semantic word cloud generation based on word embeddings
ABSTRACT: background_label: Word clouds have been widely used to present the contents and themes in the text for summary and visualization.
objective_label: In this paper, we propose a new semantic word cloud taking into account the word semantic meanings.
method_label: Distributed word representation is applied to accurately describe the semantic meaning of words, and a word similarity graph is constructed based on the semantic distance between words to lay out words in a more compact and aesthetic manner.
method_label: Word-related interactions are introduced to guide users fast read and understand the text.
method_label: We apply the proposed word cloud to user generated reviews in different fields to demonstrate the effectiveness of our method.

===================================
paper_id: 15855502; YEAR: 2010
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Self-Training without Reranking for Parser Domain Adaptation and Its Impact on Semantic Role Labeling
ABSTRACT: background_label: AbstractWe compare self-training with and without reranking for parser domain adaptation, and examine the impact of syntactic parser adaptation on a semantic role labeling system.
method_label: Although self-training without reranking has been found not to improve in-domain accuracy for parsers trained on the WSJ Penn Treebank, we show that it is surprisingly effective for parser domain adaptation.
result_label: We also show that simple self-training of a syntactic parser improves out-of-domain accuracy of a semantic role labeler.

===================================
paper_id: 201124803; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: On the Robustness of Unsupervised and Semi-supervised Cross-lingual Word Embedding Learning
ABSTRACT: background_label: Cross-lingual word embeddings are vector representations of words in different languages where words with similar meaning are represented by similar vectors, regardless of the language.
background_label: Recent developments which construct these embeddings by aligning monolingual spaces have shown that accurate alignments can be obtained with little or no supervision.
background_label: However, the focus has been on a particular controlled scenario for evaluation, and there is no strong evidence on how current state-of-the-art systems would fare with noisy text or for language pairs with major linguistic differences.
method_label: In this paper we present an extensive evaluation over multiple cross-lingual embedding models, analyzing their strengths and limitations with respect to different variables such as target language, training corpora and amount of supervision.
result_label: Our conclusions put in doubt the view that high-quality cross-lingual embeddings can always be learned without much supervision.

===================================
paper_id: 3331110; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: Building a Word Segmenter for Sanskrit Overnight
ABSTRACT: background_label: There is an abundance of digitised texts available in Sanskrit.
background_label: However, the word segmentation task in such texts are challenging due to the issue of 'Sandhi'.
background_label: In Sandhi, words in a sentence often fuse together to form a single chunk of text, where the word delimiter vanishes and sounds at the word boundaries undergo transformations, which is also reflected in the written text.
method_label: Here, we propose an approach that uses a deep sequence to sequence (seq2seq) model that takes only the sandhied string as the input and predicts the unsandhied string.
method_label: The state of the art models are linguistically involved and have external dependencies for the lexical and morphological analysis of the input.
method_label: Our model can be trained"overnight"and be used for production.
result_label: In spite of the knowledge lean approach, our system preforms better than the current state of the art by gaining a percentage increase of 16.79 % than the current state of the art.

===================================
paper_id: 830429; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Unsupervised learning of object semantic parts from internal states of CNNs by population encoding
ABSTRACT: background_label: We address the key question of how object part representations can be found from the internal states of CNNs that are trained for high-level tasks, such as object classification.
objective_label: This work provides a new unsupervised method to learn semantic parts and gives new understanding of the internal representations of CNNs.
method_label: Our technique is based on the hypothesis that semantic parts are represented by populations of neurons rather than by single filters.
method_label: We propose a clustering technique to extract part representations, which we call Visual Concepts.
result_label: We show that visual concepts are semantically coherent in that they represent semantic parts, and visually coherent in that corresponding image patches appear very similar.
background_label: Also, visual concepts provide full spatial coverage of the parts of an object, rather than a few sparse parts as is typically found in keypoint annotations.
method_label: Furthermore, We treat single visual concept as part detector and evaluate it for keypoint detection using the PASCAL3D+ dataset and for part detection using our newly annotated ImageNetPart dataset.
method_label: The experiments demonstrate that visual concepts can be used to detect parts.
method_label: We also show that some visual concepts respond to several semantic parts, provided these parts are visually similar.
result_label: Thus visual concepts have the essential properties: semantic meaning and detection capability.
result_label: Note that our ImageNetPart dataset gives rich part annotations which cover the whole object, making it useful for other part-related applications.

===================================
paper_id: 41069201; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: A Comparative Study to Understanding about Poetics Based on Natural Language Processing
ABSTRACT: objective_label: This paper tries to find out five poets’ (Thomas Hardy, Wilde, Browning, Yeats, and Tagore) differences and similarities through analyzing their works on nineteenth Century by using natural language understanding technology and word vector model.
method_label: Firstly, we collect enough poems from these five poets, build five corpus respectively, and calculate their high-frequency words, by using Natural Language Processing method.
method_label: Then, based on the word vector model, we calculate the word vectors of the five poets’ high-frequency words, and combine the word vectors of each poet into one vector.
method_label: Finally, we analyze the similarity between the combined word vectors by using the hierarchical clustering method.
result_label: The result shows that the poems of Hardy, Browning, and Wilde are similar; the poems of Tagore and Yeats are relatively close—but the gap between the two is relatively large.
result_label: In addition, we evaluate the stability of our approach by altering the word vector dimension, and try to analyze the results of clustering in a literary (poetic) perspective.
result_label: Yeats and Tagore possessed a kind of mysticism poetics thought, while Hardy, Browning, and Wilde have the elements of realism combined with tragedy and comedy.
result_label: The results are similar comparing to those we get from the word vector model.

===================================
paper_id: 30574734; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Effective articulatory modeling for pronunciation error detection of L2 learner without non-native training data
ABSTRACT: background_label: For effective articulatory feedback in computer-assisted pronunciation training (CAPT) systems, we address effective articulatory models of second language (L2) learners' speech without using such data, which is difficult to collect and annotate in a large scale.
background_label: Context-dependent articulatory attributes (placement and manner of articulation) are modeled based on deep neural network (DNN).
method_label: In order to efficiently train the non-native articulatory models, we exploit large speech corpora of native and target language to model inter-language phenomena.
method_label: This multi-lingual learning is then combined with multi-task learning, which uses phone-classification as a sub-task.
method_label: These methods are applied to Mandarin Chinese pronunciation learning by Japanese native speakers.
result_label: Effects are confirmed in the native attribute classification and pronunciation error detection of non-native speech.

===================================
paper_id: 1524421; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidf
TITLE: Morphological Priors for Probabilistic Neural Word Embeddings
ABSTRACT: background_label: Word embeddings allow natural language processing systems to share statistical information across related words.
background_label: These embeddings are typically based on distributional statistics, making it difficult for them to generalize to rare or unseen words.
objective_label: We propose to improve word embeddings by incorporating morphological information, capturing shared sub-word features.
method_label: Unlike previous work that constructs word embeddings directly from morphemes, we combine morphological and distributional information in a unified probabilistic framework, in which the word embedding is a latent variable.
method_label: The morphological information provides a prior distribution on the latent word embeddings, which in turn condition a likelihood function over an observed corpus.
result_label: This approach yields improvements on intrinsic word similarity evaluations, and also in the downstream task of part-of-speech tagging.

===================================
paper_id: 2444688; YEAR: 1994
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: cited - title_tfidf - title_tfidfcbow200 - title_cbow200 - specter
TITLE: Constructing Lexical Transducers
ABSTRACT: background_label: A lexical transducer, first discussed in Karttunen, Kaplan and Zaenen 1992, is a specialised finite-state automaton that maps inflected surface forms to lexical forms, and vice versa.
background_label: The lexical form consists of a canonical representation of the word and a sequence of tags that show the morphological characteristics of the form in question and its syntactic category.
other_label: For example, a lexical transducer for French might relate the surface form veut to the lexical form vouloir+IndPr+SG+P3.
other_label: In order to map between these two forms, the transducer may contain a path like the one shown in Fig.
result_label: 1.

===================================
paper_id: 4857014; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: Aesthetical Attributes for Segmenting Arabic Word
ABSTRACT: background_label: The connected allograph representing calligraphic Arabic word does not appear individually in any calligraphic resource but in association with other letters all adapted to each other.
background_label: The graphic segmentation of the word by respecting aesthetical attributes indicating the grapheme of every letter is far from being an obvious task.
objective_label: The question consists in discovering every letter constituting the word, points of cutting which separate its grapheme from other constituents of word's shape.
method_label: The obtained segment must be a complete drawing of the represented letter.
method_label: This segmentation according to contextual graphic and qualitative criteria connecting the attached allograph will have to satisfy typographic constraints varying in conformity with the possibilities offered by the wanted technology.
objective_label: In this paper, we develop an approach for segmenting Arabic word from which the purpose is to extract graphemes respecting the design of Arabic letters such as it is in the calligraphic literature.
method_label: The procedure bases itself on the principle that the Arabic connected letters have a common part included in the cursive area, which must not be lost during the process of cutting.

===================================
paper_id: 91787832; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: Learning cellular morphology with neural networks
ABSTRACT: background_label: Reconstruction and annotation of volume electron microscopy data sets of brain tissue is challenging, but can reveal invaluable information about neuronal circuits.
background_label: Significant progress has recently been made in automated neuron reconstruction, as well as automated detection of synapses.
background_label: However, methods for automating the morphological analysis of nanometer-resolution reconstructions are less established, despite their diverse application possibilities.
method_label: Here, we introduce cellular morphology neural networks (CMNs), based on multi-view projections sampled from automatically reconstructed cellular fragments of arbitrary size and shape.
method_label: Using unsupervised training we inferred morphology embeddings ("Neuron2vec") of neuron reconstructions and trained CMNs to identify glia cells in a supervised classification paradigm which was used to resolve neuron reconstruction errors.
result_label: Finally, we demonstrate that CMNs can be used to identify subcellular compartments and the cell types of neuron reconstructions.

===================================
paper_id: 77391799; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200
TITLE: Unsupervised Quality Estimation Without Reference Corpus for Subtitle Machine Translation Using Word Embeddings
ABSTRACT: background_label: We demonstrate the potential for using aligned bilingual word embeddings to create an unsupervised method to evaluate machine translations without a need for parallel translation corpus or reference corpus.
method_label: We explain why movie subtitles differ from other text and share our experimental results conducted on them for four target languages (French, German, Portuguese and Spanish) with English source subtitles.
method_label: We propose a novel automated evaluation method of calculating edits (insertion, deletion, substitution and shifts) to indicate translation quality and human aided post edit requirements to perfect machine translation.

===================================
paper_id: 12643315; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidf - title_tfidfcbow200
TITLE: KNET: A General Framework for Learning Word Embedding using Morphological Knowledge
ABSTRACT: background_label: Neural network techniques are widely applied to obtain high-quality distributed representations of words, i.e., word embeddings, to address text mining, information retrieval, and natural language processing tasks.
background_label: Recently, efficient methods have been proposed to learn word embeddings from context that captures both semantic and syntactic relationships between words.
background_label: However, it is challenging to handle unseen words or rare words with insufficient context.
objective_label: In this paper, inspired by the study on word recognition process in cognitive psychology, we propose to take advantage of seemingly less obvious but essentially important morphological knowledge to address these challenges.
method_label: In particular, we introduce a novel neural network architecture called KNET that leverages both contextual information and morphological word similarity built based on morphological knowledge to learn word embeddings.
method_label: Meanwhile, the learning architecture is also able to refine the pre-defined morphological knowledge and obtain more accurate word similarity.
result_label: Experiments on an analogical reasoning task and a word similarity task both demonstrate that the proposed KNET framework can greatly enhance the effectiveness of word embeddings.

===================================
paper_id: 28950708; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_cbow200
TITLE: Sanskrit Sandhi Splitting using seq2(seq)2
ABSTRACT: background_label: AbstractIn Sanskrit, small words (morphemes) are combined to form compound words through a process known as Sandhi.
background_label: Sandhi splitting is the process of splitting a given compound word into its constituent morphemes.
background_label: Although rules governing word splitting exists in the language, it is highly challenging to identify the location of the splits in a compound word.
method_label: Though existing Sandhi splitting systems incorporate these pre-defined splitting rules, they have a low accuracy as the same compound word might be broken down in multiple ways to provide syntactically correct splits.In this research, we propose a novel deep learning architecture called Double Decoder RNN (DD-RNN), which (i) predicts the location of the split(s) with 95% accuracy, and (ii) predicts the constituent words (learning the Sandhi splitting rules) with 79.5% accuracy, outperforming the state-of-art by 20%.
result_label: Additionally, we show the generalization capability of our deep learning model, by showing competitive results in the problem of Chinese word segmentation, as well.

===================================
paper_id: 199442510; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200 - title_tfidfcbow200
TITLE: Semi-supervised Thai Sentence Segmentation Using Local and Distant Word Representations
ABSTRACT: background_label: A sentence is typically treated as the minimal syntactic unit used for extracting valuable information from a longer piece of text.
background_label: However, in written Thai, there are no explicit sentence markers.
objective_label: We proposed a deep learning model for the task of sentence segmentation that includes three main contributions.
method_label: First, we integrate n-gram embedding as a local representation to capture word groups near sentence boundaries.
method_label: Second, to focus on the keywords of dependent clauses, we combine the model with a distant representation obtained from self-attention modules.
method_label: Finally, due to the scarcity of labeled data, for which annotation is difficult and time-consuming, we also investigate and adapt Cross-View Training (CVT) as a semi-supervised learning technique, allowing us to utilize unlabeled data to improve the model representations.
method_label: In the Thai sentence segmentation experiments, our model reduced the relative error by 7.4% and 10.5% compared with the baseline models on the Orchid and UGWC datasets, respectively.
method_label: We also applied our model to the task of pronunciation recovery on the IWSLT English dataset.
result_label: Our model outperformed the prior sequence tagging models, achieving a relative error reduction of 2.5%.
result_label: Ablation studies revealed that utilizing n-gram presentations was the main contributing factor for Thai, while the semi-supervised training helped the most for English.

===================================
paper_id: 2609764; YEAR: 2000
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: An associational model of birdsong sensorimotor learning I. Efference copy and the learning of song syllables.
ABSTRACT: background_label: Birdsong learning provides an ideal model system for studying temporally complex motor behavior.
background_label: Guided by the well-characterized functional anatomy of the song system, we have constructed a computational model of the sensorimotor phase of song learning.
method_label: Our model uses simple Hebbian and reinforcement learning rules and demonstrates the plausibility of a detailed set of hypotheses concerning sensory-motor interactions during song learning.
method_label: The model focuses on the motor nuclei HVc and robust nucleus of the archistriatum (RA) of zebra finches and incorporates the long-standing hypothesis that a series of song nuclei, the Anterior Forebrain Pathway (AFP), plays an important role in comparing the bird's own vocalizations with a previously memorized song, or "template."
method_label: This "AFP comparison hypothesis" is challenged by the significant delay that would be experienced by presumptive auditory feedback signals processed in the AFP.
method_label: We propose that the AFP does not directly evaluate auditory feedback, but instead, receives an internally generated prediction of the feedback signal corresponding to each vocal gesture, or song "syllable."
method_label: This prediction, or "efference copy," is learned in HVc by associating premotor activity in RA-projecting HVc neurons with the resulting auditory feedback registered within AFP-projecting HVc neurons.
method_label: We also demonstrate how negative feedback "adaptation" can be used to separate sensory and motor signals within HVc.
method_label: The model predicts that motor signals recorded in the AFP during singing carry sensory information and that the primary role for auditory feedback during song learning is to maintain an accurate efference copy.
result_label: The simplicity of the model suggests that associational efference copy learning may be a common strategy for overcoming feedback delay during sensorimotor learning.

===================================
paper_id: 8210979; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Discovery of Visual Semantics by Unsupervised and Self-Supervised Representation Learning
ABSTRACT: background_label: The success of deep learning in computer vision is rooted in the ability of deep networks to scale up model complexity as demanded by challenging visual tasks.
background_label: As complexity is increased, so is the need for large amounts of labeled data to train the model.
background_label: This is associated with a costly human annotation effort.
method_label: To address this concern, with the long-term goal of leveraging the abundance of cheap unlabeled data, we explore methods of unsupervised"pre-training.
method_label: "In particular, we propose to use self-supervised automatic image colorization.
method_label: We show that traditional methods for unsupervised learning, such as layer-wise clustering or autoencoders, remain inferior to supervised pre-training.
method_label: In search for an alternative, we develop a fully automatic image colorization method.
background_label: Our method sets a new state-of-the-art in revitalizing old black-and-white photography, without requiring human effort or expertise.
method_label: Additionally, it gives us a method for self-supervised representation learning.
method_label: In order for the model to appropriately re-color a grayscale object, it must first be able to identify it.
method_label: This ability, learned entirely self-supervised, can be used to improve other visual tasks, such as classification and semantic segmentation.
method_label: As a future direction for self-supervision, we investigate if multiple proxy tasks can be combined to improve generalization.
method_label: This turns out to be a challenging open problem.
result_label: We hope that our contributions to this endeavor will provide a foundation for future efforts in making self-supervision compete with supervised pre-training.

===================================
paper_id: 564727; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf - abs_tfidfcbow200
TITLE: Alleviating Overfitting for Polysemous Words for Word Representation Estimation Using Lexicons
ABSTRACT: background_label: Though there are some works on improving distributed word representations using lexicons, the improper overfitting of the words that have multiple meanings is a remaining issue deteriorating the learning when lexicons are used, which needs to be solved.
method_label: An alternative method is to allocate a vector per sense instead of a vector per word.
method_label: However, the word representations estimated in the former way are not as easy to use as the latter one.
method_label: Our previous work uses a probabilistic method to alleviate the overfitting, but it is not robust with a small corpus.
method_label: In this paper, we propose a new neural network to estimate distributed word representations using a lexicon and a corpus.
background_label: We add a lexicon layer in the continuous bag-of-words model and a threshold node after the output of the lexicon layer.
background_label: The threshold rejects the unreliable outputs of the lexicon layer that are less likely to be the same with their inputs.
background_label: In this way, it alleviates the overfitting of the polysemous words.
method_label: The proposed neural network can be trained using negative sampling, which maximizing the log probabilities of target words given the context words, by distinguishing the target words from random noises.
method_label: We compare the proposed neural network with the continuous bag-of-words model, the other works improving it, and the previous works estimating distributed word representations using both a lexicon and a corpus.
result_label: The experimental results show that the proposed neural network is more efficient and balanced for both semantic tasks and syntactic tasks than the previous works, and robust to the size of the corpus.

===================================
paper_id: 67440317; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200
TITLE: Unsupervised Learning of Structure in Spectroscopic Cubes
ABSTRACT: background_label: We consider the problem of analyzing the structure of spectroscopic cubes using unsupervised machine learning techniques.
objective_label: We propose representing the target's signal as a homogeneous set of volumes through an iterative algorithm that separates the structured emission from the background while not overestimating the flux.
method_label: Besides verifying some basic theoretical properties, the algorithm is designed to be tuned by domain experts, because its parameters have meaningful values in the astronomical context.
method_label: Nevertheless, we propose a heuristic to automatically estimate the signal-to-noise ratio parameter of the algorithm directly from data.
method_label: The resulting light-weighted set of samples ($\leq 1\%$ compared to the original data) offer several advantages.
method_label: For instance, it is statistically correct and computationally inexpensive to apply well-established techniques of the pattern recognition and machine learning domains; such as clustering and dimensionality reduction algorithms.
method_label: We use ALMA science verification data to validate our method, and present examples of the operations that can be performed by using the proposed representation.
result_label: Even though this approach is focused on providing faster and better analysis tools for the end-user astronomer, it also opens the possibility of content-aware data discovery by applying our algorithm to big data.

===================================
paper_id: 5786016; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: A joint model of word segmentation and meaning acquisition through cross-situational learning.
ABSTRACT: background_label: Human infants learn meanings for spoken words in complex interactions with other people, but the exact learning mechanisms are unknown.
background_label: Among researchers, a widely studied learning mechanism is called cross-situational learning (XSL).
background_label: In XSL, word meanings are learned when learners accumulate statistical information between spoken words and co-occurring objects or events, allowing the learner to overcome referential uncertainty after having sufficient experience with individually ambiguous scenarios.
background_label: Existing models in this area have mainly assumed that the learner is capable of segmenting words from speech before grounding them to their referential meaning, while segmentation itself has been treated relatively independently of the meaning acquisition.
method_label: In this article, we argue that XSL is not just a mechanism for word-to-meaning mapping, but that it provides strong cues for proto-lexical word segmentation.
result_label: If a learner directly solves the correspondence problem between continuous speech input and the contextual referents being talked about, segmentation of the input into word-like units emerges as a by-product of the learning.
background_label: We present a theoretical model for joint acquisition of proto-lexical segments and their meanings without assuming a priori knowledge of the language.
method_label: We also investigate the behavior of the model using a computational implementation, making use of transition probability-based statistical learning.
result_label: Results from simulations show that the model is not only capable of replicating behavioral data on word learning in artificial languages, but also shows effective learning of word segments and their meanings from continuous speech.
result_label: Moreover, when augmented with a simple familiarity preference during learning, the model shows a good fit to human behavioral data in XSL tasks.
result_label: These results support the idea of simultaneous segmentation and meaning acquisition and show that comprehensive models of early word segmentation should take referential word meanings into account.
other_label: (PsycINFO Database Record

===================================
paper_id: 1793706; YEAR: 2006
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf - abs_tfidfcbow200
TITLE: Initial Explorations In English To Turkish Statistical Machine Translation
ABSTRACT: background_label: This paper presents some very preliminary results for and problems in developing a statistical machine translation system from English to Turkish.
method_label: Starting with a baseline word model trained from about 20K aligned sentences, we explore various ways of exploiting morphological structure to improve upon the baseline system.
method_label: As Turkish is a language with complex agglutinative word structures, we experiment with morphologically segmented and disambiguated versions of the parallel texts in order to also uncover relations between morphemes and function words in one language with morphemes and functions words in the other, in addition to relations between open class content words.
method_label: Morphological segmentation on the Turkish side also conflates the statistics from allomorphs so that sparseness can be alleviated to a certain extent.
result_label: We find that this approach coupled with a simple grouping of most frequent morphemes and function words on both sides improve the BLEU score from the baseline of 0.0752 to 0.0913 with the small training data.
result_label: We close with a discussion on why one should not expect distortion parameters to model word-local morpheme ordering and that a new approach to handling complex morphotactics is needed.

===================================
paper_id: 88482655; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Authorship Verification via k-Nearest Neighbor Estimation
ABSTRACT: method_label: In this paper we describe our k-Nearest Neighbor (k-NN) based Authorship Verification method for the Author Identification (AI) task of the PAN 2013 challenge.
method_label: The method follows an ensemble classification technique based on the combination of suitable feature categories.
method_label: For each chosen feature category we apply a k-NN classifier to calculate a style deviation score between the training documents of the true author A and the document from an author, who claims to be A.
method_label: Depending on the score and a given threshold, a decision for or against the alleged author is generated and stored into a list.
result_label: Afterwards, the final decision regarding the alleged authorship is determined through a majority vote among all decisions within this list.
background_label: The method provides a number of benefits as for instance the independence of linguistic resources like ontologies, thesauruses or even language models.
background_label: A further benefit is the language-independency among different Indo-European languages as the approach is applicable on languages like Spanish, English, Greek or German.
method_label: Another benefit is the low runtime of the method, since there is no need for deep linguistic processing like POS-tagging, chunking or parsing.
method_label: Moreover, the method can be extended or modified for instance by replacing the classification function, the threshold or the underlying features including their parameters (e.g.
method_label: n-Gram sizes or the amount of feature frequencies).
result_label: In addition to the PAN 2013 AI-training-corpus, where we gained an overall accuracy score of 80%, we also evaluated the algorithm on our own dataset with an accuracy of 77.50%.

===================================
paper_id: 1148525; YEAR: 1996
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200 - specter
TITLE: Unsupervised Discovery of Phonological Categories through Supervised Learning of Morphological Rules
ABSTRACT: background_label: We describe a case study in the application of {\em symbolic machine learning} techniques for the discovery of linguistic rules and categories.
method_label: A supervised rule induction algorithm is used to learn to predict the correct diminutive suffix given the phonological representation of Dutch nouns.
method_label: The system produces rules which are comparable to rules proposed by linguists.
method_label: Furthermore, in the process of learning this morphological task, the phonemes used are grouped into phonologically relevant categories.
method_label: We discuss the relevance of our method for linguistics and language technology.

===================================
paper_id: 6694945; YEAR: 2000
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_cbow200
TITLE: Extraction of semantic relations from a Basque monolingual dictionary using Constraint Grammar
ABSTRACT: objective_label: This paper deals with the exploitation of dictionaries for the semi-automatic construction of lexicons and lexical knowledge bases.
objective_label: The final goal of our research is to enrich the Basque Lexical Database with semantic information such as senses, definitions, semantic relations, etc., extracted from a Basque monolingual dictionary.
objective_label: The work here presented focuses on the extraction of the semantic relations that best characterise the headword, that is, those of synonymy, antonymy, hypernymy, and other relations marked by specific relators and derivation.
method_label: All nominal, verbal and adjectival entries were treated.
method_label: Basque uses morphological inflection to mark case, and therefore semantic relations have to be inferred from suffixes rather than from prepositions.
method_label: Our approach combines a morphological analyser and surface syntax parsing (based on Constraint Grammar), and has proven very successful for highly inflected languages such as Basque.
method_label: Both the effort to write the rules and the actual processing time of the dictionary have been very low.
result_label: At present we have extracted 42,533 relations, leaving only 2,943 (9%) definitions without any extracted relation.
result_label: The error rate is extremely low, as only 2.2% of the extracted relations are wrong.

===================================
paper_id: 174801119; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Shrinking Japanese Morphological Analyzers With Neural Networks and Semi-supervised Learning
ABSTRACT: background_label: AbstractFor languages without natural word boundaries, like Japanese and Chinese, word segmentation is a prerequisite for downstream analysis.
background_label: For Japanese, segmentation is often done jointly with part of speech tagging, and this process is usually referred to as morphological analysis.
method_label: Morphological analyzers are trained on data hand-annotated with segmentation boundaries and part of speech tags.
method_label: A segmentation dictionary or character n-gram information is also provided as additional inputs to the model.
result_label: Incorporating this extra information makes models large.
background_label: Modern neural morphological analyzers can consume gigabytes of memory.
objective_label: We propose a compact alternative to these cumbersome approaches which do not rely on any externally provided n-gram or word representations.
method_label: The model uses only unigram character embeddings, encodes them using either stacked bi-LSTM or a self-attention network, and independently infers both segmentation and part of speech information.
method_label: The model is trained in an end-to-end and semisupervised fashion, on labels produced by a state-of-the-art analyzer.
method_label: We demonstrate that the proposed technique rivals performance of a previous dictionary-based state-of-the-art approach and can even surpass it when training with the combination of human-annotated and automatically-annotated data.
result_label: Our model itself is significantly smaller than the dictionarybased one: it uses less than 15 megabytes of space.

===================================
paper_id: 3829104; YEAR: 2011
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200
TITLE: Semantic Clustering: an Attempt to Identify Multiword Expressions in Bengali
ABSTRACT: background_label: AbstractOne of the key issues in both natural language understanding and generation is the appropriate processing of Multiword Expressions (MWEs).
background_label: MWE can be defined as a semantic issue of a phrase where the meaning of the phrase may not be obtained from its constituents in a straightforward manner.
method_label: This paper presents an approach of identifying bigram noun-noun MWEs from a medium-size Bengali corpus by clustering the semantically related nouns and incorporating a vector space model for similarity measurement.
method_label: Additional inclusion of the English WordNet::Similarity module also improves the results considerably.
method_label: The present approach also contributes to locate clusters of the synonymous noun words present in a document.
result_label: Experimental results draw a satisfactory conclusion after analyzing the Precision, Recall and F-score values.

===================================
paper_id: 149449424; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: specter
TITLE: Neural Morphological Disambiguation Using Surface and Contextual Morphological Awareness
ABSTRACT: background_label: AbstractMorphological disambiguation, particularly for morphologically rich languages, is a crucial step in many NLP tasks.
background_label: Morphological analyzers provide multiple analyses of a word, only one of which is true in context.
method_label: We present a language-agnostic deep neural system for morphological disambiguation, with experiments on Hindi.
method_label: We achieve accuracies of around 95.22% without the use of any language-specific features or heuristics, which outperforms the existing state of the art.
objective_label: One contribution through this work is building the first morphological disambiguation system for Hindi.
method_label: We also show that using phonological features can improve performance.
result_label: On using phonological features and pre-trained word vectors, we report an accuracy of 97.02% for Hindi.

===================================
paper_id: 14026227; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: title_cbow200
TITLE: Semi-supervised learning of morphological paradigms and lexicons
ABSTRACT: background_label: We present a semi-supervised approach to the problem of paradigm induction from inflection tables.
background_label: Our system extracts generalizations from inflection tables, representing the resulting paradigms in an abstract form.
objective_label: The process is intended to be language-independent, and to provide human-readable generalizations of paradigms.
method_label: The tools we provide can be used by linguists for the rapid creation of lexical resources.
method_label: We evaluate the system through an inflection table reconstruction task using Wiktionary data for German, Spanish, and Finnish.
result_label: With no additional corpus information available, the evaluation yields per word form accuracy scores on inflecting unseen base forms in different languages ranging from 87.81% (German nouns) to 99.52% (Spanish verbs); with additional unlabeled text corpora available for training the scores range from 91.81% (German nouns) to 99.58% (Spanish verbs).
result_label: We separately evaluate the system in a simulated task of Swedish lexicon creation, and show that on the basis of a small number of inflection tables, the system can accurately collect from a list of noun forms a lexicon with inflection information ranging from 100.0% correct (collect 100 words), to 96.4% correct (collect 1000 words).

===================================
paper_id: 13647555; YEAR: 2003
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf - abs_cbow200
TITLE: Unsupervised Word Segmentation Without Dictionary
ABSTRACT: background_label: This prototype system demonstrates a novel method of word segmentation based on corpus statistics.
background_label: Since the central technique we used is unsupervised training based on a large corpus, we refer to this approach as unsupervised word segmentation.The unsupervised approach is general in scope and can be applied to both Mandarin Chinese and Taiwanese.
method_label: In this prototype, we illustrate its use in word segmentation of Taiwanese Hank and Church (1990) and Sproat and Shih (1990) .
method_label: If A and B have a relatively high MI that is over a certain threshold, we prefer to identify AB as a word over those having lower MI values.
method_label: In the experiment with Taiwanese Bible, the system identified Hanzi and Romanized syllables.
method_label: Out of those, we obtained pairs of consecutive single or double Hanzi characters and Romanized syllables.
method_label: So those pairs are commonly known as character bigrams, trigrams, and fourgrams.
method_label: We differed from the common N-gram calculation and treated those as pairs of character sequence in order to apply mutual information statistics.
result_label: Table 1 shows some examples of the pairs and MI values.
result_label: We have excluded pairs having MI 2.2 or lower.

===================================
paper_id: 2554708; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: A Multigraph Representation for Improved Unsupervised/Semi-supervised Learning of Human Actions
ABSTRACT: background_label: Graph-based methods are a useful class of methods for improving the performance of unsupervised and semi-supervised machine learning tasks, such as clustering or information retrieval.
background_label: However, the performance of existing graph-based methods is highly dependent on how well the affinity graph reflects the original data structure.
objective_label: We propose that multimedia such as images or videos consist of multiple separate components, and therefore more than one graph is required to fully capture the relationship between them.
method_label: Accordingly, we present a new spectral method - the Feature Grouped Spectral Multigraph (FGSM) - which comprises the following steps.
method_label: First, mutually independent subsets of the original feature space are generated through feature clustering.
method_label: Secondly, a separate graph is generated from each feature subset.
method_label: Finally, a spectral embedding is calculated on each graph, and the embeddings are scaled/aggregated into a single representation.
result_label: Using this representation, a variety of experiments are performed on three learning tasks - clustering, retrieval and recognition - on human action datasets, demonstrating considerably better performance than the state-of-the-art.

===================================
paper_id: 56517214; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Unsupervised Meta-learning of Figure-Ground Segmentation via Imitating Visual Effects
ABSTRACT: background_label: This paper presents a"learning to learn"approach to figure-ground image segmentation.
method_label: By exploring webly-abundant images of specific visual effects, our method can effectively learn the visual-effect internal representations in an unsupervised manner and uses this knowledge to differentiate the figure from the ground in an image.
method_label: Specifically, we formulate the meta-learning process as a compositional image editing task that learns to imitate a certain visual effect and derive the corresponding internal representation.
method_label: Such a generative process can help instantiate the underlying figure-ground notion and enables the system to accomplish the intended image segmentation.
method_label: Whereas existing generative methods are mostly tailored to image synthesis or style transfer, our approach offers a flexible learning mechanism to model a general concept of figure-ground segmentation from unorganized images that have no explicit pixel-level annotations.
result_label: We validate our approach via extensive experiments on six datasets to demonstrate that the proposed model can be end-to-end trained without ground-truth pixel labeling yet outperforms the existing methods of unsupervised segmentation tasks.

===================================
paper_id: 8425601; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: UWaterloo at SemEval-2017 Task 7: Locating the Pun Using Syntactic Characteristics and Corpus-based Metrics
ABSTRACT: objective_label: AbstractThe paper presents a system for locating a pun word.
method_label: The developed method calculates a score for each word in a pun, using a number of components, including its Inverse Document Frequency (IDF), Normalized Pointwise Mutual Information (NPMI) with other words in the pun text, its position in the text, part-ofspeech and some syntactic features.
method_label: The method achieved the best performance in the Heterographic category and the second best in the Homographic.
result_label: Further analysis showed that IDF is the most useful characteristic, whereas the count of words with which the given word has high NPMI has a negative effect on performance.

===================================
paper_id: 28765259; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: specter
TITLE: Character-Aware Neural Morphological Disambiguation
ABSTRACT: background_label: AbstractWe develop a language-independent, deep learning-based approach to the task of morphological disambiguation.
objective_label: Guided by the intuition that the correct analysis should be "most similar" to the context, we propose dense representations for morphological analyses and surface context and a simple yet effective way of combining the two to perform disambiguation.
method_label: Our approach improves on the languagedependent state of the art for two agglutinative languages (Turkish and Kazakh) and can be potentially applied to other morphologically complex languages.

===================================
paper_id: 16674049; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: Autoconvolution for Unsupervised Feature Learning
ABSTRACT: background_label: AbstractIn visual recognition tasks, supervised learning shows excellent performance.
background_label: On the other hand, unsupervised learning exploits cheap unlabeled data and can help to solve the same tasks more efficiently.
method_label: We show that the recursive autoconvolutional operator, adopted from physics, boosts existing unsupervised methods to learn more powerful filters.
method_label: We use a well established multilayer convolutional network and train filters layer-wise.
method_label: To build a stronger classifier, we design a very light committee of SVM models.
method_label: The total number of trainable parameters is also greatly reduced by using shared filters in higher layers.
result_label: We evaluate our networks on the MNIST, CIFAR-10 and STL-10 benchmarks and report several state of the art results among other unsupervised methods.

===================================
paper_id: 17568696; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200
TITLE: Identification of live or studio versions of a song via supervised learning
ABSTRACT: objective_label: We aim to distinguish between the “live” and “studio” versions of songs by using supervised techniques.
background_label: We show which segments of a song are the most relevant to this classification task, and we also discuss the relative importance of audio, music and acoustic features, given this challenge.
background_label: This distinction is crucial in practice since the listening experience of the user of online streaming services is often affected, depending on whether the song played is the original studio version or a secondary live recording.
background_label: However, manual labelling can be tedious and challenging.
method_label: Therefore, we propose to classify automatically a music data set by using Machine Learning techniques under a supervised setting.
result_label: To the best of our knowledge, this issue has never been addressed before.
result_label: Our proposed system is proven to perform with high accuracy on a 1066-song data set with distinct genres and across different languages.

===================================
paper_id: 20255210; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200
TITLE: Train-O-Matic: Large-Scale Supervised Word Sense Disambiguation in Multiple Languages without Manual Training Data
ABSTRACT: background_label: AbstractAnnotating large numbers of sentences with senses is the heaviest requirement of current Word Sense Disambiguation.
method_label: We present Train-O-Matic, a languageindependent method for generating millions of sense-annotated training instances for virtually all meanings of words in a language's vocabulary.
method_label: The approach is fully automatic: no human intervention is required and the only type of human knowledge used is a WordNet-like resource.
method_label: Train-O-Matic achieves consistently state-of-the-art performance across gold standard datasets and languages, while at the same time removing the burden of manual annotation.
other_label: All the training data is available for research purposes at http://trainomatic.org.

===================================
paper_id: 2211971; YEAR: 2004
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: Word Sense Disambiguation by Web Mining for Word Co-occurrence Probabilities
ABSTRACT: background_label: This paper describes the National Research Council (NRC) Word Sense Disambiguation (WSD) system, as applied to the English Lexical Sample (ELS) task in Senseval-3.
method_label: The NRC system approaches WSD as a classical supervised machine learning problem, using familiar tools such as the Weka machine learning software and Brill's rule-based part-of-speech tagger.
method_label: Head words are represented as feature vectors with several hundred features.
method_label: Approximately half of the features are syntactic and the other half are semantic.
method_label: The main novelty in the system is the method for generating the semantic features, based on word \hbox{co-occurrence} probabilities.
result_label: The probabilities are estimated using the Waterloo MultiText System with a corpus of about one terabyte of unlabeled text, collected by a web crawler.

===================================
paper_id: 14544878; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_tfidf
TITLE: A Game-Theoretic Approach to Word Sense Disambiguation
ABSTRACT: background_label: This paper presents a new model for word sense disambiguation formulated in terms of evolutionary game theory, where each word to be disambiguated is represented as a node on a graph whose edges represent word relations and senses are represented as classes.
background_label: The words simultaneously update their class membership preferences according to the senses that neighboring words are likely to choose.
method_label: We use distributional information to weigh the influence that each word has on the decisions of the others and semantic similarity information to measure the strength of compatibility among the choices.
method_label: With this information we can formulate the word sense disambiguation problem as a constraint satisfaction problem and solve it using tools derived from game theory, maintaining the textual coherence.
method_label: The model is based on two ideas: similar words should be assigned to similar classes and the meaning of a word does not depend on all the words in a text but just on some of them.
method_label: The paper provides an in-depth motivation of the idea of modeling the word sense disambiguation problem in terms of game theory, which is illustrated by an example.
result_label: The conclusion presents an extensive analysis on the combination of similarity measures to use in the framework and a comparison with state-of-the-art systems.
result_label: The results show that our model outperforms state-of-the-art algorithms and can be applied to different tasks and in different scenarios.

===================================
paper_id: 91184364; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: HoloGAN: Unsupervised learning of 3D representations from natural images
ABSTRACT: background_label: We propose a novel generative adversarial network (GAN) for the task of unsupervised learning of 3D representations from natural images.
background_label: Most generative models rely on 2D kernels to generate images and make few assumptions about the 3D world.
background_label: These models therefore tend to create blurry images or artefacts in tasks that require a strong 3D understanding, such as novel-view synthesis.
method_label: HoloGAN instead learns a 3D representation of the world, and to render this representation in a realistic manner.
method_label: Unlike other GANs, HoloGAN provides explicit control over the pose of generated objects through rigid-body transformations of the learnt 3D features.
method_label: Our experiments show that using explicit 3D features enables HoloGAN to disentangle 3D pose and identity, which is further decomposed into shape and appearance, while still being able to generate images with similar or higher visual quality than other generative models.
method_label: HoloGAN can be trained end-to-end from unlabelled 2D images only.
method_label: Particularly, we do not require pose labels, 3D shapes, or multiple views of the same objects.
result_label: This shows that HoloGAN is the first generative model that learns 3D representations from natural images in an entirely unsupervised manner.

