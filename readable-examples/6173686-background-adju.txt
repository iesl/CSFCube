======================================================================
paper_id: 6173686; YEAR: 2001
TITLE: ABL: Alignment-Based Learning
ABSTRACT: background_label: This paper introduces a new type of grammar learning algorithm, inspired by string edit distance (Wagner and Fischer, 1974).
method_label: The algorithm takes a corpus of flat sentences as input and returns a corpus of labelled, bracketed sentences.
method_label: The method works on pairs of unstructured sentences that have one or more words in common.
method_label: When two sentences are divided into parts that are the same in both sentences and parts that are different, this information is used to find parts that are interchangeable.
method_label: These parts are taken as possible constituents of the same type.
method_label: After this alignment learning step, the selection learning step selects the most probable constituents from all possible constituents.
method_label: This method was used to bootstrap structure on the ATIS corpus (Marcus et al., 1993) and on the OVIS (Openbaar Vervoer Informatie Systeem (OVIS) stands for Public Transport Information System.)
method_label: corpus (Bonnema et al., 1997).
result_label: While the results are encouraging (we obtained up to 89.25 % non-crossing brackets precision), this paper will point out some of the shortcomings of our approach and will suggest possible solutions.
===================================
paper_id: 726421; YEAR: 2001
adju relevance: Identical (+3)
difference: 0; annotator1: 3; annotator3: 3
sources: specter - abs_cbow200 - abs_tfidf
TITLE: Bootstrapping Structure using Similarity
ABSTRACT: background_label: In this paper a new similarity-based learning algorithm, inspired by string edit-distance (Wagner and Fischer, 1974), is applied to the problem of bootstrapping structure from scratch.
method_label: The algorithm takes a corpus of unannotated sentences as input and returns a corpus of bracketed sentences.
method_label: The method works on pairs of unstructured sentences or sentences partially bracketed by the algorithm that have one or more words in common.
method_label: It finds parts of sentences that are interchangeable (i.e.
method_label: the parts of the sentences that are different in both sentences).
method_label: These parts are taken as possible constituents of the same type.
method_label: While this corresponds to the basic bootstrapping step of the algorithm, further structure may be learned from comparison with other (similar) sentences.
method_label: We used this method for bootstrapping structure from the flat sentences of the Penn Treebank ATIS corpus, and compared the resulting structured sentences to the structured sentences in the ATIS corpus.
method_label: Similarly, the algorithm was tested on the OVIS corpus.
result_label: We obtained 86.04 % non-crossing brackets precision on the ATIS corpus and 89.39 % non-crossing brackets precision on the OVIS corpus.

===================================
paper_id: 1645458; YEAR: 2000
adju relevance: Identical (+3)
difference: 0; annotator1: 3; annotator3: 3
sources: title_tfidf - title_cbow200 - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - specter
TITLE: ABL: Alignment-Based Learning
ABSTRACT: background_label: AbstractThis ])al)er introdu(:es a new tyl)e of grammar learning algorithm, iilst)ired l)y sl;ring edit distance (Wagner and Fis(:her, 1974).
method_label: The algorithm takes a (:ortms of tlat S(~lltell(:es as input and returns a (:ortms of lat)elled, l)ra(:ket(~(1 sen-~ ten(:(~s.
method_label: The method works on 1)airs of unstru(:-tllr(?
other_label: (l SelltellC(~,s that have one or more words in (:onunon.
method_label: W]lc, ll two senten('es are (tivided into parts that are the same in 1)oth s(mten(:es and parl;s tha|; are (litl'erent, this intbrmation is used to lind l)arl;s that are interchangeal)le.
result_label: These t)arts are taken as t)ossil)le (:onstituents of the same tyl)e. After this alignment learning stel) , the selection learning stc l) sel(~('ts the most l)rot)-at)le constituents from all 1)ossit)le (:onstituents.
method_label: This m(;thod was used to t)ootstrat) structure (m the ATIS (:ortms (Mar(:us et al., 1f)93) and on the OVIS ~ (:ort)us (Bommma et ~d., 1997).
result_label: While the results are en(:om:aging (we ol)|;ained Ul) to 89.25 % non-crossing l)ra(:kets precision), this 1)at)er will 1)oint out some of the shortcomings of our at)l)roa(:h and will suggest 1)ossible solul;ions.

===================================
paper_id: 14905234; YEAR: 1997
adju relevance: Similar (+2)
difference: 0; annotator1: 2; annotator3: 2
sources: abs_tfidf
TITLE: A DOP Model for Semantic Interpretation
ABSTRACT: background_label: In data-oriented language processing, an annotated language corpus is used as a stochastic grammar.
background_label: The most probable analysis of a new sentence is constructed by combining fragments from the corpus in the most probable way.
method_label: This approach has been successfully used for syntactic analysis, using corpora with syntactic annotations such as the Penn Tree-bank.
method_label: If a corpus with semantically annotated sentences is used, the same approach can also generate the most probable semantic interpretation of an input sentence.
method_label: The present paper explains this semantic interpretation method.
method_label: A data-oriented semantic interpretation algorithm was tested on two semantically annotated corpora: the English ATIS corpus and the Dutch OVIS corpus.
result_label: Experiments show an increase in semantic accuracy if larger corpus-fragments are taken into consideration.

===================================
paper_id: 6708408; YEAR: 2009
adju relevance: Similar (+2)
difference: 2; annotator1: 0; annotator3: 2
sources: title_cbow200 - title_tfidf
TITLE: Sampling-based Multilingual Alignment
ABSTRACT: background_label: AbstractWe present a sub-sentential alignment method that extracts high quality multi-word alignments from sentence-aligned multilingual parallel corpora.
method_label: Unlike other methods, it exploits low frequency terms, which makes it highly scalable.
method_label: As it relies on alingual concepts, it can process any number of languages at once.
result_label: Experiments have shown that it is competitive with state-of-the-art methods.

===================================
paper_id: 17900194; YEAR: 1996
adju relevance: Similar (+2)
difference: 2; annotator1: 1; annotator3: 3
sources: abs_tfidf
TITLE: A Data-Oriented Approach to Semantic Interpretation
ABSTRACT: background_label: In Data-Oriented Parsing (DOP), an annotated language corpus is used as a stochastic grammar.
background_label: The most probable analysis of a new input sentence is constructed by combining sub-analyses from the corpus in the most probable way.
method_label: This approach has been succesfully used for syntactic analysis, using corpora with syntactic annotations such as the Penn Treebank.
method_label: If a corpus with semantically annotated sentences is used, the same approach can also generate the most probable semantic interpretation of an input sentence.
method_label: The present paper explains this semantic interpretation method, and summarizes the results of a preliminary experiment.
method_label: Semantic annotations were added to the syntactic annotations of most of the sentences of the ATIS corpus.
result_label: A data-oriented semantic interpretation algorithm was succesfully tested on this semantically enriched corpus.

===================================
paper_id: 17563796; YEAR: 2002
adju relevance: Similar (+2)
difference: 2; annotator1: 1; annotator3: 3
sources: abs_tfidfcbow200 - title_cbow200 - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - title_tfidf - specter
TITLE: Bootstrapping Structure into Language: Alignment-Based Learning
ABSTRACT: background_label: This thesis introduces a new unsupervised learning framework, called Alignment-Based Learning, which is based on the alignment of sentences and Harris's (1951) notion of substitutability.
method_label: Instances of the framework can be applied to an untagged, unstructured corpus of natural language sentences, resulting in a labelled, bracketed version of that corpus.
method_label: Firstly, the framework aligns all sentences in the corpus in pairs, resulting in a partition of the sentences consisting of parts of the sentences that are equal in both sentences and parts that are unequal.
method_label: Unequal parts of sentences can be seen as being substitutable for each other, since substituting one unequal part for the other results in another valid sentence.
method_label: The unequal parts of the sentences are thus considered to be possible (possibly overlapping) constituents, called hypotheses.
result_label: Secondly, the selection learning phase considers all hypotheses found by the alignment learning phase and selects the best of these.
background_label: The hypotheses are selected based on the order in which they were found, or based on a probabilistic function.
background_label: The framework can be extended with a grammar extraction phase.
background_label: This extended framework is called parseABL.
method_label: Instead of returning a structured version of the unstructured input corpus, like the ABL system, this system also returns a stochastic context-free or tree substitution grammar.
method_label: Different instances of the framework have been tested on the English ATIS corpus, the Dutch OVIS corpus and the Wall Street Journal corpus.
result_label: One of the interesting results, apart from the encouraging numerical results, is that all instances can (and do) learn recursive structures.

===================================
paper_id: 696805; YEAR: 1992
adju relevance: Similar (+2)
difference: 1; annotator1: 2; annotator3: 3
sources: cited - abs_tfidf
TITLE: Inside-Outside Reestimation from Partially Bracketed Corpora
ABSTRACT: background_label: The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information in a partially parsed corpus.
background_label: Experiments on formal and natural language parsed corpora show that the new algorithm can achieve faster convergence and better modelling of hierarchical structure than the original one.
result_label: In particular, over 90% of the constituents in the most likely analyses of a test set are compatible with test set constituents for a grammar trained on a corpus of 700 hand-parsed part-of-speech strings for ATIS sentences.

===================================
paper_id: 13342424; YEAR: 1993
adju relevance: Similar (+2)
difference: 0; annotator1: 2; annotator3: 2
sources: abs_tfidf
TITLE: Parsing the Wall Street Journal with the Inside-Outside Algorithm
ABSTRACT: background_label: We report grammar inference experiments on partially parsed sentences taken from the Wall Street Journal corpus using the inside-outside algorithm for stochastic context-free grammars.
background_label: The initial grammar for the inference process makes no assumption of the kinds of structures and their distributions.
method_label: The inferred grammar is evaluated by its predicting power and by comparing the bracketing of held out sentences imposed by the inferred grammar with the partial bracketings of these sentences given in the corpus.
result_label: Using part-of-speech tags as the only source of lexical information, high bracketing accuracy is achieved even with a small subset of the available training material (1045 sentences): 94.4% for test sentences shorter than 10 words and 90.2% for sentences shorter than 15 words.

===================================
paper_id: 1805910; YEAR: 2001
adju relevance: Similar (+2)
difference: 3; annotator1: 0; annotator3: 3
sources: specter - title_cbow200 - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - title_tfidf
TITLE: Bootstrapping Syntax and Recursion using Alignment-Based Learning
ABSTRACT: background_label: This paper introduces a new type of unsupervised learning algorithm, based on the alignment of sentences and Harris's (1951) notion of interchangeability.
method_label: The algorithm is applied to an untagged, unstructured corpus of natural language sentences, resulting in a labelled, bracketed version of the corpus.
method_label: Firstly, the algorithm aligns all sentences in the corpus in pairs, resulting in a partition of the sentences consisting of parts of the sentences that are similar in both sentences and parts that are dissimilar.
method_label: This information is used to find (possibly overlapping) constituents.
method_label: Next, the algorithm selects (non-overlapping) constituents.
method_label: Several instances of the algorithm are applied to the ATIS corpus (Marcus et al., 1993) and the OVIS (Openbaar Vervoer Informatie Systeem (OVIS) stands for Public Transport Information System.)
method_label: corpus (Bonnema et al., 1997).
result_label: Apart from the promising numerical results, the most striking result is that even the simplest algorithm based on alignment learns recursion.

===================================
paper_id: 13381535; YEAR: 1974
adju relevance: Similar (+2)
difference: 1; annotator1: 3; annotator3: 2
sources: cited - abs_tfidf
TITLE: The String-to-String Correction Problem
ABSTRACT: objective_label: The string-to-string correction problem is to determine the distance between two strings as measured by the minimum cost sequence of “edit operations” needed to change the one string into the other.
method_label: The edit operations investigated allow changing one symbol of a string into another single symbol, deleting one symbol from a string, or inserting a single symbol into a string.
method_label: An algorithm is presented which solves this problem in time proportional to the product of the lengths of the two strings.
result_label: Possible applications are to the problems of automatic spelling correction and determining the longest subsequence of characters common to two strings.

===================================
paper_id: 5080881; YEAR: 2018
adju relevance: Related (+1)
difference: 1; annotator1: 0; annotator3: 1
sources: title_tfidfcbow200 - title_cbow200 - title_tfidf
TITLE: ALIGNet: Partial-Shape Agnostic Alignment via Unsupervised Learning
ABSTRACT: background_label: The process of aligning a pair of shapes is a fundamental operation in computer graphics.
background_label: Traditional approaches rely heavily on matching corresponding points or features to guide the alignment, a paradigm that falters when significant shape portions are missing.
background_label: These techniques generally do not incorporate prior knowledge about expected shape characteristics, which can help compensate for any misleading cues left by inaccuracies exhibited in the input shapes.
method_label: We present an approach based on a deep neural network, leveraging shape datasets to learn a shape-aware prior for source-to-target alignment that is robust to shape incompleteness.
method_label: In the absence of ground truth alignments for supervision, we train a network on the task of shape alignment using incomplete shapes generated from full shapes for self-supervision.
method_label: Our network, called ALIGNet, is trained to warp complete source shapes to incomplete targets, as if the target shapes were complete, thus essentially rendering the alignment partial-shape agnostic.
objective_label: We aim for the network to develop specialized expertise over the common characteristics of the shapes in each dataset, thereby achieving a higher-level understanding of the expected shape space to which a local approach would be oblivious.
method_label: We constrain ALIGNet through an anisotropic total variation identity regularization to promote piecewise smooth deformation fields, facilitating both partial-shape agnosticism and post-deformation applications.
method_label: We demonstrate that ALIGNet learns to align geometrically distinct shapes, and is able to infer plausible mappings even when the target shape is significantly incomplete.
result_label: We show that our network learns the common expected characteristics of shape collections, without over-fitting or memorization, enabling it to produce plausible deformations on unseen data during test time.

===================================
paper_id: 15734968; YEAR: 1996
adju relevance: Related (+1)
difference: 0; annotator1: 1; annotator3: 1
sources: cited - abs_tfidf
TITLE: Unsupervised Language Acquisition
ABSTRACT: background_label: This thesis presents a computational theory of unsupervised language acquisition, precisely defining procedures for learning language from ordinary spoken or written utterances, with no explicit help from a teacher.
background_label: The theory is based heavily on concepts borrowed from machine learning and statistical estimation.
background_label: In particular, learning takes place by fitting a stochastic, generative model of language to the evidence.
objective_label: Much of the thesis is devoted to explaining conditions that must hold for this general learning strategy to arrive at linguistically desirable grammars.
method_label: The thesis introduces a variety of technical innovations, among them a common representation for evidence and grammars, and a learning strategy that separates the ``content'' of linguistic parameters from their representation.
method_label: Algorithms based on it suffer from few of the search problems that have plagued other computational approaches to language acquisition.
method_label: The theory has been tested on problems of learning vocabularies and grammars from unsegmented text and continuous speech, and mappings between sound and representations of meaning.
result_label: It performs extremely well on various objective criteria, acquiring knowledge that causes it to assign almost exactly the same structure to utterances as humans do.
result_label: This work has application to data compression, language modeling, speech recognition, machine translation, information retrieval, and other tasks that rely on either structural or stochastic descriptions of language.

===================================
paper_id: 3204267; YEAR: 1996
adju relevance: Related (+1)
difference: 0; annotator1: 1; annotator3: 1
sources: abs_tfidf
TITLE: Learning Translation Rules From A Bilingual Corpus
ABSTRACT: objective_label: This paper proposes a mechanism for learning pattern correspondences between two languages from a corpus of translated sentence pairs.
objective_label: The proposed mechanism uses analogical reasoning between two translations.
method_label: Given a pair of translations, the similar parts of the sentences in the source language must correspond the similar parts of the sentences in the target language.
method_label: Similarly, the different parts should correspond to the respective parts in the translated sentences.
method_label: The correspondences between the similarities, and also differences are learned in the form of translation rules.
result_label: The system is tested on a small training dataset and produced promising results for further investigation.

===================================
paper_id: 60946181; YEAR: 1998
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200 - specter
TITLE: Corpus-based parsing and sublanguage studies
ABSTRACT: background_label: There are two main topics in this thesis, a corpus-based parser and a study of sublanguage.
objective_label: A novel approach to corpus-based parsing is proposed.
method_label: In this framework, a probabilistic grammar is constructed whose rules are partial trees from a syntactically-bracketed corpus.
method_label: The distinctive feature is that the partial trees are multi-layered.
method_label: In other words, only a small number of non-terminals are used to cut the initial trees; other grammatical nodes are embedded into the partial trees, and hence into the grammar rules.
method_label: Good parsing performance was obtained, even with small training corpora.
method_label: Several techniques were developed to improve the parser's accuracy, including in particular two methods for incorporating lexical information.
method_label: One method uses probabilities of binary lexical dependencies; the other directly lexicalizes the grammar rules.
result_label: Because the grammar rules are long, the number of rules is huge--more than thirty thousand from a corpus of one million words.
background_label: A parsing algorithm which can efficiently handle such a large grammar is described.
background_label: A Japanese parser based on the same idea was also developed.
method_label: Corpus-based sublanguage studies were conducted to relate the notion of sublanguage to lexical and syntactic properties of a text.
method_label: A statistical method based on word frequencies was developed to define sublanguages within a collection of documents; this method was evaluated by identifying the sublanguage of new documents.
method_label: Relative frequencies of different syntactic structures were used to assess the domain dependency of syntactic structure in a multi-domain corpus.
method_label: Cross-entropy measurements showed a clear distinction between fiction and non-fiction domains.
method_label: Experiments were then performed in which grammars trained on individual domains, or sets of domains, were used to parse texts in the same or other domains.
result_label: The results correlate with the measurements of syntactic variation across domains; in particular, the best performance is achieved using grammars trained on the same or similar domains.
result_label: The parsing and sublanguage techniques were applied to speech recognition.
result_label: Sublanguage techniques were able to increase recognition accuracy, and some promising cases were found where the parser was able to correct recognition errors.

===================================
paper_id: 24427295; YEAR: 2010
adju relevance: Related (+1)
difference: 2; annotator1: 2; annotator3: 0
sources: abs_cbow200
TITLE: Character confusion versus focus word-based correction of spelling and OCR variants in corpora
ABSTRACT: background_label: We present a new approach based on anagram hashing to handle globally the lexical variation in large and noisy text collections.
background_label: Lexical variation addressed by spelling correction systems is primarily typographical variation.
background_label: This is typically handled in a local fashion: given one particular text string some system of retrieving near-neighbors is applied, where near-neighbors are other text strings that differ from the particular string by a given number of characters.
method_label: The difference in characters between the original string and one of its retrieved near-neighbors constitutes a particular character confusion.
method_label: We present a global way of performing this action: for all possible particular character confusions given a particular edit distance, we sequentially identify all the pairs of text strings in the text collection that display a particular confusion.
method_label: We work on large digitized corpora, which contain lexical variation due to both the OCR process and typographical or typesetting error and show that all these types of variation can be handled equally well in the framework we present.
method_label: The character confusion-based prototype of Text-Induced Corpus Clean-up (ticcl) is compared to its focus word-based counterpart and evaluated on 6 years’ worth of digitized Dutch Parliamentary documents.
result_label: The character confusion approach is shown to gain an order of magnitude in speed on its word-based counterpart on large corpora.
result_label: Insights gained about the useful contribution of global corpus variation statistics are shown to also benefit the more traditional word-based approach to spelling correction.
result_label: Final tests on a held-out set comprising the 1918 edition of the Dutch daily newspaper ‘Het Volk’ show that the system is not sensitive to domain variation.

===================================
paper_id: 15991124; YEAR: 2014
adju relevance: Related (+1)
difference: 0; annotator1: 1; annotator3: 1
sources: abs_tfidfcbow200
TITLE: Chinese Word Ordering Errors Detection and Correction for Non-Native Chinese Language Learners
ABSTRACT: background_label: AbstractWord Ordering Errors (WOEs) are the most frequent type of grammatical errors at sentence level for non-native Chinese language learners.
background_label: Learners taking Chinese as a foreign language often place character(s) in the wrong places in sentences, and that results in wrong word(s) or ungrammatical sentences.
background_label: Besides, there are no clear word boundaries in Chinese sentences.
background_label: That makes WOEs detection and correction more challenging.
objective_label: In this paper, we propose methods to detect and correct WOEs in Chinese sentences.
method_label: Conditional random fields (CRFs) based WOEs detection models identify the sentence segments containing WOEs.
background_label: Segment point-wise mutual information (PMI), inter-segment PMI difference, language model, tag of the previous segment, and CRF bigram template are explored.
background_label: Words in the segments containing WOEs are reordered to generate candidates that may have correct word orderings.
method_label: Ranking SVM based models rank the candidates and suggests the most proper corrections.
method_label: Training and testing sets are selected from HSK dynamic composition corpus created by Beijing Language and Culture University.
method_label: Besides the HSK WOE dataset, Google Chinese Web 5-gram corpus is used to learn features for WOEs detection and correction.
result_label: The best model achieves an accuracy of 0.834 for detecting WOEs in sentence segments.
result_label: On the average, the correct word orderings are ranked 4.8 among 184.48 candidates.

===================================
paper_id: 2283888; YEAR: 2016
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator3: 2
sources: abs_tfidfcbow200
TITLE: Aligning Texts and Knowledge Bases with Semantic Sentence Simplification
ABSTRACT: background_label: Finding the natural language equivalent of structured data is both a challenging and promising task.
background_label: In particular, an efficient alignment of knowledge bases with texts would benefit many applications, including natural language generation, information retrieval and text simplification.
objective_label: In this paper, we present an approach to build a dataset of triples aligned with equivalent sentences written in natural language.
method_label: Our approach consists of three main steps.
method_label: First, target sentences are annotated automatically with knowledge base (KB) concepts and instances.
background_label: The triples linking these elements in the KB are extracted as candidate facts to be aligned with the annotated sentence.
method_label: Second, we use textual mentions referring to the subject and object of these facts to semantically simplify the target sentence via crowdsourcing.
method_label: Third, the sentences provided by different contributors are post-processed to keep only the most relevant simplifications for the alignment with KB facts.
method_label: We present different filtering methods, and share the constructed datasets in the public domain.
method_label: These datasets contain 1050 sentences aligned with 1885 triples.
result_label: They can be used to train natural language generators as well as semantic or contextual text simplifiers.

===================================
paper_id: 13153561; YEAR: 1998
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator3: 0
sources: title_cbow200
TITLE: Lazy Transformation-Based Learning
ABSTRACT: background_label: We introduce a significant improvement for a relatively new machine learning method called Transformation-Based Learning.
method_label: By applying a Monte Carlo strategy to randomly sample from the space of rules, rather than exhaustively analyzing all possible rules, we drastically reduce the memory and time costs of the algorithm, without compromising accuracy on unseen data.
method_label: This enables Transformation- Based Learning to apply to a wider range of domains, as it can effectively consider a larger number of different features and feature interactions in the data.
result_label: In addition, the Monte Carlo improvement decreases the labor demands on the human developer, who no longer needs to develop a minimal set of rule templates to maintain tractability.

===================================
paper_id: 769616; YEAR: 2010
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator3: 2
sources: abs_tfidf
TITLE: Towards Effective Sentence Simplification for Automatic Processing of Biomedical Text
ABSTRACT: background_label: The complexity of sentences characteristic to biomedical articles poses a challenge to natural language parsers, which are typically trained on large-scale corpora of non-technical text.
objective_label: We propose a text simplification process, bioSimplify, that seeks to reduce the complexity of sentences in biomedical abstracts in order to improve the performance of syntactic parsers on the processed sentences.
method_label: Syntactic parsing is typically one of the first steps in a text mining pipeline.
method_label: Thus, any improvement in performance would have a ripple effect over all processing steps.
method_label: We evaluated our method using a corpus of biomedical sentences annotated with syntactic links.
result_label: Our empirical results show an improvement of 2.90% for the Charniak-McClosky parser and of 4.23% for the Link Grammar parser when processing simplified sentences rather than the original sentences in the corpus.

===================================
paper_id: 7938312; YEAR: 2005
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator3: 0
sources: title_tfidf
TITLE: Evaluation of Iterative Alignment Algorithms for Multiple Alignment
ABSTRACT: background_label: MOTIVATION Iteration has been used a number of times as an optimization method to produce multiple alignments, either alone or in combination with other methods.
background_label: Iteration has a great advantage in that it is often very simple both in terms of coding the algorithms and the complexity of the time and memory requirements.
method_label: In this paper, we systematically test several different iteration strategies by comparing the results on sets of alignment test cases.
method_label: RESULTS We tested three schemes where iteration is used to improve an existing alignment.
result_label: This was found to be remarkably effective and could induce a significant improvement in the accuracy of alignments from most packages.
result_label: For example the average accuracy of ClustalW was improved by over 6% on the hardest test cases.
background_label: Iteration was found to be even more powerful when it was directly incorporated into a progressive alignment scheme.
background_label: Here, iteration was used to improve subalignments at each step of progressive alignment.
background_label: The beneficial effects of iteration come, in part, from the ability to get round the usual local minimum problem with progressive alignment.
method_label: This ability can also be used to help reduce the complexity of T-Coffee, without losing accuracy.
method_label: Alignments can be generated, using T-Coffee, to align subgroups of sequences, which can then be iteratively improved and merged.
other_label: AVAILABILITY All of the scripts are freely available on the web at http://www.bioinf.ucd.ie/people/iain/iteration.html   CONTACT iain.wallace@ucd.ie.

===================================
paper_id: 102354853; YEAR: 2019
adju relevance: Related (+1)
difference: 2; annotator1: 2; annotator3: 0
sources: title_tfidfcbow200
TITLE: Siamese Encoding and Alignment by Multiscale Learning with Self-Supervision
ABSTRACT: method_label: We propose a method of aligning a source image to a target image, where the transform is specified by a dense vector field.
method_label: The two images are encoded as feature hierarchies by siamese convolutional nets.
method_label: Then a hierarchy of aligner modules computes the transform in a coarse-to-fine recursion.
method_label: Each module receives as input the transform that was computed by the module at the level above, aligns the source and target encodings at the same level of the hierarchy, and then computes an improved approximation to the transform using a convolutional net.
result_label: The entire architecture of encoder and aligner nets is trained in a self-supervised manner to minimize the squared error between source and target remaining after alignment.
background_label: We show that siamese encoding enables more accurate alignment than the image pyramids of SPyNet, a previous deep learning approach to coarse-to-fine alignment.
background_label: Furthermore, self-supervision applies even without target values for the transform, unlike the strongly supervised SPyNet.
background_label: We also show that our approach outperforms one-shot approaches to alignment, because the fine pathways in the latter approach may fail to contribute to alignment accuracy when displacements are large.
method_label: As shown by previous one-shot approaches, good results from self-supervised learning require that the loss function additionally penalize non-smooth transforms.
method_label: We demonstrate that"masking out"the penalty function near discontinuities leads to correct recovery of non-smooth transforms.
result_label: Our claims are supported by empirical comparisons using images from serial section electron microscopy of brain tissue.

===================================
paper_id: 1508626; YEAR: 2010
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator3: 0
sources: abs_cbow200
TITLE: Nearest neighbor based collection OCR
ABSTRACT: background_label: Conventional optical character recognition (OCR) systems operate on individual characters and words, and do not normally exploit document or collection context.
background_label: We describe a Collection OCR which takes advantage of the fact that multiple examples of the same word (often in the same font) may occur in a document or collection.
method_label: The idea here is that an OCR or a reCAPTCHA like process generates a partial set of recognized words.
method_label: In the second stage, a nearest neighbor algorithm compares the remaining word-images to those already recognized and propagates labels from the nearest neighbors.
result_label: It is shown that by using an approximate fast nearest neighbor algorithm based on Hierarchical K-Means (HKM), we can do this accurately and efficiently.
background_label: It is also shown that profile based features perform much better than SIFT and Pyramid Histogram of Gradient (PHOG) features.
background_label: We believe that this is because profile features are more robust to word degradations (common in our documents).
method_label: This approach is applied to a collection of Telugu books - a language for which no commercial OCR exists.
method_label: We show from a selection of 33 Telugu books that starting with OCR labels for only 30% of the collection we can recognize the remaining 70% of the words in the collection with 70% accuracy using this approach.
method_label: Since the approach makes no language specific assumptions, it should be applicable to a large number of languages.
result_label: In particular we are interested in its applicability to Indic languages and scripts.

===================================
paper_id: 18592955; YEAR: 2006
adju relevance: Related (+1)
difference: 0; annotator1: 1; annotator3: 1
sources: specter
TITLE: Automated Multiword Expression Prediction For Grammar Engineering
ABSTRACT: background_label: However large a hand-crafted wide-coverage grammar is, there are always going to be words and constructions that are not included in it and are going to cause parse failure.
background_label: Due to their heterogeneous and flexible nature, Multiword Expressions (MWEs) provide an endless source of parse failures.
background_label: As the number of such expressions in a speaker's lexicon is equiparable to the number of single word units (Jackendoff, 1997), one major challenge for robust natural language processing systems is to be able to deal with MWEs.
method_label: In this paper we propose to semi-automatically detect MWE candidates in texts using some error mining techniques and validating them using a combination of the World Wide Web as a corpus and some statistical measures.
method_label: For the remaining candidates possible lexico-syntactic types are predicted, and they are subsequently added to the grammar as new lexical entries.
result_label: This approach provides a significant increase in the coverage of these expressions.

===================================
paper_id: 5789309; YEAR: 2002
adju relevance: Related (+1)
difference: 0; annotator1: 1; annotator3: 1
sources: specter - abs_tfidf
TITLE: Active Learning for Statistical Natural Language Parsing
ABSTRACT: background_label: It is necessary to have a (large) annotated corpus to build a statistical parser.
background_label: Acquisition of such a corpus is costly and time-consuming.
method_label: This paper presents a method to reduce this demand using active learning, which selects what samples to annotate, instead of annotating blindly the whole training corpus.Sample selection for annotation is based upon "representativeness" and "usefulness".
method_label: A model-based distance is proposed to measure the difference of two sentences and their most likely parse trees.
method_label: Based on this distance, the active learning process analyzes the sample distribution by clustering and calculates the density of each sample to quantify its representativeness.
method_label: Further more, a sentence is deemed as useful if the existing model is highly uncertain about its parses, where uncertainty is measured by various entropy-based scores.Experiments are carried out in the shallow semantic parser of an air travel dialog system.
result_label: Our result shows that for about the same parsing accuracy, we only need to annotate a third of the samples as compared to the usual random selection method.

===================================
paper_id: 1123406; YEAR: 1994
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator3: 0
sources: cited - abs_tfidf
TITLE: Memory-Based Lexical Acquisition and Processing
ABSTRACT: background_label: Current approaches to computational lexicology in language technology are knowledge-based (competence-oriented) and try to abstract away from specific formalisms, domains, and applications.
background_label: This results in severe complexity, acquisition and reusability bottlenecks.
objective_label: As an alternative, we propose a particular performance-oriented approach to Natural Language Processing based on automatic memory-based learning of linguistic (lexical) tasks.
method_label: The consequences of the approach for computational lexicology are discussed, and the application of the approach on a number of lexical acquisition and disambiguation tasks in phonology, morphology and syntax is described.

===================================
paper_id: 62241074; YEAR: 2013
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator3: 0
sources: abs_tfidfcbow200 - abs_tfidf
TITLE: A multidimensional approach to aligned sentences in translated text
ABSTRACT: background_label: Using unsupervised clustering techniques this study explores sentence alignment patterns in a parallel corpus of Norwegian source texts and Spanish translations, the NSPC (Hareide and Hofland 2012).
background_label: The results show that three strategies with respect to sentence alignment dominate: one to one correspondence, merging two sentences into one, and removing sentences altogether (omission).
background_label: The strategies are intricately correlated with the variables translator , author , and genre .
method_label: However, we show how visualization techniques for cluster analyses offer a possibility for teasing apart these interactions as well as their relative importance.
result_label: Our results indicate that non-fiction texts allow translators more freedom with respect to the treatment of sentences than do texts that are written by professional authors of fiction.
result_label: The style of the author appears to play only a secondary role, but is especially important in fiction.
other_label: Keywords: corpus based translation, cluster analysis, parallel corpora, corpus alignment, unidirectional bilingual corpus

===================================
paper_id: 15366005; YEAR: 2008
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator3: 0
sources: title_tfidf - title_cbow200 - title_tfidfcbow200
TITLE: Simultaneous learning and alignment: Multiinstance and multi-pose learning
ABSTRACT: background_label: In object recognition in general and in face detection in par- ticular, data alignment is necessary to achieve good classification results with certain statistical learning approaches such as Viola-Jones.
background_label: Data can be aligned in one of two ways: (1) by separating the data into coherent groups and training separate classifiers for each; (2) by adjusting training samples so they lie in correspondence.
background_label: If done manually, both procedures are labor intensive and can significantly add to the cost of labeling.
objective_label: In this paper we present a unified boosting framework for simultaneous learn- ing and alignment.
method_label: We present a novel boosting algorithm for Multiple Pose Learning (mpl), where the goal is to simultaneously split data into groups and train classifiers for each.
method_label: We also review Multiple Instance Learning (mil), and in particular mil-boost, and describe how to use it to simultaneously train a classifier and bring data into correspondence.
result_label: We show results on variations of LFW and MNIST, demonstrating the potential of these approaches.

===================================
paper_id: 42146106; YEAR: 2011
adju relevance: Related (+1)
difference: 2; annotator1: 2; annotator3: 0
sources: specter
TITLE: Automated Grammatical Error Detection for Language Learners
ABSTRACT: background_label: This book is a useful survey of the current state of the art in automated grammatical error detection in (mostly) non-native English, by some of the leading researchers in the field, aimed at audiences in computational linguistics and computer-aided language learning.
background_label: The book begins with a working definition of grammatical error, distinguishing these from typos and some kinds of spelling and punctuation errors, and points out that whereas (only) 400 million people have some kind of English as their first language, another billion speak it as their second (or are trying to learn it as such).Chapter 2 presents a brief historical overview of the field.
method_label: Until the advent of statistical methods in the 1990s, most systems used some kind of rule-based parsing supplemented with mechanisms for dealing with ill-formed input.
background_label: These mechanisms included pattern matching, the addition of special "mal-rules" for spotting frequently occurring forms of error, or different types of "parse fitting" and "relaxation" techiques (e.g., in the case of unification grammars, allowing for some types of unification failure).
method_label: Probably the most successful of these, both in commercial terms and in terms of performance, was the Epistle system originating from IBM and subsequently included in Microsoft Word, although its primary purpose was first-language rather than secondlanguage correction.In Chapter 3 the authors explore the problems faced by second-language learners.
result_label: On the basis of evidence from the Cambridge University Press corpus of learner English, it seems that the most frequent error type is simply an incorrect choice of content word, followed in turn by incorrect preposition, and incorrect determiner choice.
result_label: Like many collocation errors, it is difficult to use the techniques described in the book to address incorrect content word choice, because this usually does not result in ungrammaticality, and the state of the art is not such that semantic anomaly can be reliably detected.
result_label: Many systems therefore have concentrated on preposition and determiner errors.

===================================
paper_id: 2596605; YEAR: 1998
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: cited - abs_tfidf
TITLE: Distributional Information: A Powerful Cue for Acquiring Syntactic Categories
ABSTRACT: background_label: Many theorists have dismissed a priori the idea that distributional information could play a significant role in syntactic category acquisition.
method_label: We demonstrate empirically that such information provides a powerful cue to syntactic category membership, which can be exploited by a variety of simple, psychologically plausible mechanisms.We present a range of results using a large corpus of child-directed speech and explore their psychological implications.
result_label: While our results show that a considerable amount of information concerning the syntactic categories can be obtained from distributional information alone, we stress that many other sources of information may also be potential contributors to the identification of syntactic classes.

===================================
paper_id: 16652006; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: specter
TITLE: Automatic parsing as an efficient pre-annotation tool for historical texts
ABSTRACT: background_label: AbstractHistorical treebanks tend to be manually annotated, which is not surprising, since state-of-the-art parsers are not accurate enough to ensure high-quality annotation for historical texts.
objective_label: We test whether automatic parsing can be an efficient pre-annotation tool for Old East Slavic texts.
method_label: We use the TOROT treebank from the PROIEL treebank family.
method_label: We convert the PROIEL format to the CONLL format and use MaltParser to create syntactic pre-annotation.
method_label: Using the most conservative evaluation method, which takes into account PROIEL-specific features, MaltParser by itself yields 0.845 unlabelled attachment score, 0.779 labelled attachment score and 0.741 secondary dependency accuracy (note, though, that the test set comes from a relatively simple genre and contains rather short sentences).
result_label: Experiments with human annotators show that preparsing, if limited to sentences where no changes to word or sentence boundaries are required, increases their annotation rate.
result_label: For experienced annotators, the speed gain varies from 5.80% to 16.57%, for inexperienced annotators from 14.61% to 32.17% (using conservative estimates).
result_label: There are no strong reliable differences in the annotation accuracy, which means that there is no reason to suspect that using preparsing might lower the final annotation quality.

===================================
paper_id: 119425731; YEAR: 1972
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: title_tfidf
TITLE: Unzerlegbare Darstellungen I
ABSTRACT: background_label: LetK be the structure got by forgetting the composition law of morphisms in a given category.
background_label: A linear representation ofK is given by a map V associating with any morphism ϕ: a→e ofK a linear vector space map V(ϕ): V(a)→V(e).
method_label: We classify thoseK having only finitely many isomorphy classes of indecomposable linear representations.
other_label: This classification is related to an old paper by Yoshii [3].

===================================
paper_id: 14230784; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: title_tfidf - title_cbow200 - title_tfidfcbow200
TITLE: Alignment-based transfer learning for robot models
ABSTRACT: background_label: Robot manipulation tasks require on robot models.
background_label: When exact physical parameters of the robot are not available, learning robot models from data becomes an appealing alternative.
background_label: Most learning approaches are formulated in a supervised learning framework and are based on clearly defined training sets.
method_label: We propose a method that improves the learning process by using additional data obtained from other experiments of the robot or even from experiments with different robot architectures.
method_label: Incorporating experiences from other experiments requires transfer learning that has been used with success in machine learning.
method_label: The proposed method can be used for arbitrary robot model, together with any type of learning algorithm.
result_label: Experimental results indicate that task transfer between different robot architectures is a sound concept.
result_label: Furthermore, clear improvement is gained on forward kinematics model learning in a task-space control task.

===================================
paper_id: 11622923; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: title_tfidf
TITLE: Mutual Alignment Transfer Learning
ABSTRACT: background_label: Training robots for operation in the real world is a complex, time consuming and potentially expensive task.
background_label: Despite significant success of reinforcement learning in games and simulations, research in real robot applications has not been able to match similar progress.
background_label: While sample complexity can be reduced by training policies in simulation, such policies can perform sub-optimally on the real platform given imperfect calibration of model dynamics.
method_label: We present an approach -- supplemental to fine tuning on the real robot -- to further benefit from parallel access to a simulator during training and reduce sample requirements on the real robot.
method_label: The developed approach harnesses auxiliary rewards to guide the exploration for the real world agent based on the proficiency of the agent in simulation and vice versa.
result_label: In this context, we demonstrate empirically that the reciprocal alignment for both agents provides further benefit as the agent in simulation can adjust to optimize its behaviour for states commonly visited by the real-world agent.

===================================
paper_id: 53084138; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: abs_cbow200
TITLE: Resolving Citation Links With Neural Networks
ABSTRACT: background_label: This work demonstrates how neural network models (NNs) can be exploited towards resolving citation links in the scientific literature, which involves locating passages in the source paper the author had intended when citing the paper.
background_label: We look at two kinds of models: triplet and binary.
method_label: The triplet network model works by ranking potential candidates, using what is generally known as the triplet loss, while the binary model tackles the issue by turning it into a binary decision problem, i.e., by labeling a candidate as true or false, depending on how likely a target it is.
method_label: Experiments are conducted using three datasets developed by the CL-SciSumm project from a large repository of scientific papers in the Association for Computational Linguistics (ACL) repository.
result_label: The results find that NNs are extremely susceptible to how the input is represented: they perform better on inputs expressed in binary format than on those encoded using the TFIDF metric or neural embeddings.
method_label: Furthermore, in response to a difficulty NNs and baselines had in predicting the exact location of a target, we introduce the idea of approximately correct targets (ACTs) where the goal is to find a region which likely contains a true target rather than its exact location.
result_label: We show that with the ACTs, NNs consistently outperform Ranking SVM and TFIDF on the aforementioned datasets.

===================================
paper_id: 52012943; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: abs_cbow200
TITLE: A New Approach to Animacy Detection
ABSTRACT: background_label: AbstractAnimacy is a necessary property for a referent to be an agent, and thus animacy detection is useful for a variety of natural language processing tasks, including word sense disambiguation, co-reference resolution, semantic role labeling, and others.
background_label: Prior work treated animacy as a word-level property, and has developed statistical classifiers to classify words as either animate or inanimate.
method_label: We discuss why this approach to the problem is ill-posed, and present a new approach based on classifying the animacy of co-reference chains.
method_label: We show that simple voting approaches to inferring the animacy of a chain from its constituent words perform relatively poorly, and then present a hybrid system merging supervised machine learning (ML) and a small number of handbuilt rules to compute the animacy of referring expressions and co-reference chains.
method_label: This method achieves state of the art performance.
method_label: The supervised ML component leverages features such as word embeddings over referring expressions, parts of speech, and grammatical and semantic roles.
background_label: The rules take into consideration parts of speech and the hypernymy structure encoded in WordNet.
background_label: The system achieves an F 1 of 0.88 for classifying the animacy of referring expressions, which is comparable to state of the art results for classifying the animacy of words, and achieves an F 1 of 0.75 for classifying the animacy of coreference chains themselves.
method_label: We release our training and test dataset, which includes 142 texts (all narratives) comprising 156,154 words, 34,698 referring expressions, and 10,941 co-reference chains.
method_label: We test the method on a subset of the OntoNotes dataset, showing using manual sampling that animacy classification is 90%±2% accurate for coreference chains, and 92%±1% for referring expressions.
method_label: The data also contains 46 folktales, which present an interesting challenge because they often involve characters who are members of traditionally inanimate classes (e.g., stoves that walk, trees that talk).
result_label: We show that our system is able to detect the animacy of these unusual referents with an F 1 of 0.95.

===================================
paper_id: 15175549; YEAR: 2010
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: abs_cbow200
TITLE: Generating Confusion Sets for Context-Sensitive Error Correction
ABSTRACT: background_label: AbstractIn this paper, we consider the problem of generating candidate corrections for the task of correcting errors in text.
objective_label: We focus on the task of correcting errors in preposition usage made by non-native English speakers, using discriminative classifiers.
method_label: The standard approach to the problem assumes that the set of candidate corrections for a preposition consists of all preposition choices participating in the task.
method_label: We determine likely preposition confusions using an annotated corpus of nonnative text and use this knowledge to produce smaller sets of candidates.We propose several methods of restricting candidate sets.
method_label: These methods exclude candidate prepositions that are not observed as valid corrections in the annotated corpus and take into account the likelihood of each preposition confusion in the non-native text.
result_label: We find that restricting candidates to those that are observed in the non-native data improves both the precision and the recall compared to the approach that views all prepositions as possible candidates.
result_label: Furthermore, the approach that takes into account the likelihood of each preposition confusion is shown to be the most effective.

===================================
paper_id: 479; YEAR: 1995
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: specter - abs_cbow200 - abs_tfidfcbow200
TITLE: Multilingual Sentence Categorization according to Language
ABSTRACT: background_label: In this paper, we describe an approach to sentence categorization which has the originality to be based on natural properties of languages with no training set dependency.
background_label: The implementation is fast, small, robust and textual errors tolerant.
method_label: Tested for french, english, spanish and german discrimination, the system gives very interesting results, achieving in one test 99.4% correct assignments on real sentences.
method_label: The resolution power is based on grammatical words (not the most common words) and alphabet.
method_label: Having the grammatical words and the alphabet of each language at its disposal, the system computes for each of them its likelihood to be selected.
method_label: The name of the language having the optimum likelihood will tag the sentence --- but non resolved ambiguities will be maintained.
method_label: We will discuss the reasons which lead us to use these linguistic facts and present several directions to improve the system's classification performance.
result_label: Categorization sentences with linguistic properties shows that difficult problems have sometimes simple solutions.

===================================
paper_id: 27832760; YEAR: 2010
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: title_tfidfcbow200
TITLE: Combining Self Learning and Active Learning for Chinese Named Entity Recognition
ABSTRACT: objective_label: This paper proposes a combination of active learning and self-training method to reduce the labeling effort for Chinese Named Entity Recognition (NER).
background_label: Active learning and self-training are two different ways to use unlabeled data.
method_label: They are complement when choosing unlabeled data for further training.
method_label: A new strategy based on Information Density (ID) for sample selecting in sequential labeling problem is also proposed, which is suitable for both active learning and self-training.
method_label: Conditional Random Fields (CRFs) is chosen as the underlying model for active learning and self-training in the proposed approach due to its promising performance in many sequence labeling tasks.
result_label: Experiment results show the effect of the proposed method.
result_label: On Sighan bakeoff 2006 MSRA NER corpus, an F1 score of 77.4% is achieved by using only 15,000 training sentences chosen by the proposed hybrid method.

===================================
paper_id: 52276927; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200 - title_tfidf
TITLE: Adversarial Similarity Network for Evaluating Image Alignment in Deep Learning based Registration.
ABSTRACT: background_label: This paper introduces an unsupervised adversarial similarity network for image registration.
background_label: Unlike existing deep learning registration frameworks, our approach does not require ground-truth deformations and specific similarity metrics.
method_label: We connect a registration network and a discrimination network with a deformable transformation layer.
method_label: The registration network is trained with feedback from the discrimination network, which is designed to judge whether a pair of registered images are sufficiently similar.
method_label: Using adversarial training, the registration network is trained to predict deformations that are accurate enough to fool the discrimination network.
result_label: Experiments on four brain MRI datasets indicate that our method yields registration performance that is promising in both accuracy and efficiency compared with state-of-the-art registration methods, including those based on deep learning.

===================================
paper_id: 14139945; YEAR: 2004
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: specter
TITLE: Using Linguistic Principles to Recover Empty Categories
ABSTRACT: background_label: This paper describes an algorithm for detecting empty nodes in the Penn Treebank (Marcus et al., 1993), finding their antecedents, and assigning them function tags, without access to lexical information such as valency.
method_label: Unlike previous approaches to this task, the current method is not corpus-based, but rather makes use of the principles of early Government-Binding theory (Chomsky, 1981), the syntactic theory that underlies the annotation.
method_label: Using the evaluation metric proposed by Johnson (2002), this approach outperforms previously published approaches on both detection of empty categories and antecedent identification, given either annotated input stripped of empty categories or the output of a parser.
method_label: Some problems with this evaluation metric are noted and an alternative is proposed along with the results.
result_label: The paper considers the reasons a principle-based approach to this problem should outperform corpus-based approaches, and speculates on the possibility of a hybrid approach.

===================================
paper_id: 14091131; YEAR: 2009
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: title_cbow200
TITLE: Manifold-Based Learning and Synthesis
ABSTRACT: objective_label: This paper proposes a new approach to analyze high-dimensional data set using low-dimensional manifold.
objective_label: This manifold-based approach provides a unified formulation for both learning from and synthesis back to the input space.
method_label: The manifold learning method desires to solve two problems in many existing algorithms.
method_label: The first problem is the local manifold distortion caused by the cost averaging of the global cost optimization during the manifold learning.
method_label: The second problem results from the unit variance constraint generally used in those spectral embedding methods where global metric information is lost.
method_label: For the out-of-sample data points, the proposed approach gives simple solutions to transverse between the input space and the feature space.
method_label: In addition, this method can be used to estimate the underlying dimension and is robust to the number of neighbors.
result_label: Experiments on both low-dimensional data and real image data are performed to illustrate the theory.

===================================
paper_id: 27035051; YEAR: 2001
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Transformation-Based Learning Using Multirelational Aggregation
ABSTRACT: background_label: Given the very widespread use of multirelational databases, ILP systems are increasingly being used on data originating from such warehouses.
background_label: Unfortunately, even though not complex in structure, such business data often contain highly non-determinate components, making them difficult for ILP learners geared towards structurally complex tasks.
objective_label: In this paper, we build on popular transformation-based approaches to ILP and describe how they can naturally be extended with relational aggregation.
result_label: We experimentall y show that this results in a multirelational learner that outperforms a structurally-oriented ILP system both in speed and accuracy on this class of problems.

===================================
paper_id: 9268953; YEAR: 1996
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: abs_cbow200 - specter
TITLE: Comparison of two tree-structured approaches for grapheme-to-phoneme conversion
ABSTRACT: background_label: Recently, we described a two step self learning approach for grapheme to phoneme (G2P) conversion (O. Anderson and P. Dalsgaard, 1995).
method_label: In the first step, grapheme and phoneme strings in the training data are aligned via an iterative Viterbi procedure that may insert graphemic and phonemic nulls where required.
method_label: In the second step, a Trie structure, encoding pronunciation rules is generated.
method_label: We describe the alignment module, and give alignment accuracies on the NETtalk database.
method_label: We also compare transcription accuracies for two approaches to the second step on three databases: the NETtalk database, the CMU dictionary and the French part of the ONOMASTICA lexicon.
method_label: The two transcription approaches applied in this research are a Trie approach and an approach based on binary decision trees grown by means of the Gelfand-Ravishankar-Delp algorithm (F. Breiman et al., 1984; S. Gelfand et al., 1991; R. Kuhn et al., 1995).
method_label: We discuss the choice of questions for these decision trees-it may be possible to formulate questions about groups of characters (e.g., "is the next letter a vowel?")
method_label: that yield better trees than those that only use questions about individual characters (e.g., "is the next letter an 'A' ?").
result_label: Finally, we discuss the implications of our work for G2P conversion.

===================================
paper_id: 11259251; YEAR: 2012
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: title_cbow200
TITLE: Template-based learning of grasp selection
ABSTRACT: background_label: The ability to grasp unknown objects is an important skill for personal robots, which has been addressed by many present and past research projects, but still remains an open problem.
background_label: A crucial aspect of grasping is choosing an appropriate grasp configuration, i.e.
background_label: the 6d pose of the hand relative to the object and its finger configuration.
background_label: Finding feasible grasp configurations for novel objects, however, is challenging because of the huge variety in shape and size of these objects.
background_label: Moreover, possible configurations also depend on the specific kinematics of the robotic arm and hand in use.
method_label: In this paper, we introduce a new grasp selection algorithm able to find object grasp poses based on previously demonstrated grasps.
background_label: Assuming that objects with similar shapes can be grasped in a similar way, we associate to each demonstrated grasp a grasp template.
method_label: The template is a local shape descriptor for a possible grasp pose and is constructed using 3d information from depth sensors.
method_label: For each new object to grasp, the algorithm then finds the best grasp candidate in the library of templates.
method_label: The grasp selection is also able to improve over time using the information of previous grasp attempts to adapt the ranking of the templates.
method_label: We tested the algorithm on two different platforms, the Willow Garage PR2 and the Barrett WAM arm which have very different hands.
result_label: Our results show that the algorithm is able to find good grasp configurations for a large set of objects from a relatively small set of demonstrations, and does indeed improve its performance over time.

===================================
paper_id: 1751478; YEAR: 2009
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: abs_tfidfcbow200 - abs_tfidf
TITLE: Sentence retrieval for abstracts of randomized controlled trials
ABSTRACT: background_label: BACKGROUND The practice of evidence-based medicine (EBM) requires clinicians to integrate their expertise with the latest scientific research.
background_label: But this is becoming increasingly difficult with the growing numbers of published articles.
background_label: There is a clear need for better tools to improve clinician's ability to search the primary literature.
background_label: Randomized clinical trials (RCTs) are the most reliable source of evidence documenting the efficacy of treatment options.
objective_label: This paper describes the retrieval of key sentences from abstracts of RCTs as a step towards helping users find relevant facts about the experimental design of clinical studies.
method_label: METHOD Using Conditional Random Fields (CRFs), a popular and successful method for natural language processing problems, sentences referring to Intervention, Participants and Outcome Measures are automatically categorized.
result_label: This is done by extending a previous approach for labeling sentences in an abstract for general categories associated with scientific argumentation or rhetorical roles: Aim, Method, Results and Conclusion.
result_label: Methods are tested on several corpora of RCT abstracts.
background_label: First structured abstracts with headings specifically indicating Intervention, Participant and Outcome Measures are used.
background_label: Also a manually annotated corpus of structured and unstructured abstracts is prepared for testing a classifier that identifies sentences belonging to each category.
method_label: RESULTS Using CRFs, sentences can be labeled for the four rhetorical roles with F-scores from 0.93-0.98.
method_label: This outperforms the use of Support Vector Machines.
method_label: Furthermore, sentences can be automatically labeled for Intervention, Participant and Outcome Measures, in unstructured and structured abstracts where the section headings do not specifically indicate these three topics.
result_label: F-scores of up to 0.83 and 0.84 are obtained for Intervention and Outcome Measure sentences.
result_label: CONCLUSION Results indicate that some of the methodological elements of RCTs are identifiable at the sentence level in both structured and unstructured abstract reports.
result_label: This is promising in that sentences labeled automatically could potentially form concise summaries, assist in information retrieval and finer-grained extraction.

===================================
paper_id: 12219473; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200 - specter
TITLE: A Hybrid Morphological Disambiguation System for Turkish
ABSTRACT: objective_label: AbstractIn this paper, we propose a morphological disambiguation method for Turkish, which is an agglutinative language.
method_label: We use a hybrid method, which combines statistical information with handcrafted rules and learned rules.
method_label: Five different steps are applied for disambiguation.
method_label: In the first step, the most likely tags of words are selected.
method_label: In the second step, we use handcrafted rules to constrain possible parses or select the correct parse.
method_label: Next, the most likely tags are selected for still ambiguous words according to the suffixes of the words that are unseen in the training corpus.
background_label: Then, we use transformation-based rules that are learned by a variation of Brill tagger.
method_label: If the word is still ambiguous, we use some heuristics for the disambiguation.
method_label: We constructed a hand-tagged dataset for training and applied a ten-fold cross validation with this dataset.
method_label: We obtained 93.4% accuracy on the average when whole morphological parses are considered in calculation.
result_label: The accuracy increased to 94.1% when only part-of-speech tags and inflections of last derivations are considered.
result_label: Our accuracy is 96.9% in terms of part-of-speech tagging.

===================================
paper_id: 9010625; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_tfidf
TITLE: Towards a hybrid NLG system for Data2Text in Portuguese
ABSTRACT: background_label: In many new interactions with machines, such as dialogue or output using voice, there is the need to convert information internal to a system into sentences, using Data2Text systems.
background_label: Trying to avoid the limitations of template-based and classical NLG methods, systems based on automatic translation have been proposed in recent years.
background_label: Despite providing sentences with the important variability needed for a better interaction, this doesn't come without a cost.
background_label: Contrary to template-based, these systems produce sentences with heterogeneous quality.
method_label: In this paper we proposed to combine a translation based NLG system with a classifier module capable of providing information on the Intelligibility or Quality of the sentences.
method_label: Sentences marked as unacceptable are replaced by template-based generated ones.
method_label: This classifier module is the main focus of the paper and combines extraction of linguistic features with a classifier trained in a manually annotated corpus.
result_label: Results suggest that our approach is valid as best results obtained have false positives below 8% and this metric can be even lower in practical applications, decreasing to around 3%, as the generation module produces low quality sentences at a rate lower than 30%.

===================================
paper_id: 121084921; YEAR: 1979
adju relevance: Irrelevant (0)
difference: 2; annotator1: 0; annotator3: 2
sources: cited - abs_tfidf
TITLE: Trainable grammars for speech recognition
ABSTRACT: background_label: Algorithms which are based on modeling speech as a finite‐state, hidden Markov process have been very successful in recent years.
background_label: This paper presents a generalization of these algorithms to certain denumerable‐state, hidden Markov processes.
method_label: This algorithm permits automatic training of the stochastic analog of an arbitrary context free grammar.
method_label: In particular, in contrast to many grammatical inference methods, the new algorithm allows the grammar to have an arbitrary degree of ambiguity.
method_label: Since natural language is often syntactically ambiguous, it is necessary for the grammatical inference algorithm to allow for this ambiguity.
result_label: Furthermore, allowing ambiguity in the grammar allows errors in the recognition process to be explicitly modeled in the grammar rather than added as an extra component.

===================================
paper_id: 10146127; YEAR: 2005
adju relevance: Irrelevant (0)
difference: 1; annotator1: 0; annotator3: 1
sources: specter - abs_cbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf
TITLE: Association-Based Bilingual Word Alignment
ABSTRACT: background_label: AbstractBilingual word alignment forms the foundation of current work on statistical machine translation.Standard wordalignment methods involve the use of probabilistic generative models that are complex to implement and slow to train.
objective_label: In this paper we show that it is possible to approach the alignment accuracy of the standard models using algorithms that are much faster, and in some ways simpler, based on basic word-association statistics.
method_label: MotivationBilingual word alignment is the first step of most current approaches to statistical machine translation.
method_label: Although the best performing systems are "phrasebased" (see, for instance, Och and Ney (2004) or Koehn et al.
method_label: (2003) ), possible phrase translations must first be extracted from word-aligned bilingual text segments.
method_label: The standard approach to word alignment makes use of five translation models defined by Brown et al.
method_label: (1993) , sometimes augmented by an HMM-based model or Och and Ney's "Model 6" (Och and Ney, 2003) .
result_label: The best of these models can produce high accuracy alignments, at least when trained on a large parallel corpus of fairly direct translations in closely related languages.There are a number of ways in which these standard models are less than ideal, however.
background_label: The higher-accuracy models are mathematically complex, and also difficult to train, as they do not factor in a way that permits a dynamic programming solution.
method_label: It can thus take many hours of processing time on current standard computers to train the models and produce an alignment of a large parallel corpus.In this paper, we take a different approach to word alignment, based on the use of bilingual wordassociation statistics rather than the generative probabilistic framework that the IBM and HMM models use.
method_label: In the end we obtain alignment algorithms that are much faster, and in some ways simpler, whose accuracy comes surprisingly close to the established probabilistic generative approach.
method_label: Data and Methodology for these ExperimentsThe experiments reported here were carried out using data from the workshop on building and using parallel texts held at HLT-NAACL 2003 (Mihalcea and Pedersen, 2003) .
method_label: For the majority of our experiments, we used a subset of the Canadian Hansards bilingual corpus supplied for the workshop, comprising 500,000 English-French sentences pairs, including 37 sentence pairs designated as "trial" data, and 447 sentence pairs designated as test data.
method_label: The trial and test data have been manually aligned at the word level, noting particular pairs of words either as "sure" or "possible" alignments.
result_label: As an additional test, we evaluated our best alignment method using the workshop corpus of approximately 49,000 English-Romanian sentences pairs from diverse sources, including 248 manually aligned sentence pairs designated as test data.
result_label: 1

===================================
paper_id: 62097085; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Orc-based Learning - Evaluating a Game-Based Learning Approach
ABSTRACT: background_label: Using game mechanics to improve the motivation and efforts becomes a popular approach.
background_label: Especially in higher education many projects have been realized to create a greater engagement of students in learning processes.
background_label: Because of these innovative ideas there is a lack of corresponding evaluation methods which respect all relevant aspects.
method_label: For this we created an evaluation model to meet the needs for evaluating a game-based learning approach.

===================================
paper_id: 8282751; YEAR: 2012
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: title_tfidf
TITLE: Temporal alignment
ABSTRACT: background_label: In order to process interval timestamped data, the sequenced semantics has been proposed.
objective_label: This paper presents a relational algebra solution that provides native support for the three properties of the sequenced semantics: snapshot reducibility, extended snapshot reducibility, and change preservation.
method_label: We introduce two temporal primitives, temporal splitter and temporal aligner, and define rules that use these primitives to reduce the operators of a temporal algebra to their nontemporal counterparts.
method_label: Our solution supports the three properties of the sequenced semantics through interval adjustment and timestamp propagation.
method_label: We have implemented the temporal primitives and reduction rules in the kernel of PostgreSQL to get native database support for processing interval timestamped data.
method_label: The support is comprehensive and includes outer joins, antijoins, and aggregations with predicates and functions over the time intervals of argument relations.
result_label: The implementation and empirical evaluation confirms effectiveness and scalability of our solution that leverages existing database query optimization techniques.

===================================
paper_id: 4573790; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 1; annotator1: 0; annotator3: 1
sources: title_tfidf
TITLE: Alignment of dynamic networks
ABSTRACT: background_label: Networks can model real-world systems in a variety of domains.
objective_label: Network alignment (NA) aims to find a node mapping that conserves similar regions between compared networks.
method_label: NA is applicable to many fields, including computational biology, where NA can guide the transfer of biological knowledge from well- to poorly-studied species across aligned network regions.
method_label: Existing NA methods can only align static networks.
method_label: However, most complex real-world systems evolve over time and should thus be modeled as dynamic networks.
result_label: We hypothesize that aligning dynamic network representations of evolving systems will produce superior alignments compared to aligning the systems' static network representations, as is currently done.
method_label: For this purpose, we introduce the first ever dynamic NA method, DynaMAGNA++.
method_label: This proof-of-concept dynamic NA method is an extension of a state-of-the-art static NA method, MAGNA++.
method_label: Even though both MAGNA++ and DynaMAGNA++ optimize edge as well as node conservation across the aligned networks, MAGNA++ conserves static edges and similarity between static node neighborhoods, while DynaMAGNA++ conserves dynamic edges (events) and similarity between evolving node neighborhoods.
method_label: For this purpose, we introduce the first ever measure of dynamic edge conservation and rely on our recent measure of dynamic node conservation.
method_label: Importantly, the two dynamic conservation measures can be optimized using any state-of-the-art NA method and not just MAGNA++.
result_label: We confirm our hypothesis that dynamic NA is superior to static NA, under fair comparison conditions, on synthetic and real-world networks, in computational biology and social network domains.
result_label: DynaMAGNA++ is parallelized and it includes a user-friendly graphical interface.

===================================
paper_id: 82456167; YEAR: 2007
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: title_tfidf
TITLE: Janeway's Immunobiology
ABSTRACT: background_label: Part I An Introduction to Immunobiology and Innate Immunity 1.
background_label: Basic Concepts in Immunology 2.
background_label: Innate Immunity Part II The Recognition of Antigen 3.
background_label: Antigen Recognition by B-cell and T-cell Receptors 4.
method_label: The Generation of Lymphocyte Antigen Receptors 5.
method_label: Antigen Presentation to T Lymphocytes Part III The Development of Mature Lymphocyte Receptor Repertoires 6.
method_label: Signaling Through Immune System Receptors 7.
result_label: The Development and Survival of Lymphocytes Part IV The Adaptive Immune Response 8.
background_label: T Cell-Mediated Immunity 9.
background_label: The Humoral Immune Response 10.
background_label: Dynamics of Adaptive Immunity 11.
other_label: The Mucosal Immune System Part V The Immune System in Health and Disease 12.
background_label: Failures of Host Defense Mechanism 13.
other_label: Allergy and Hypersensitivity 14.
other_label: Autoimmunity and Transplantation 15.
other_label: Manipulation of the Immune Response Part VI The Origins of Immune Responses 16.
other_label: Evolution of the Immune System Appendix I Immunologists' Toolbox Appendix II CD Antigens Appendix III Cytokines and their Receptors Appendix IV Chemokines and their Receptors Appendix V Immunological Constants

===================================
paper_id: 16389135; YEAR: 2002
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: title_cbow200
TITLE: Expertness based cooperative Q-learning
ABSTRACT: background_label: By using other agents' experiences and knowledge, a learning agent may learn faster, make fewer mistakes, and create some rules for unseen situations.
background_label: These benefits would be gained if the learning agent can extract proper rules from the other agents' knowledge for its own requirements.
objective_label: One possible way to do this is to have the learner assign some expertness values (intelligence level values) to the other agents and use their knowledge accordingly.
method_label: Some criteria to measure the expertness of the reinforcement learning agents are introduced.
method_label: Also, a new cooperative learning method, called weighted strategy sharing (WSS) is presented.
method_label: In this method, each agent measures the expertness of its teammates and assigns a weight to their knowledge and learns from them accordingly.
method_label: The presented methods are tested on two Hunter-Prey systems.
method_label: We consider that the agents are all learning from each other and compare them with those who cooperate only with the more expert ones.
method_label: Also, the effect of communication noise, as a source of uncertainty, on the cooperative learning method is studied.
result_label: Moreover, the Q-table of one of the cooperative agents is changed randomly and its effects on the presented methods are examined.

===================================
paper_id: 12253672; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: title_cbow200
TITLE: Encoder Based Lifelong Learning
ABSTRACT: background_label: This paper introduces a new lifelong learning solution where a single model is trained for a sequence of tasks.
background_label: The main challenge that vision systems face in this context is catastrophic forgetting: as they tend to adapt to the most recently seen task, they lose performance on the tasks that were learned previously.
method_label: Our method aims at preserving the knowledge of the previous tasks while learning a new one by using autoencoders.
method_label: For each task, an under-complete autoencoder is learned, capturing the features that are crucial for its achievement.
method_label: When a new task is presented to the system, we prevent the reconstructions of the features with these autoencoders from changing, which has the effect of preserving the information on which the previous tasks are mainly relying.
method_label: At the same time, the features are given space to adjust to the most recent environment as only their projection into a low dimension submanifold is controlled.
result_label: The proposed system is evaluated on image classification tasks and shows a reduction of forgetting over the state-of-the-art

===================================
paper_id: 61213138; YEAR: 2009
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: abs_cbow200
TITLE: Dictionary-free categorization of very similar objects via stacked evidence trees
ABSTRACT: background_label: Current work in object categorization discriminates among objects that typically possess gross differences which are readily apparent.
background_label: However, many applications require making much finer distinctions.
objective_label: We address an insect categorization problem that is so challenging that even trained human experts cannot readily categorize images of insects considered in this paper.
method_label: The state of the art that uses visual dictionaries, when applied to this problem, yields mediocre results (16.1% error).
method_label: Three possible explanations for this are (a) the dictionaries are unsupervised, (b) the dictionaries lose the detailed information contained in each keypoint, and (c) these methods rely on hand-engineered decisions about dictionary size.
result_label: This paper presents a novel, dictionary-free methodology.
background_label: A random forest of trees is first trained to predict the class of an image based on individual keypoint descriptors.
background_label: A unique aspect of these trees is that they do not make decisions but instead merely record evidence-i.e., the number of descriptors from training examples of each category that reached each leaf of the tree.
method_label: We provide a mathematical model showing that voting evidence is better than voting decisions.
method_label: To categorize a new image, descriptors for all detected keypoints are “dropped” through the trees, and the evidence at each leaf is summed to obtain an overall evidence vector.
method_label: This is then sent to a second-level classifier to make the categorization decision.
method_label: We achieve excellent performance (6.4% error) on the 9-class STONEFLY9 data set.
result_label: Also, our method achieves an average AUC of 0.921 on the PASCAL06 VOC, which places it fifth out of 21 methods reported in the literature and demonstrates that the method also works well for generic object categorization.

===================================
paper_id: 18600154; YEAR: 2011
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200 - specter
TITLE: The Microsoft Research Sentence Completion Challenge
ABSTRACT: background_label: Work on modeling semantics in text is progressing quickly, yet currently there are few public datasets which authors can use to measure and compare their systems.
background_label: This work takes a step towards addressing this issue.
method_label: We present the MSR Sentence Completion Challenge Data, which consists of 1,040 sentences, each of which has four impostor sentences, in which a single (fixed) word in the original sentence has been replaced by an impostor word with similar occurrence statistics.
method_label: For each sentence the task is then to determine which of the five choices for that word is the correct one.
method_label: This dataset was constructed from Project Gutenberg data.
method_label: Seed sentences were selected from five of Sir Arthur Conan Doyle’s Sherlock Holmes novels, and then imposter words were suggested with the aid of a language model trained on over 500 19th century novels.
method_label: The language model was used to compute 30 alternative words for a given low frequency word in a sentence, and human judges then picked the 4 best impostor words, based on a set of provided guidelines.
result_label: Although the data presented here will not be changed, this is still a work in progress, and we plan to add similar datasets based on other sources.
result_label: This technical report is a living document and will be updated appropriately as new datasets are constructed and new results on existing datasets (for example, using human subjects) are reported.

===================================
paper_id: 11490849; YEAR: 1996
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: specter
TITLE: A Faster Structured-Tag Word-Classification Method
ABSTRACT: background_label: Several methods have been proposed for processing a corpus to induce a tagset for the sub-language represented by the corpus.
method_label: This paper examines a structured-tag word classification method introduced by McMahon (1994) and discussed further by McMahon&Smith (1995) in cmp-lg/9503011 .
method_label: Two major variations, (1) non-random initial assignment of words to classes and (2) moving multiple words in parallel, together provide robust non-random results with a speed increase of 200% to 450%, at the cost of slightly lower quality than McMahon's method's average quality.
method_label: Two further variations, (3) retaining information from less- frequent words and (4) avoiding reclustering closed classes, are proposed for further study.
method_label: Note: The speed increases quoted above are relative to my implementation of my understanding of McMahon's algorithm; this takes time measured in hours and days on a home PC.
result_label: A revised version of the McMahon&Smith (1995) paper has appeared (June 1996) in Computational Linguistics 22(2):217- 247; this refers to a time of"several weeks"to cluster 569 words on a Sparc-IPC.

===================================
paper_id: 8703520; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: abs_tfidf - abs_tfidfcbow200
TITLE: Detecting Context Dependence in Exercise Item Candidates Selected from Corpora
ABSTRACT: background_label: We explore the factors influencing the dependence of single sentences on their larger textual context in order to automatically identify candidate sentences for language learning exercises from corpora which are presentable in isolation.
background_label: An in-depth investigation of this question has not been previously carried out.
objective_label: Understanding this aspect can contribute to a more efficient selection of candidate sentences which, besides reducing the time required for item writing, can also ensure a higher degree of variability and authenticity.
method_label: We present a set of relevant aspects collected based on the qualitative analysis of a smaller set of context-dependent corpus example sentences.
method_label: Furthermore, we implemented a rule-based algorithm using these criteria which achieved an average precision of 0.76 for the identification of different issues related to context dependence.
result_label: The method has also been evaluated empirically where 80% of the sentences in which our system did not detect context-dependent elements were also considered context-independent by human raters.

===================================
paper_id: 85529605; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: title_tfidfcbow200 - title_tfidf
TITLE: Shouji: A Fast and Efficient Pre-Alignment Filter for Sequence Alignment
ABSTRACT: background_label: Motivation: The ability to generate massive amounts of sequencing data continues to overwhelm the processing capability of existing algorithms and compute infrastructures.
objective_label: In this work, we explore the use of hardware/software co-design and hardware acceleration to significantly reduce the execution time of short sequence alignment, a crucial step in analyzing sequenced genomes.
method_label: We introduce Shouji, a highly-parallel and accurate pre-alignment filter that remarkably reduces the need for computationally-costly dynamic programming algorithms.
method_label: The first key idea of our proposed pre-alignment filter is to provide high filtering accuracy by correctly detecting all common subsequences shared between two given sequences.
method_label: The second key idea is to design a hardware accelerator that adopts modern FPGA (Field-Programmable Gate Array) architectures to further boost the performance of our algorithm.
result_label: Results: Shouji significantly improves the accuracy of pre-alignment filtering by up to two orders of magnitude compared to the state-of-the-art pre-alignment filters, GateKeeper and SHD.
background_label: Our FPGA-based accelerator is up to three orders of magnitude faster than the equivalent CPU implementation of Shouji.
method_label: Using a single FPGA chip, we benchmark the benefits of integrating Shouji with five state-of-the-art sequence aligners, designed for different computing platforms.
method_label: The addition of Shouji as a pre-alignment step reduces the execution time of the five state-of-the-art sequence aligners by up to 18.8x.
method_label: Shouji can be adapted for any bioinformatics pipeline that performs sequence alignment for verification.
method_label: Unlike most existing methods that aim to accelerate sequence alignment, Shouji does not sacrifice any of the aligner capabilities, as it does not modify or replace the alignment step.
other_label: Availability: https://github.com/CMU-SAFARI/Shouji

===================================
paper_id: 52930901; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: title_tfidfcbow200 - title_tfidf
TITLE: Partial Recovery of Erdős-Rényi Graph Alignment via k-Core Alignment
ABSTRACT: background_label: AbstractWe determine information theoretic conditions under which it is possible to partially recover the alignment used to generate a pair of sparse, correlated Erdős-Rényi graphs.
method_label: To prove our achievability result, we introduce the k-core alignment estimator.
method_label: This estimator searches for an alignment in which the intersection of the correlated graphs using this alignment has a minimum degree of k. We prove a matching converse bound.
method_label: As the number of vertices grows, recovery of the alignment for a fraction of the vertices tending to one is possible when the average degree of the intersection of the graph pair tends to infinity.
objective_label: It was previously known that exact alignment is possible when this average degree grows faster than the logarithm of the number of vertices.Graph alignment, or graph matching, is the problem of finding a correspondence between the vertex sets of a pair of graphs using structural information from the graphs.
method_label: It can be thought of as the noisy generalization of the graph isomorphism problem.
result_label: Graph matching has applications in the privacy of social network data, the analysis of biological protein interaction networks, and in computer vision.We consider the graph matching problem for random graphs that have been generated in a correlated way, so there is a planted ground-truth alignment of their vertices.
background_label: In this setting, the combinatorial optimization problem of maximizing edge overlap is also the maximum a posteriori estimator.
background_label: Related workA number of authors have worked to determine the information theoretic or statistical conditions under which graph alignments can be recovered by any algorithm.
method_label: Wright determined the conditions under which an Erdős-Rényi graph has a trivial automorphism group, or equivalently under which the isomorphism recovery problem has a unique solution [1] .
method_label: Pedarsani and Grossglauser obtained achievability conditions for exact recovery in the noisy case [2] .
method_label: Cullina and Kiyavash obtained matching achievability and converse conditions for exact recovery [3, 4] .
result_label: Kazemi, Yartseva, and Grossglauser considered alignment of graphs with overlapping but not identical vertex sets [5] .
other_label: *

===================================
paper_id: 12961905; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: On Comparison between Evolutionary Programming Network-based Learning and Novel Evolution Strategy Algorithm-based Learning
ABSTRACT: background_label: This paper presents two different evolutionary systems - Evolutionary Programming Network (EPNet) and Novel Evolutions Strategy (NES) Algorithm.
background_label: EPNet does both training and architecture evolution simultaneously, whereas NES does a fixed network and only trains the network.
method_label: Five mutation operators proposed in EPNet to reflect the emphasis on evolving ANNs behaviors.
method_label: Close behavioral links between parents and their offspring are maintained by various mutations, such as partial training and node splitting.
method_label: On the other hand, NES uses two new genetic operators - subpopulation-based max-mean arithmetical crossover and time-variant mutation.
method_label: The above-mentioned two algorithms have been tested on a number of benchmark problems, such as the medical diagnosis problems (breast cancer, diabetes, and heart disease).
result_label: The results and the comparison between them are also presented in this paper.

===================================
paper_id: 9148295; YEAR: 2007
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200 - specter
TITLE: Morphology-aware statistical machine translation based on morphs induced in an unsupervised manner
ABSTRACT: method_label: In this paper, we apply a method of unsupervised morphology learning to a state-of-the-art phrase-based statistical ma chine translation (SMT) system.
background_label: In SMT, words are traditionally used as the smallest units of translation.
background_label: Such a system generalizes poorl y to word forms that do not occur in the training data.
background_label: In particular, this is problematic for languages that are highly compounding, highly inflecting, or both.
method_label: An alternative way is to use sub-word units, such as morphemes.
method_label: We use the Morfessor algorithm to find statistical mo rphemelike units (called morphs) that can be used to reduce the size of the lexicon and improve the ability to generalize.
method_label: Transl ation and language models are trained directly on morphs instead of words.
method_label: The approach is tested on three Nordic languages (Danish, Finnish, and Swedish) that are included in the Europarl corpus consisting of the Proceedings of the European Parliament.
result_label: However, in our experiments we did not obtain higher BLEU scores for the morph model than for the standard word-based approach.
result_label: Nonetheless, the proposed morph-based solution has clear benefits, as morpho logically well motivated structures (phrases) are learned , and the proportion of words left untranslated is clearly reduced.

===================================
paper_id: 1356419; YEAR: 1998
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: The Role of Verbs in Document Analysis
ABSTRACT: background_label: We present results of two methods for assessing the event profile of news articles as a function of verb type.
objective_label: The unique contribution of this research is the focus on the role of verbs, rather than nouns.
method_label: Two algorithms are presented and evaluated, one of which is shown to accurately discriminate documents by type and semantic properties, i.e.
method_label: the event profile.
method_label: The initial method, using WordNet (Miller et al.
method_label: 1990), produced multiple cross-classification of articles, primarily due to the bushy nature of the verb tree coupled with the sense disambiguation problem.
method_label: Our second approach using English Verb Classes and Alternations (EVCA) Levin (1993) showed that monosemous categorization of the frequent verbs in WSJ made it possible to usefully discriminate documents.
result_label: For example, our results show that articles in which communication verbs predominate tend to be opinion pieces, whereas articles with a high percentage of agreement verbs tend to be about mergers or legal cases.
result_label: An evaluation is performed on the results using Kendall's Tau.
result_label: We present convincing evidence for using verb semantic classes as a discriminant in document classification.

===================================
paper_id: 13058297; YEAR: 2009
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: title_cbow200
TITLE: Learning based digital matting
ABSTRACT: background_label: We cast some new insights into solving the digital matting problem by treating it as a semi-supervised learning task in machine learning.
background_label: A local learning based approach and a global learning based approach are then produced, to fit better the scribble based matting and the trimap based matting, respectively.
method_label: Our approaches are easy to implement because only some simple matrix operations are needed.
method_label: They are also extremely accurate because they can efficiently handle the nonlinear local color distributions by incorporating the kernel trick, that are beyond the ability of many previous works.
method_label: Our approaches can outperform many recent matting methods, as shown by the theoretical analysis and comprehensive experiments.
result_label: The new insights may also inspire several more works.

===================================
paper_id: 17686875; YEAR: 1995
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: A Case Frame Learning Method for Japanese Polysemous Verbs
ABSTRACT: background_label: This paper presents a new method for learning case frames of Japanese polysemous verbs from a roughly parsed corpus when given a semantic hierarchy for nouns (thesaurus).
background_label: Japanese verbs usually have several meanings which take different case frames.
background_label: Each contains different types and numbers of case particles (case marker) which turn select different noun categories.
method_label: The proposed method employs a bottom-up covering technique to avoid combinatorial explosion of more than ten case particles in Japanese and more than 3000 semantic categories in our thesaurus.
method_label: First, a sequence of case frame candidates is produced by generalizing training instances using the thesaurus.
method_label: Then to select the most plausible frame, we introduce a new compression-based utility criteria which can uniformly compare candidates consisting of different structures.
method_label: Finally, we remove the instances covered by the frame and iterate the procedure until the utility measure becomes less than a predefined threshold.
method_label: This produces a set of case frames each corresponding to a single verb meaning.
result_label: The proposed method is experimentally evaluated by typical polysemous verbs taken from one-year newspaper articles.

===================================
paper_id: 1150510; YEAR: 2007
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: Recognizing Humor Without Recognizing Meaning
ABSTRACT: other_label: Abstract.
background_label: We present a machine learning approach for classifying sentences as one-liner jokes or normal sentences.
method_label: We use no deep analysis of the meaning to try to see if it is humorous, instead we rely on a combination of simple features to see if these are enough to detect humor.
method_label: Features such as word overlap with other jokes, presence of words common in jokes, ambiguity and word overlap with common idioms turn out to be useful.
result_label: When training and testing on equal amounts of jokes and sentences from the British National Corpus, a classification accuracy of 85% is achieved.

===================================
paper_id: 14799233; YEAR: 1994
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Population Based Incremental Learning: A Method for Integrating Genetic Search Based Function Optimization and Competitve Learning
ABSTRACT: background_label: Genetic algorithms (GAs) are biologically motivated adaptive systems which have been used, with varying degrees of success, for function optimization.
background_label: In this study, an abstraction of the basic genetic algorithm, the Equilibrium Genetic Algorithm (EGA), and the GA in turn, are reconsidered within the framework of competitive learning.
background_label: This new perspective reveals a number of different possibilities for performance improvements.
method_label: This paper explores population-based incremental learning (PBIL), a method of combining the mechanisms of a generational genetic algorithm with simple competitive learning.
method_label: The combination of these two methods reveals a tool which is far simpler than a GA, and which out-performs a GA on large set of optimization problems in terms of both speed and accuracy.
method_label: This paper presents an empirical analysis of where the proposed technique will outperform genetic algorithms, and describes a class of problems in which a genetic algorithm may be able to perform better.
method_label: Extensions to this algorithm are discussed and analyzed.
result_label: PBIL and extensions are compared with a standard GA on twelve problems, including standard numerical optimization functions, traditional GA test suite problems, and NP-Complete problems.

===================================
paper_id: 198179481; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: title_cbow200
TITLE: Adaptive Compression-based Lifelong Learning
ABSTRACT: background_label: The problem of a deep learning model losing performance on a previously learned task when fine-tuned to a new one is a phenomenon known as Catastrophic forgetting.
background_label: There are two major ways to mitigate this problem: either preserving activations of the initial network during training with a new task; or restricting the new network activations to remain close to the initial ones.
background_label: The latter approach falls under the denomination of lifelong learning, where the model is updated in a way that it performs well on both old and new tasks, without having access to the old task's training samples anymore.
background_label: Recently, approaches like pruning networks for freeing network capacity during sequential learning of tasks have been gaining in popularity.
method_label: Such approaches allow learning small networks while making redundant parameters available for the next tasks.
method_label: The common problem encountered with these approaches is that the pruning percentage is hard-coded, irrespective of the number of samples, of the complexity of the learning task and of the number of classes in the dataset.
method_label: We propose a method based on Bayesian optimization to perform adaptive compression/pruning of the network and show its effectiveness in lifelong learning.
method_label: Our method learns to perform heavy pruning for small and/or simple datasets while using milder compression rates for large and/or complex data.
result_label: Experiments on classification and semantic segmentation demonstrate the applicability of learning network compression, where we are able to effectively preserve performances along sequences of tasks of varying complexity.

===================================
paper_id: 12820463; YEAR: 2012
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: title_cbow200
TITLE: Dictionary learning based pan-sharpening
ABSTRACT: background_label: Pan-sharpening is an image fusion process in which high resolution (HR) panchromatic (Pan) imagery is used to sharpen the corresponding low resolution (LR) multi-spectral (MS) imagery.
background_label: Pan-sharpened MS images generally have high spatial resolutions, but exhibit color distortions.
objective_label: In this paper, we propose a dictionary learning based pan-sharpening process to reduce the color distortion caused by the interpolation of the MS imagery.
method_label: Instead of interpolating the LR MS image before fusion, we generate an improved MS image which is sparse with respect to a dictionary learned from the image data.
result_label: Our experiments on degraded QuickBird and IKONOS images demonstrate that the distortion in the MS images produced using our approach is significantly reduced.

===================================
paper_id: 118988729; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: title_tfidf
TITLE: A Microphotonic Astrocomb
ABSTRACT: background_label: One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers.
background_label: It has already been realized that the large number of exactly known optical frequencies provided by laser frequency combs ('astrocombs') can significantly surpass conventionally used hollow-cathode lamps as calibration light sources.
background_label: A remaining challenge, however, is generation of frequency combs with lines resolvable by astronomical spectrometers.
method_label: Here we demonstrate an astrocomb generated via soliton formation in an on-chip microphotonic resonator ('microresonator') with a resolvable line spacing of 23.7 GHz.
method_label: This comb is providing wavelength calibration on the 10 cm/s radial velocity level on the GIANO-B high-resolution near-infrared spectrometer.
result_label: As such, microresonator frequency combs have the potential of providing broadband wavelength calibration for the next-generation of astronomical instruments in planet-hunting and cosmological research.

===================================
paper_id: 8414529; YEAR: 2009
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: specter
TITLE: Towards a universal wordnet by learning from combined evidence
ABSTRACT: background_label: Lexical databases are invaluable sources of knowledge about words and their meanings, with numerous applications in areas like NLP, IR, and AI.
objective_label: We propose a methodology for the automatic construction of a large-scale multilingual lexical database where words of many languages are hierarchically organized in terms of their meanings and their semantic relations to other words.
method_label: This resource is bootstrapped from WordNet, a well-known English-language resource.
method_label: Our approach extends WordNet with around 1.5 million meaning links for 800,000 words in over 200 languages, drawing on evidence extracted from a variety of resources including existing (monolingual) wordnets, (mostly bilingual) translation dictionaries, and parallel corpora.
method_label: Graph-based scoring functions and statistical learning techniques are used to iteratively integrate this information and build an output graph.
result_label: Experiments show that this wordnet has a high level of precision and coverage, and that it can be useful in applied tasks such as cross-lingual text classification.

===================================
paper_id: 44069964; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: A discriminative learning based approach for automated nasopharyngeal carcinoma segmentation leveraging multi-modality similarity metric learning
ABSTRACT: background_label: The combination of imaging information from multi-modality images may be highly beneficial for radiotherapy treatment planning in terms of tumor delineation.
objective_label: This paper proposes a discriminative learning based approach for automated nasopharyngeal carcinoma (NPC) segmentation using multi-modality images.
method_label: Specially, an image-patch-based multi-modality convolutional neural network (CNN) is designed to jointly learn a multi-modality similarity metric and classification of paired image patch of different modalities.
method_label: The CNN integrates two normal classification sub-networks into a Siamese-like sub-network.
method_label: With the help of the multi-modality similarity metric learning provided by the Siamese-like sub-network, the classification sub-networks are able to take advantage of each other's multimodal information.
method_label: Validation of our method was performed on 50 CT-MR subjects.
result_label: Experimental results demonstrate our method achieves improved segmentation performance compared to its counterpart without multi-modality similarity metric learning and the segmentation method of solely using CT, with a Dice Similarity Coefficient metric of 0.712 compared to 0.659 and 0.636.

===================================
paper_id: 24855552; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: abs_tfidf
TITLE: A semi-supervised learning framework for biomedical event extraction based on hidden topics.
ABSTRACT: background_label: OBJECTIVES Scientists have devoted decades of efforts to understanding the interaction between proteins or RNA production.
background_label: The information might empower the current knowledge on drug reactions or the development of certain diseases.
background_label: Nevertheless, due to the lack of explicit structure, literature in life science, one of the most important sources of this information, prevents computer-based systems from accessing.
background_label: Therefore, biomedical event extraction, automatically acquiring knowledge of molecular events in research articles, has attracted community-wide efforts recently.
background_label: Most approaches are based on statistical models, requiring large-scale annotated corpora to precisely estimate models' parameters.
background_label: However, it is usually difficult to obtain in practice.
result_label: Therefore, employing un-annotated data based on semi-supervised learning for biomedical event extraction is a feasible solution and attracts more interests.
background_label: METHODS AND MATERIAL In this paper, a semi-supervised learning framework based on hidden topics for biomedical event extraction is presented.
method_label: In this framework, sentences in the un-annotated corpus are elaborately and automatically assigned with event annotations based on their distances to these sentences in the annotated corpus.
method_label: More specifically, not only the structures of the sentences, but also the hidden topics embedded in the sentences are used for describing the distance.
method_label: The sentences and newly assigned event annotations, together with the annotated corpus, are employed for training.
result_label: RESULTS Experiments were conducted on the multi-level event extraction corpus, a golden standard corpus.
result_label: Experimental results show that more than 2.2% improvement on F-score on biomedical event extraction is achieved by the proposed framework when compared to the state-of-the-art approach.
result_label: CONCLUSION The results suggest that by incorporating un-annotated data, the proposed framework indeed improves the performance of the state-of-the-art event extraction system and the similarity between sentences might be precisely described by hidden topics and structures of the sentences.

===================================
paper_id: 12646358; YEAR: 1996
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: abs_cbow200
TITLE: The Complexity and Entropy of Literary Styles
ABSTRACT: background_label: Since Shannon's original experiment in 1951, several methods have been applied to the problem of determining the entropy of English text.
background_label: These methods were based either on prediction by human subjects, or on computer-implemented parametric models for the data, of a certain Markov order.
background_label: We ask why computer-based experiments almost always yield much higher entropy estimates than the ones produced by humans.
objective_label: We argue that there are two main reasons for this discrepancy.
method_label: First, the long-range correlations of English text are not captured by Markovian models and, second, computerbased models only take advantage of the text statistics without being able to \understand" the contextual structure and the semantics of the given text.
objective_label: The second question we address is what does the \entropy" of a text say about the author's literary style.
objective_label: In particular, is there an intuitive notion of \complexity of style" that is captured by the entropy?
result_label: We present preliminary results based on a non-parametric entropy estimation algorithm that o er partial answers to these questions.
background_label: These results indicate that taking long-range correlations into account signi cantly improves the entropy estimates.
background_label: We get an estimate of 1.77 bits-per-character for a onemillion-character sample taken from Jane Austen's works.
method_label: Also comparing the estimates obtained from several di erent texts provides some insight into the interpretation of the notion of \entropy" when applied to English text rather than to random processes, and the relationship between the entropy and the \literary complexity" of an author's style.
method_label: Advantages of this entropy estimation method are that it does not require prior training, it is uniformly good over di erent styles and languages, and it seems to converge reasonably fast.
method_label: This paper was submitted as a term-project for the \Special Topics in Information Theory" class EE478 taught by Prof. Tom Cover (EE Dept., Stanford Univ.)
method_label: during Spring 1996.
other_label: This work was supported in part by grants NSF #NCR-9205663, JSEP #DAAH04-94-G-0058, ARPA #J-FBI-94-218-2.
other_label: I. Kontoyiannis is with the Information Systems Laboratory Durand Bldg 141A, Stanford University, Stanford CA 94305.
other_label: Email: yiannis@isl.stanford.edu

===================================
paper_id: 15073829; YEAR: 2011
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Learning diphone-based segmentation.
ABSTRACT: background_label: This paper reconsiders the diphone-based word segmentation model of Cairns, Shillcock, Chater, and Levy (1997) and Hockema (2006), previously thought to be unlearnable.
background_label: A statistically principled learning model is developed using Bayes' theorem and reasonable assumptions about infants' implicit knowledge.
method_label: The ability to recover phrase-medial word boundaries is tested using phonetic corpora derived from spontaneous interactions with children and adults.
method_label: The (unsupervised and semi-supervised) learning models are shown to exhibit several crucial properties.
method_label: First, only a small amount of language exposure is required to achieve the model's ceiling performance, equivalent to between 1 day and 1 month of caregiver input.
method_label: Second, the models are robust to variation, both in the free parameter and the input representation.
result_label: Finally, both the learning and baseline models exhibit undersegmentation, argued to have significant ramifications for speech processing as a whole.

===================================
paper_id: 6090713; YEAR: 1999
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: cited - abs_tfidf
TITLE: Beyond Grammar : An Experience-based Theory of Language
ABSTRACT: other_label: 1.
other_label: Introduction: what are the productive units of natural language?
other_label: 2.
other_label: A DOP model for tree representations 3.
other_label: Formal stochastic language theory 4.
other_label: Parsing and disambiguation 5.
other_label: Testing DOP: redundancy vs. minimality 6.
other_label: Learning new words 7.
other_label: Learning new structures 8.
background_label: A DOP model for compositional semantic representations 9.
other_label: Speech understanding and dialogue processing 10.
other_label: DOP models for non-context-free representations 11.
other_label: Conclusion: linguistics reconsidered References.

===================================
paper_id: 36117198; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: title_tfidf
TITLE: DeepMind_Commentary
ABSTRACT: background_label: We agree with Lake and colleagues on their list of key ingredients for building humanlike intelligence, including the idea that model-based reasoning is essential.
background_label: However, we favor an approach that centers on one additional ingredient: autonomy.
objective_label: In particular, we aim toward agents that can both build and exploit their own internal models, with minimal human hand-engineering.
method_label: We believe an approach centered on autonomous learning has the greatest chance of success as we scale toward real-world complexity, tackling domains for which ready-made formal models are not available.
result_label: Here we survey several important examples of the progress that has been made toward building autonomous agents with humanlike abilities, and highlight some outstanding challenges.

===================================
paper_id: 52301596; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Deep Learning Based Rib Centerline Extraction and Labeling
ABSTRACT: background_label: Automated extraction and labeling of rib centerlines is a typically needed prerequisite for more advanced assisted reading tools that help the radiologist to efficiently inspect all 24 ribs in a CT volume.
method_label: In this paper, we combine a deep learning-based rib detection with a dedicated centerline extraction algorithm applied to the detection result for the purpose of fast, robust and accurate rib centerline extraction and labeling from CT volumes.
method_label: More specifically, we first apply a fully convolutional neural network (FCNN) to generate a probability map for detecting the first rib pair, the twelfth rib pair, and the collection of all intermediate ribs.
method_label: In a second stage, a newly designed centerline extraction algorithm is applied to this multi-label probability map.
method_label: Finally, the distinct detection of first and twelfth rib separately, allows to derive individual rib labels by simple sorting and counting the detected centerlines.
method_label: We applied our method to CT volumes from 116 patients which included a variety of different challenges and achieved a centerline accuracy of 0.787 mm with respect to manual centerline annotations.
background_label: This article is a preprint version of: Lenga M., Klinder T., B\"urger C., von Berg J., Franz A., Lorenz C. (2019) Deep Learning Based Rib Centerline Extraction and Labeling.
other_label: In: Vrtovec T., Yao J., Zheng G., Pozo J.
other_label: (eds) Computational Methods and Clinical Applications in Musculoskeletal Imaging.
other_label: MSKI 2018.
other_label: Lecture Notes in Computer Science, vol 11404.
other_label: Springer, Cham

===================================
paper_id: 3265631; YEAR: 1997
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: specter - abs_tfidf
TITLE: An Empirical Comparison of Probability Models for Dependency Grammar
ABSTRACT: background_label: This technical report is an appendix to Eisner (1996): it gives superior experimental results that were reported only in the talk version of that paper.
method_label: Eisner (1996) trained three probability models on a small set of about 4,000 conjunction-free, dependency-grammar parses derived from the Wall Street Journal section of the Penn Treebank, and then evaluated the models on a held-out test set, using a novel O(n^3) parsing algorithm.
method_label: The present paper describes some details of the experiments and repeats them with a larger training set of 25,000 sentences.
result_label: As reported at the talk, the more extensive training yields greatly improved performance.
result_label: Nearly half the sentences are parsed with no misattachments; two-thirds are parsed with at most one misattachment.
background_label: Of the models described in the original written paper, the best score is still obtained with the generative (top-down)"model C."However, slightly better models are also explored, in particular, two variants on the comprehension (bottom-up)"model B.
background_label: "The better of these has an attachment accuracy of 90%, and (unlike model C) tags words more accurately than the comparable trigram tagger.
result_label: Differences are statistically significant.
method_label: If tags are roughly known in advance, search error is all but eliminated and the new model attains an attachment accuracy of 93%.
result_label: We find that the parser of Collins (1996), when combined with a highly-trained tagger, also achieves 93% when trained and tested on the same sentences.
result_label: Similarities and differences are discussed.

===================================
paper_id: 9140590; YEAR: 1996
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: abs_cbow200
TITLE: Indexing handwriting using word matching
ABSTRACT: background_label: AbstractThere are many historical manuscripts written in a single hand which it would be useful to index.
method_label: Examples include the W. B. DuBois collection at the University of Massachusetts and the early Presidential libraries at the Library of Congress.The standard technique for indexing documents is to scan them in, convert them to machine readable form (ASCII) using Optical Character Recognition (OCR) and then index them using a text retrieval engine.
method_label: However, OCR does not work well on handwriting.Here an alternative scheme is proposed for indexing such texts.
method_label: Each page of the document is segmented into words.
method_label: The images of the words are then matched against each other to create equivalence classes (each equivalence classes contains multiple instances of the same word).
objective_label: The user then provides ASCII equivalents for say the top 2000 equivalence classes.The current paper deals with the matching aspects of this process.
result_label: Due to variations in even a single person's handwriting, it is expected that the matching will be the most difficult step in the whole process.
result_label: A matching technique based on Euclidean distance mapping is discussed.
result_label: Experiments are shown demonstrating the feasibility of the approach.

===================================
paper_id: 9280819; YEAR: 2009
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: title_tfidf
TITLE: Metric Learning for Image Alignment
ABSTRACT: background_label: Image alignment has been a long standing problem in computer vision.
background_label: Parameterized Appearance Models (PAMs) such as the Lucas-Kanade method, Eigentracking, and Active Appearance Models are commonly used to align images with respect to a template or to a previously learned model.
background_label: While PAMs have numerous advantages relative to alternate approaches, they have at least two drawbacks.
background_label: First, they are especially prone to local minima in the registration process.
background_label: Second, often few, if any, of the local minima of the cost function correspond to acceptable solutions.
method_label: To overcome these problems, this paper proposes a method to learn a metric for PAMs that explicitly optimizes that local minima occur at and only at the places corresponding to the correct fitting parameters.
method_label: To the best of our knowledge, this is the first paper to address the problem of learning a metric to explicitly model local properties of the PAMs’ error surface.
result_label: Synthetic and real examples show improvement in alignment performance in comparison with traditional approaches.
result_label: In addition, we show how the proposed criteria for a good metric can be used to select good features to track.

===================================
paper_id: 8237688; YEAR: 1999
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200
TITLE: A Knowledge-Free Method For Capitalized Word Disambiguation
ABSTRACT: background_label: In this paper we present an approach to the disambiguation of capitalized words when they are used in the positions where capitalization is expected, such as the first word in a sentence or after a period, quotes, etc..
background_label: Such words can act as proper names or can be just capitalized variants of common words.
method_label: The main feature of our approach is that it uses a minimum of prebuilt resources and tires to dynamically infer the disambiguation clues from the entire document.
result_label: The approach was thoroughly tested and achieved about 98.5% accuracy on unseen texts from The New York Times 1996 corpus.

===================================
paper_id: 1585762; YEAR: 2004
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: title_cbow200
TITLE: Interpolation-based Q-learning
ABSTRACT: background_label: We consider a variant of Q-learning in continuous state spaces under the total expected discounted cost criterion combined with local function approximation methods.
method_label: Provided that the function approximator satisfies certain interpolation properties, the resulting algorithm is shown to converge with probability one.
method_label: The limit function is shown to satisfy a fixed point equation of the Bellman type, where the fixed point operator depends on the stationary distribution of the exploration policy and the function approximation method.
method_label: The basic algorithm is extended in several ways.
method_label: In particular, a variant of the algorithm is obtained that is shown to converge in probability to the optimal Q function.
result_label: Preliminary computer simulations are presented that confirm the validity of the approach.

===================================
paper_id: 11740443; YEAR: 2004
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: abs_tfidf - abs_tfidfcbow200
TITLE: Mining Very-Non-Parallel Corpora: Parallel Sentence And Lexicon Extraction Via Bootstrapping And EM
ABSTRACT: method_label: AbstractWe present a method capable of extracting parallel sentences from far more disparate "very-non-parallel corpora" than previous "comparable corpora" methods, by exploiting bootstrapping on top of IBM Model 4 EM.Step 1 of our method, like previous methods, uses similarity measures to find matching documents in a corpus first, and then extracts parallel sentences as well as new word translations from these documents.
method_label: But unlike previous methods, we extend this with an iterative bootstrapping framework based on the principle of "find-one-get-more", which claims that documents found to contain one pair of parallel sentences must contain others even if the documents are judged to be of low similarity.
method_label: We re-match documents based on extracted sentence pairs, and refine the mining process iteratively until convergence.
method_label: This novel "find-one-get-more" principle allows us to add more parallel sentences from dissimilar documents, to the baseline set.
result_label: Experimental results show that our proposed method is nearly 50% more effective than the baseline method without iteration.
background_label: We also show that our method is effective in boosting the performance of the IBM Model 4 EM lexical learner as the latter, though stronger than Model 1 used in previous work, does not perform well on data from very-non-parallel corpus.
other_label: Figure1.
method_label: Parallel sentence and lexicon extraction via Bootstrapping and EMThe most challenging task is to extract bilingual sentences and lexicon from very-non-parallel data.
method_label: Recent work (Munteanu et al., 2004, Zhao and Vogel, 2002) on extracting parallel sentences from comparable data, and others on extracting paraphrasing sentences from monolingual corpora (Barzilay and Elhadad 2003) are based on the "find-topic-extract-sentence" principle which claims that parallel sentences only exist in document pairs with high similarity.
method_label: They all use lexical information (e.g.
result_label: word overlap, cosine similarity) to match documents first, before extracting sentences from these documents.

===================================
paper_id: 7405607; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: abs_tfidf
TITLE: Learning to Explain Entity Relationships in Knowledge Graphs
ABSTRACT: objective_label: We study the problem of explaining relationships between pairs of knowledge graph entities with human-readable descriptions.
method_label: Our method extracts and enriches sentences that refer to an entity pair from a corpus and ranks the sentences according to how well they describe the relationship between the entities.
method_label: We model this task as a learning to rank problem for sentences and employ a rich set of features.
result_label: When evaluated on a large set of manually annotated sentences, we find that our method significantly improves over state-of-the-art baseline models.

===================================
paper_id: 11969397; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: abs_tfidf
TITLE: Detecting Content-Heavy Sentences: A Cross-Language Case Study
ABSTRACT: background_label: The information conveyed by some sentences would be more easily understood by a reader if it were expressed in multiple sentences.
background_label: We call such sentences content heavy: these are possibly grammatical but difficult to comprehend, cumbersome sentences.
objective_label: In this paper we introduce the task of detecting content-heavy sentences in cross-lingual context.
method_label: Specifically we develop methods to identify sentences in Chinese for which English speakers would prefer translations consisting of more than one sentence.
method_label: We base our analysis and definitions on evidence from multiple human translations and reader preferences on flow and understandability.
result_label: We show that machine translation quality when translating content heavy sentences is markedly worse than overall quality and that this type of sentence are fairly common in Chinese news.
result_label: We demonstrate that sentence length and punctuation usage in Chinese are not sufficient clues for accurately detecting heavy sentences and present a richer classification model that accurately identifies these sentences.

===================================
paper_id: 52100716; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200 - title_tfidf
TITLE: REGAL: Representation Learning-based Graph Alignment
ABSTRACT: background_label: Problems involving multiple networks are prevalent in many scientific and other domains.
background_label: In particular, network alignment, or the task of identifying corresponding nodes in different networks, has applications across the social and natural sciences.
objective_label: Motivated by recent advancements in node representation learning for single-graph tasks, we propose REGAL (REpresentation learning-based Graph ALignment), a framework that leverages the power of automatically-learned node representations to match nodes across different graphs.
method_label: Within REGAL we devise xNetMF, an elegant and principled node embedding formulation that uniquely generalizes to multi-network problems.
result_label: Our results demonstrate the utility and promise of unsupervised representation learning-based network alignment in terms of both speed and accuracy.
result_label: REGAL runs up to 30x faster in the representation learning stage than comparable methods, outperforms existing network alignment methods by 20 to 30% accuracy on average, and scales to networks with millions of nodes each.

===================================
paper_id: 73626254; YEAR: 2012
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: abs_tfidfcbow200
TITLE: Priority Information Determining the Canonical Word Order of Written Sinhalese Sentences
ABSTRACT: background_label: The present study investigated the priority of information among case particles, thematic roles or grammatical functions in determining the canonical SOV word order of written Sinhalese.
background_label: Four types of sentences were given to native Sinhalese speakers to perform sentence correctness decisions.
background_label: The active sentences with transitive verbs in Experiment 1 and with ditransitive verbs in Experiment 2 revealed that canonical sentences (i.e., SOV or SOOV) were processed more quickly and accurately than the scrambled sentences (i.e., OSV or OSOV), which supported the existence of scrambling effects.
method_label: However, since thematic roles, case particles and grammatical functions provide the same information for the SOV canonical order, two further experiments were conducted to single out the priority of information.
method_label: In Experiment 3, native Sinhalese speakers processed passive sentences with canonical word order defined by case particles (i.e., SOV) more quickly and accurately than those defined by thematic roles (i.e., OSV).
result_label: In Experiment 4, native speakers processed potential sentences defined by grammatical functions (i.e., SOV) more quickly and accurately than the information provided by case markers (i.e., OSV).
result_label: Therefore, the present study concluded that grammatical functions play a crucial role to determine SOV canonical order.

