======================================================================
paper_id: 174799296; YEAR: 2019
TITLE: From Balustrades to Pierre Vinken: Looking for Syntax in Transformer Self-Attentions
ABSTRACT: method_label: We inspect the multi-head self-attention in Transformer NMT encoders for three source languages, looking for patterns that could have a syntactic interpretation.
result_label: In many of the attention heads, we frequently find sequences of consecutive states attending to the same position, which resemble syntactic phrases.
method_label: We propose a transparent deterministic method of quantifying the amount of syntactic information present in the self-attentions, based on automatically building and evaluating phrase-structure trees from the phrase-like sequences.
result_label: We compare the resulting trees to existing constituency treebanks, both manually and by computing precision and recall.
===================================
paper_id: 184486755; YEAR: 2019
adju relevance: Identical (+3)
difference: 2; annotator4: 1; annotator3: 3
sources: specter - abs_tfidf - title_tfidf
TITLE: Analyzing the Structure of Attention in a Transformer Language Model
ABSTRACT: background_label: The Transformer is a fully attention-based alternative to recurrent networks that has achieved state-of-the-art results across a range of NLP tasks.
objective_label: In this paper, we analyze the structure of attention in a Transformer language model, the GPT-2 small pretrained model.
method_label: We visualize attention for individual instances and analyze the interaction between attention and syntax over a large corpus.
method_label: We find that attention targets different parts of speech at different layer depths within the model, and that attention aligns with dependency relations most strongly in the middle layers.
result_label: We also find that the deepest layers of the model capture the most distant relationships.
result_label: Finally, we extract exemplar sentences that reveal highly specific patterns targeted by particular attention heads.

===================================
paper_id: 106402715; YEAR: 2019
adju relevance: Similar (+2)
difference: 0; annotator4: 2; annotator3: 2
sources: cited - abs_tfidfcbow200 - specter
TITLE: A Structural Probe for Finding Syntax in Word Representations
ABSTRACT: background_label: AbstractRecent work has improved our ability to detect linguistic knowledge in word representations.
background_label: However, current methods for detecting syntactic knowledge do not test whether syntax trees are represented in their entirety.
objective_label: In this work, we propose a structural probe, which evaluates whether syntax trees are embedded in a linear transformation of a neural network's word representation space.
method_label: The probe identifies a linear transformation under which squared L2 distance encodes the distance between words in the parse tree, and one in which squared L2 norm encodes depth in the parse tree.
result_label: Using our probe, we show that such transformations exist for both ELMo and BERT but not in baselines, providing evidence that entire syntax trees are embedded implicitly in deep models' vector geometry.

===================================
paper_id: 3353110; YEAR: 2018
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: title_tfidf
TITLE: Image Transformer
ABSTRACT: background_label: Image generation has been successfully cast as an autoregressive sequence generation or transformation problem.
background_label: Recent work has shown that self-attention is an effective way of modeling textual sequences.
method_label: In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood.
method_label: By restricting the self-attention mechanism to attend to local neighborhoods we significantly increase the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks.
result_label: While conceptually simple, our generative models significantly outperform the current state of the art in image generation on ImageNet, improving the best published negative log-likelihood on ImageNet from 3.83 to 3.77.
result_label: We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture.
result_label: In a human evaluation study, we find that images generated by our super-resolution model fool human observers three times more often than the previous state of the art.

===================================
paper_id: 52845092; YEAR: 2018
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: cited - abs_tfidfcbow200 - specter
TITLE: Language Modeling Teaches You More Syntax than Translation Does: Lessons Learned Through Auxiliary Task Analysis
ABSTRACT: background_label: Recent work using auxiliary prediction task classifiers to investigate the properties of LSTM representations has begun to shed light on why pretrained representations, like ELMo (Peters et al., 2018) and CoVe (McCann et al., 2017), are so beneficial for neural language understanding models.
background_label: We still, though, do not yet have a clear understanding of how the choice of pretraining objective affects the type of linguistic information that models learn.
method_label: With this in mind, we compare four objectives---language modeling, translation, skip-thought, and autoencoding---on their ability to induce syntactic and part-of-speech information.
method_label: We make a fair comparison between the tasks by holding constant the quantity and genre of the training data, as well as the LSTM architecture.
method_label: We find that representations from language models consistently perform best on our syntactic auxiliary prediction tasks, even when trained on relatively small amounts of data.
result_label: These results suggest that language modeling may be the best data-rich pretraining task for transfer learning applications requiring syntactic information.
result_label: We also find that the representations from randomly-initialized, frozen LSTMs perform strikingly well on our syntactic auxiliary tasks, but this effect disappears when the amount of training data for the auxiliary tasks is reduced.

===================================
paper_id: 21663989; YEAR: 2018
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: cited - abs_tfidfcbow200 - specter
TITLE: Deep RNNs Encode Soft Hierarchical Syntax
ABSTRACT: background_label: We present a set of experiments to demonstrate that deep recurrent neural networks (RNNs) learn internal representations that capture soft hierarchical notions of syntax from highly varied supervision.
method_label: We consider four syntax tasks at different depths of the parse tree; for each word, we predict its part of speech as well as the first (parent), second (grandparent) and third level (great-grandparent) constituent labels that appear above it.
method_label: These predictions are made from representations produced at different depths in networks that are pretrained with one of four objectives: dependency parsing, semantic role labeling, machine translation, or language modeling.
result_label: In every case, we find a correspondence between network depth and syntactic depth, suggesting that a soft syntactic hierarchy emerges.
result_label: This effect is robust across all conditions, indicating that the models encode significant amounts of syntax even in the absence of an explicit syntactic training supervision.

===================================
paper_id: 84841767; YEAR: 2019
adju relevance: Related (+1)
difference: 1; annotator4: 0; annotator3: 1
sources: cited - abs_tfidfcbow200 - specter
TITLE: Linguistic Knowledge and Transferability of Contextual Representations
ABSTRACT: background_label: Contextual word representations derived from large-scale neural language models are successful across a diverse set of NLP tasks, suggesting that they encode useful and transferable features of language.
background_label: To shed light on the linguistic knowledge they capture, we study the representations produced by several recent pretrained contextualizers (variants of ELMo, the OpenAI transformer language model, and BERT) with a suite of seventeen diverse probing tasks.
method_label: We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification).
method_label: To investigate the transferability of contextual word representations, we quantify differences in the transferability of individual layers within contextualizers, especially between recurrent neural networks (RNNs) and transformers.
method_label: For instance, higher layers of RNNs are more task-specific, while transformer layers do not exhibit the same monotonic trend.
method_label: In addition, to better understand what makes contextual word representations transferable, we compare language model pretraining with eleven supervised pretraining tasks.
result_label: For any given task, pretraining on a closely related task yields better performance than language model pretraining (which is better on average) when the pretraining dataset is fixed.
result_label: However, language model pretraining on more data gives the best results.

===================================
paper_id: 18193214; YEAR: 2016
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 2
sources: title_tfidf
TITLE: Supervised Attentions for Neural Machine Translation
ABSTRACT: background_label: In this paper, we improve the attention or alignment accuracy of neural machine translation by utilizing the alignments of training sentence pairs.
method_label: We simply compute the distance between the machine attentions and the"true"alignments, and minimize this cost in the training procedure.
result_label: Our experiments on large-scale Chinese-to-English task show that our model improves both translation and alignment qualities significantly over the large-vocabulary neural machine translation system, and even beats a state-of-the-art traditional syntax-based system.

===================================
paper_id: 174799480; YEAR: 2019
adju relevance: Related (+1)
difference: 1; annotator4: 2; annotator3: 1
sources: abs_tfidf
TITLE: Improving Neural Language Models by Segmenting, Attending, and Predicting the Future
ABSTRACT: background_label: Common language models typically predict the next word given the context.
method_label: In this work, we propose a method that improves language modeling by learning to align the given context and the following phrase.
method_label: The model does not require any linguistic annotation of phrase segmentation.
method_label: Instead, we define syntactic heights and phrase segmentation rules, enabling the model to automatically induce phrases, recognize their task-specific heads, and generate phrase embeddings in an unsupervised learning manner.
method_label: Our method can easily be applied to language models with different network architectures since an independent module is used for phrase induction and context-phrase alignment, and no change is required in the underlying language modeling network.
method_label: Experiments have shown that our model outperformed several strong baseline models on different data sets.
result_label: We achieved a new state-of-the-art performance of 17.4 perplexity on the Wikitext-103 dataset.
result_label: Additionally, visualizing the outputs of the phrase induction module showed that our model is able to learn approximate phrase-level structural knowledge without any annotation.

===================================
paper_id: 13756489; YEAR: 2017
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: cited - abs_tfidfcbow200 - specter
TITLE: Attention Is All You Need
ABSTRACT: background_label: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration.
background_label: The best performing models also connect the encoder and decoder through an attention mechanism.
objective_label: We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.
method_label: Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.
method_label: Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU.
result_label: On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.
result_label: We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.

===================================
paper_id: 202540407; YEAR: 2019
adju relevance: Related (+1)
difference: 1; annotator4: 0; annotator3: 1
sources: abs_tfidfcbow200 - abs_cbow200 - specter
TITLE: Self-Attention with Structural Position Representations
ABSTRACT: background_label: Although self-attention networks (SANs) have advanced the state-of-the-art on various NLP tasks, one criticism of SANs is their ability of encoding positions of input words (Shaw et al., 2018).
objective_label: In this work, we propose to augment SANs with structural position representations to model the latent structure of the input sentence, which is complementary to the standard sequential positional representations.
method_label: Specifically, we use dependency tree to represent the grammatical structure of a sentence, and propose two strategies to encode the positional relationships among words in the dependency tree.
result_label: Experimental results on NIST Chinese-to-English and WMT14 English-to-German translation tasks show that the proposed approach consistently boosts performance over both the absolute and relative sequential position representations.

===================================
paper_id: 32106959; YEAR: 2018
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: specter
TITLE: Natural Language Multitasking: Analyzing and Improving Syntactic Saliency of Hidden Representations
ABSTRACT: background_label: We train multi-task autoencoders on linguistic tasks and analyze the learned hidden sentence representations.
background_label: The representations change significantly when translation and part-of-speech decoders are added.
method_label: The more decoders a model employs, the better it clusters sentences according to their syntactic similarity, as the representation space becomes less entangled.
method_label: We explore the structure of the representation space by interpolating between sentences, which yields interesting pseudo-English sentences, many of which have recognizable syntactic structure.
result_label: Lastly, we point out an interesting property of our models: The difference-vector between two sentences can be added to change a third sentence with similar features in a meaningful way.

===================================
paper_id: 202539179; YEAR: 2019
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: abs_tfidf
TITLE: Multi-Granularity Self-Attention for Neural Machine Translation
ABSTRACT: background_label: Current state-of-the-art neural machine translation (NMT) uses a deep multi-head self-attention network with no explicit phrase information.
background_label: However, prior work on statistical machine translation has shown that extending the basic translation unit from words to phrases has produced substantial improvements, suggesting the possibility of improving NMT performance from explicit modeling of phrases.
objective_label: In this work, we present multi-granularity self-attention (Mg-Sa): a neural network that combines multi-head self-attention and phrase modeling.
method_label: Specifically, we train several attention heads to attend to phrases in either n-gram or syntactic formalism.
method_label: Moreover, we exploit interactions among phrases to enhance the strength of structure modeling - a commonly-cited weakness of self-attention.
result_label: Experimental results on WMT14 English-to-German and NIST Chinese-to-English translation tasks show the proposed approach consistently improves performance.
result_label: Targeted linguistic analysis reveals that Mg-Sa indeed captures useful phrase information at various levels of granularities.

===================================
paper_id: 12851711; YEAR: 2016
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: abs_tfidf
TITLE: Tree-to-Sequence Attentional Neural Machine Translation
ABSTRACT: background_label: Most of the existing Neural Machine Translation (NMT) models focus on the conversion of sequential data and do not directly use syntactic information.
objective_label: We propose a novel end-to-end syntactic NMT model, extending a sequence-to-sequence model with the source-side phrase structure.
method_label: Our model has an attention mechanism that enables the decoder to generate a translated word while softly aligning it with phrases as well as words of the source sentence.
result_label: Experimental results on the WAT'15 English-to-Japanese dataset demonstrate that our proposed model considerably outperforms sequence-to-sequence attentional NMT models and compares favorably with the state-of-the-art tree-to-string SMT system.

===================================
paper_id: 19206893; YEAR: 2018
adju relevance: Related (+1)
difference: 2; annotator4: 0; annotator3: 2
sources: specter
TITLE: Constituency Parsing with a Self-Attentive Encoder
ABSTRACT: background_label: We demonstrate that replacing an LSTM encoder with a self-attentive architecture can lead to improvements to a state-of-the-art discriminative constituency parser.
method_label: The use of attention makes explicit the manner in which information is propagated between different locations in the sentence, which we use to both analyze our model and propose potential improvements.
method_label: For example, we find that separating positional and content information in the encoder can lead to improved parsing accuracy.
method_label: Additionally, we evaluate different approaches for lexical representation.
result_label: Our parser achieves new state-of-the-art results for single models trained on the Penn Treebank: 93.55 F1 without the use of any external data, and 95.13 F1 when using pre-trained word representations.
result_label: Our parser also outperforms the previous best-published accuracy figures on 8 of the 9 languages in the SPMRL dataset.

===================================
paper_id: 14091946; YEAR: 2016
adju relevance: Related (+1)
difference: 1; annotator4: 0; annotator3: 1
sources: cited - abs_tfidfcbow200 - specter
TITLE: Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies
ABSTRACT: background_label: The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities.
background_label: Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations?
method_label: We begin addressing this question using number agreement in English subject-verb dependencies.
method_label: We probe the architecture's grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models.
method_label: In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1% errors), but errors increased when sequential and structural information conflicted.
method_label: The frequency of such errors rose sharply in the language-modeling setting.
result_label: We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured.

===================================
paper_id: 53083604; YEAR: 2018
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 2
sources: abs_tfidf - specter
TITLE: Phrase-level Self-Attention Networks for Universal Sentence Encoding
ABSTRACT: background_label: AbstractUniversal sentence encoding is a hot topic in recent NLP research.
background_label: Attention mechanism has been an integral part in many sentence encoding models, allowing the models to capture context dependencies regardless of the distance between elements in the sequence.
background_label: Fully attention-based models have recently attracted enormous interest due to their highly parallelizable computation and significantly less training time.
background_label: However, the memory consumption of their models grows quadratically with sentence length, and the syntactic information is neglected.
method_label: To this end, we propose Phrase-level Self-Attention Networks (PSAN) that perform self-attention across words inside a phrase to capture context dependencies at the phrase level, and use the gated memory updating mechanism to refine each word's representation hierarchically with longer-term context dependencies captured in a larger phrase.
result_label: As a result, the memory consumption can be reduced because the self-attention is performed at the phrase level instead of the sentence level.
result_label: At the same time, syntactic information can be easily integrated in the model.
result_label: Experiment results show that PSAN can achieve the state-ofthe-art transfer performance across a plethora of NLP tasks including sentence classification, natural language inference and sentence textual similarity.

===================================
paper_id: 52938959; YEAR: 2018
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 2
sources: title_tfidf - specter - abs_tfidf
TITLE: Phrase-Based Attentions
ABSTRACT: background_label: Most state-of-the-art neural machine translation systems, despite being different in architectural skeletons (e.g.
background_label: recurrence, convolutional), share an indispensable feature: the Attention.
background_label: However, most existing attention methods are token-based and ignore the importance of phrasal alignments, the key ingredient for the success of phrase-based statistical machine translation.
objective_label: In this paper, we propose novel phrase-based attention methods to model n-grams of tokens as attention entities.
result_label: We incorporate our phrase-based attentions into the recently proposed Transformer network, and demonstrate that our approach yields improvements of 1.3 BLEU for English-to-German and 0.5 BLEU for German-to-English translation tasks on WMT newstest2014 using WMT'16 training data.

===================================
paper_id: 202558670; YEAR: 2019
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_cbow200 - title_tfidf
TITLE: Dependency-Aware Named Entity Recognition with Relative and Global Attentions
ABSTRACT: background_label: Named entity recognition is one of the core tasks in NLP.
background_label: Although many improvements have been made on this task during the last years, the state-of-the-art systems do not explicitly take into account the recursive nature of language.
method_label: Instead of only treating the text as a plain sequence of words, we incorporate a linguistically-inspired way to recognize entities based on syntax and tree structures.
method_label: Our model exploits syntactic relationships among words using a Tree-LSTM guided by dependency trees.
method_label: Then, we enhance these features by applying relative and global attention mechanisms.
method_label: On the one hand, the relative attention detects the most informative words in the sentence with respect to the word being evaluated.
method_label: On the other hand, the global attention spots the most relevant words in the sequence.
method_label: Lastly, we linearly project the weighted vectors into the tagging space so that a conditional random field classifier predicts the entity labels.
result_label: Our findings show that the model detects words that disclose the entity types based on their syntactic roles in a sentence (e.g., verbs such as speak and write are attended when the entity type is PERSON, whereas meet and travel strongly relate to LOCATION).
result_label: We confirm our findings and establish a new state of the art on two datasets.

===================================
paper_id: 52100282; YEAR: 2018
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: cited - abs_tfidfcbow200 - specter
TITLE: Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures
ABSTRACT: background_label: Recently, non-recurrent architectures (convolutional, self-attentional) have outperformed RNNs in neural machine translation.
background_label: CNNs and self-attentional networks can connect distant words via shorter network paths than RNNs, and it has been speculated that this improves their ability to model long-range dependencies.
background_label: However, this theoretical argument has not been tested empirically, nor have alternative explanations for their strong performance been explored in-depth.
method_label: We hypothesize that the strong performance of CNNs and self-attentional networks could also be due to their ability to extract semantic features from the source text, and we evaluate RNNs, CNNs and self-attention networks on two tasks: subject-verb agreement (where capturing long-range dependencies is required) and word sense disambiguation (where semantic feature extraction is required).
result_label: Our experimental results show that: 1) self-attentional networks and CNNs do not outperform RNNs in modeling subject-verb agreement over long distances; 2) self-attentional networks perform distinctly better than RNNs and CNNs on word sense disambiguation.

===================================
paper_id: 7197724; YEAR: 2016
adju relevance: Related (+1)
difference: 1; annotator4: 2; annotator3: 1
sources: cited - abs_tfidfcbow200 - specter
TITLE: Does String-Based Neural MT Learn Source Syntax?
ABSTRACT: background_label: AbstractWe investigate whether a neural, encoderdecoder translation system learns syntactic information on the source side as a by-product of training.
method_label: We propose two methods to detect whether the encoder has learned local and global source syntax.
result_label: A fine-grained analysis of the syntactic structure learned by the encoder reveals which kinds of syntax are learned and which are missing.

===================================
paper_id: 202539273; YEAR: 2019
adju relevance: Related (+1)
difference: 2; annotator4: 2; annotator3: 0
sources: abs_tfidf - abs_tfidfcbow200 - abs_cbow200 - specter
TITLE: Improving Neural Machine Translation with Parent-Scaled Self-Attention
ABSTRACT: background_label: Most neural machine translation (NMT) models operate on source and target sentences, treating them as sequences of words and neglecting their syntactic structure.
background_label: Recent studies have shown that embedding the syntax information of a source sentence in recurrent neural networks can improve their translation accuracy, especially for low-resource language pairs.
background_label: However, state-of-the-art NMT models are based on self-attention networks (e.g., Transformer), in which it is still not clear how to best embed syntactic information.
objective_label: In this work, we explore different approaches to make such models syntactically aware.
method_label: Moreover, we propose a novel method to incorporate syntactic information in the self-attention mechanism of the Transformer encoder by introducing attention heads that can attend to the dependency parent of each token.
method_label: The proposed model is simple yet effective, requiring no additional parameter and improving the translation quality of the Transformer model especially for long sentences and low-resource scenarios.
result_label: We show the efficacy of the proposed approach on NC11 English-German, WMT16 and WMT17 English-German, WMT18 English-Turkish, and WAT English-Japanese translation tasks.

===================================
paper_id: 162097304; YEAR: 2002
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200
TITLE: Louis Armstrong and the Syntax of Scat
ABSTRACT: background_label: 618 Early versions of this paper were presented first inMay 2000 at “Rhythm-a-ning: A Symposium on Jazz Culture” and then in December 2000 as a lecture at the Center for Jazz Studies, both at Columbia University.
background_label: I am grateful to Robert G. O’Meally for those invitations and for discussing the project with me on a number of occasions.
background_label: The essay has also benefited from the suggestions and comments of two other audiences: the Performance of Culture seminar at the Center for the Critical Analysis of ContemporaryCulture at Rutgers University and the department of English at Johns HopkinsUniversity.
method_label: I would also like to thankMichael Cogswell at the Armstrong Archives at Queens College, Phoebe Jacobs at the Louis Armstrong Foundation, and the staff in theMusic Division at the Library of Congress.
result_label: I thank Krin Gabbard and RobertWalser for their detailed suggestions.
other_label: 1.
other_label: Boyd Atkins, “Heebie Jeebies” (Chicago, 1926), pp.
background_label: 3–4.
background_label: This source can be found in the Sheet Music Collection,Music Division, New York Public Library for the PerformingArts.
other_label: The sheet music was published after Armstrong’s single, apparently in response to the record’s popularity; it goes so far as to include a transcription of Armstrong’s improvisation as a “‘Skat’ Chorus” (p. 5), in which the piano accompanimentmirrors the melody and rhythm of Armstrong’s scatting (“Skeep!
other_label: Skipe!
other_label: Skoop!
background_label: Brip Ber Breep bar la bah”).
other_label: At the same time, the sheet music signals the inadequacy of its notation, glossing the transcriptionwith the instruction that “Note: for correct interpretation of ‘SKAT’ CHORUSHEAROKEHRECORDNo.
other_label: 8300.” Louis Armstrong and the Syntax of Scat

===================================
paper_id: 82456167; YEAR: 2007
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: Janeway's Immunobiology
ABSTRACT: background_label: Part I An Introduction to Immunobiology and Innate Immunity 1.
background_label: Basic Concepts in Immunology 2.
background_label: Innate Immunity Part II The Recognition of Antigen 3.
background_label: Antigen Recognition by B-cell and T-cell Receptors 4.
method_label: The Generation of Lymphocyte Antigen Receptors 5.
method_label: Antigen Presentation to T Lymphocytes Part III The Development of Mature Lymphocyte Receptor Repertoires 6.
method_label: Signaling Through Immune System Receptors 7.
result_label: The Development and Survival of Lymphocytes Part IV The Adaptive Immune Response 8.
background_label: T Cell-Mediated Immunity 9.
background_label: The Humoral Immune Response 10.
background_label: Dynamics of Adaptive Immunity 11.
other_label: The Mucosal Immune System Part V The Immune System in Health and Disease 12.
background_label: Failures of Host Defense Mechanism 13.
other_label: Allergy and Hypersensitivity 14.
other_label: Autoimmunity and Transplantation 15.
other_label: Manipulation of the Immune Response Part VI The Origins of Immune Responses 16.
other_label: Evolution of the Immune System Appendix I Immunologists' Toolbox Appendix II CD Antigens Appendix III Cytokines and their Receptors Appendix IV Chemokines and their Receptors Appendix V Immunological Constants

===================================
paper_id: 5279459; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Unsupervised discovery and extraction of semi-structured regions in text via self-information
ABSTRACT: background_label: We describe a general method for identifying and extracting information from semi-structured regions of text embedded within a natural language document.
background_label: These regions encode information according to ad hoc schemas and visual cues, instead of using the grammatical and presentational conventions of normal sentential language.
background_label: Examples include tables, key-value listings, or repeated enumerations of properties.
background_label: Because of their generally non-sentential nature, these regions can present problems for standard information extraction algorithms.
objective_label: Unlike previous work in table extraction, which relies on a relatively noiseless two-dimensional layout, our aim is to accommodate a wide variety of structure types.
background_label: Our approach for identifying semi-structured regions is an unsupervised one, based on scoring unusual regularity inside the document.
background_label: As content in semi-structured regions are governed by a schema, the occurrence of features encompassing textual content and visual appearance would be unusual compared to those seen in sentential language.
background_label: Regularity refers to repetition of these unusual features, as semi-structured regions commonly encode more than a single row or group of information.
method_label: To score this, we present a measure based on expected self-information, derived from statistics over patterns of textual categories and visual layout.
result_label: We describe the results of an initial study to assess the ability of these measures to detect semi-structured text in a corpus culled from the web, and show that this measure outperform baseline methods on an average precision measure.
result_label: We present initial work that uses these significant patterns to generate extraction rules, and conclude with a discussion of future directions.

===================================
paper_id: 2783229; YEAR: 2002
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: "Dahntahn" Pittsburgh: Monophthongal /aw/ and Representations of Localness in Southwestern Pennsylvania
ABSTRACT: background_label: �In this paper we report on an exploratory study of the history of the monophthongization of the diphthong /aw/ in Pittsburgh, Pennsylvania.
background_label: We suggest that the persistence of this feature may be linked to the dominant role it plays in print representations of local-sounding speech.
objective_label: In sketching the history of the variable (aw) in the speech of working-class male Pittsburghers as far back as 1850 or so, this study contributes to the small body of descriptive and historical research about the North Midland speech of Pittsburgh and southwestern Pennsylvania.
other_label: In addition, the study contributes to the growing sociolinguistic literature exploring the linguistic correlates of the rapid social and economic changes of the last few decades and their effects on people’s senses of self and place (Bailey et al.
method_label: 1993; Lane 1998; Schilling-Estes 1998; Beal 1999; Milroy and Watt 1999; Dyer 2000; Royneland 2000).
objective_label: In what follows, we provide evidence for two claims.
method_label: First, we describe exploratory work that suggests that the use of the monophthongal variant of the diphthong /aw/ (as in [at] for out or [dantan] for downtown) by white, working-class male Pittsburghers is not disappearing, as might be expected on some grounds.
method_label: Second, we show that of all the features of local speech that are the object of local stereotyping (Labov 1972, 180; Labov 2001, 196–97), the monophthongal pronunciation of /aw/ is by far the most salient, as measured by the frequency with which it is represented in the popular print media via nonstandard spelling.
result_label: Pittsburghers tell each other over and over, in newspaper cartoons, editorials, and articles, on t-shirts and refrigerator magnets, and in occasional explicit public debate about the role local speech should play in local life, that “real Pittsburghers” say “dahntahn” for downtown, “aht” for out, and so on.
result_label: At this point we can only suggest that the trajectory of monophthongal /aw/ in Pittsburgh may have

===================================
paper_id: 52967399; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited - abs_tfidfcbow200 - specter
TITLE: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
ABSTRACT: background_label: We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers.
background_label: Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.
method_label: As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.
method_label: BERT is conceptually simple and empirically powerful.
result_label: It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).

===================================
paper_id: 171076149; YEAR: 2008
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Neuroarthistory: From Aristotle and Pliny to Baxandall and Zeki
ABSTRACT: background_label: This provocative book offers a fascinating account of neuroarthistory, one of the newest and most exciting fields in the human sciences.
background_label: In recent decades there has been a dramatic increase in our knowledge of the visual brain.
background_label: Knowledge of phenomena such as neural plasticity and neural mirroring is making it possible to answer with a new level of precision some of the most challenging questions about both the creative process and the response to art.
result_label: Exploring the writings of major thinkers (among them Montesquieu, Burke, Kant, Marx and Freud), and leading art historians (including Pliny, Winckelmann, Ruskin, Pater, Gombrich and Baxandall), as well as artists such as Alberti and Leonardo and scientists from Aristotle to Zeki, John Onians shows how an understanding of the neural basis of the mind contributes to an understanding of all human behaviors--including art.

===================================
paper_id: 1448113; YEAR: 2011
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidfcbow200
TITLE: A Bayesian Model for Unsupervised Semantic Parsing
ABSTRACT: objective_label: AbstractWe propose a non-parametric Bayesian model for unsupervised semantic parsing.
method_label: Following Poon and Domingos (2009), we consider a semantic parsing setting where the goal is to (1) decompose the syntactic dependency tree of a sentence into fragments, (2) assign each of these fragments to a cluster of semantically equivalent syntactic structures, and (3) predict predicate-argument relations between the fragments.
method_label: We use hierarchical PitmanYor processes to model statistical dependencies between meaning representations of predicates and those of their arguments, as well as the clusters of their syntactic realizations.
method_label: We develop a modification of the MetropolisHastings split-merge sampler, resulting in an efficient inference algorithm for the model.
result_label: The method is experimentally evaluated by using the induced semantic representation for the question answering task in the biomedical domain.

===================================
paper_id: 38407095; YEAR: 2005
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited - abs_tfidfcbow200 - specter
TITLE: Europarl: A Parallel Corpus for Statistical Machine Translation
ABSTRACT: background_label: We collected a corpus of parallel text in 11 languages from the proceedings of the European Parliament, which are published on the web1.
background_label: This corpus has found widespread use in the NLP community.
objective_label: Here, we focus on its acquisition and its application as training data for statistical machine translation (SMT).
method_label: We trained SMT systems for 110 language pairs, which reveal interesting clues into the challenges ahead.

===================================
paper_id: 9648838; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: CODRA: A Novel Discriminative Framework for Rhetorical Analysis
ABSTRACT: background_label: Clauses and sentences rarely stand on their own in an actual discourse; rather, the relationship between them carries important information that allows the discourse to express a meaning as a whole beyond the sum of its individual parts.
background_label: Rhetorical analysis seeks to uncover this coherence structure.
method_label: In this article, we present CODRA— a COmplete probabilistic Discriminative framework for performing Rhetorical Analysis in accordance with Rhetorical Structure Theory, which posits a tree representation of a discourse.CODRA comprises a discourse segmenter and a discourse parser.
method_label: First, the discourse segmenter, which is based on a binary classifier, identifies the elementary discourse units in a given text.
method_label: Then the discourse parser builds a discourse tree by applying an optimal parsing algorithm to probabilities inferred from two Conditional Random Fields: one for intra-sentential parsing and the other for multi-sentential parsing.
method_label: We present two approaches to combine these two stages of parsing effectively.
result_label: By conducting a series of empirical evaluations over two different data sets, we demonstrate that CODRA significantly outperforms the state-of-the-art, often by a wide margin.
result_label: We also show that a reranking of the k-best parse hypotheses generated by CODRA can potentially improve the accuracy even further.

===================================
paper_id: 14857072; YEAR: 2008
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing
ABSTRACT: background_label: AbstractMorphological processes in Semitic languages deliver space-delimited words which introduce multiple, distinct, syntactic units into the structure of the input sentence.
background_label: These words are in turn highly ambiguous, breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.
objective_label: Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.
result_label: Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.

===================================
paper_id: 11432578; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf
TITLE: Improved Neural Machine Translation with Source Syntax
ABSTRACT: background_label: AbstractNeural Machine Translation (NMT) based on the encoder-decoder architecture has recently achieved the state-of-the-art performance.
background_label: Researchers have proven that extending word level attention to phrase level attention by incorporating source-side phrase structure can enhance the attention model and achieve promising improvement.
background_label: However, word dependencies that can be crucial to correctly understand a source sentence are not always in a consecutive fashion (i.e.
background_label: phrase structure), sometimes they can be in long distance.
background_label: Phrase structures are not the best way to explicitly model long distance dependencies.
method_label: In this paper we propose a simple but effective method to incorporate source-side long distance dependencies into NMT.
method_label: Our method based on dependency trees enriches each source state with global dependency structures, which can better capture the inherent syntactic structure of source sentences.
result_label: Experiments on Chinese-English and English-Japanese translation tasks show that our proposed method outperforms state-of-the-art SMT and NMT baselines.

===================================
paper_id: 7272384; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200
TITLE: Dynamic shift in isolating referents: From social to self-generated input
ABSTRACT: background_label: Infants as young as 6 months of age start comprehending some familiar words, yet there is little understanding of how young infants utilize information presented in their social environment in order to make sense of the world.
method_label: As an initial step, we used recent technology that allowed us to narrow in on the point of view of the infant to explore how infants' visual input is dynamically synchronized with their own participation, as well as from social input in the context of parent-child word-learning play.
result_label: The results reveal dynamic changes in the relation between infants' view and their level of participation.

===================================
paper_id: 161535030; YEAR: 1999
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: The mathemagician and pied puzzler : a collection in tribute to Martin Gardner
ABSTRACT: background_label: This volume comprises an imaginative collection of pieces created in tribute to Martin Gardner.
background_label: Perhaps best known for writing Scientific American's "Mathematical Games" column for years, Gardner used his personal exuberance and fascination with puzzles and magic to entice a wide range of readers into a world of mathematical discovery.
background_label: This tribute therefore contains pieces as widely varied as Gardner's own interests, ranging from limericks to lengthy treatises, from mathematical journal articles to personal stories.
method_label: This book makes a charming and unusual addition to any personal library.
result_label: Selected papers: - The Odyssey of the Figure Eight Puzzle by Stewart Coffin - Block-Packing Jambalaya by Bill Cutler - O'Beirne's Hexiamond by Richard K. Guy - Biblical Ladders by Donald E. Knuth - Three Limericks: On Space, Time and Speed by Tim Rowett.

===================================
paper_id: 46613837; YEAR: 2011
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Extracting spatial relations from document for geographic information retrieval
ABSTRACT: background_label: Geographic information retrieval (GIR) is developed to retrieve geographical information from unstructured text (commonly web documents).
background_label: Previous researches focus on applying traditional information retrieval (IR) techniques to GIR, such as ranking geographic relevance by vector space model (VSM).
background_label: In many cases, these keyword-based methods can not support spatial query very well.
background_label: For example, searching documents on “debris flow took place in Hunan last year”, the documents selected in this way may only contain the words “debris flow” and “Hunan” rather than refer to “debris flow actually occurred in Hunan”.
background_label: Lack of spatial relations between thematic activates (debris flow) and geographic entities (Hunan) is the key reason for this problem.
method_label: In this paper, we present a kernel-based approach and apply it in support vector machine (SVM) to extract spatial relations from free text for further GIS service and spatial reasoning.
background_label: First, we analyze the characters of spatial relation expressions in natural language and there are two types of spatial relations: topology and direction.
method_label: Both of them are used to qualitatively describe the relative positions of spatial objects to each other.
method_label: Then we explore the use of dependency tree (a dependency tree represents the grammatical dependencies in a sentence and it can be generated by syntax parser) to identify these spatial relations.
method_label: We observe that the features required to find a relationship between two spatial named entities in the same sentence is typically captured by the shortest path between the two entities in the dependency tree.
method_label: Therefore, we construct a shortest path dependency kernel for SVM to complete the task.
result_label: The experiment results show that our dependency tree kernel achieves significant improvement than previous method.

===================================
paper_id: 797950; YEAR: 2009
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf
TITLE: Clustering Words by Syntactic Similarity improves Dependency Parsing of Predicate-argument Structures
ABSTRACT: background_label: We present an approach for deriving syntactic word clusters from parsed text, grouping words according to their unlexicalized syntactic contexts.
method_label: We then explore the use of these syntactic clusters in leveraging a large corpus of trees generated by a high-accuracy parser to improve the accuracy of another parser based on a different formalism for representing a different level of sentence structure.
method_label: In our experiments, we use phrase-structure trees to produce syntactic word clusters that are used by a predicate-argument dependency parser, significantly improving its accuracy.

===================================
paper_id: 174802532; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: Syntax-Infused Variational Autoencoder for Text Generation
ABSTRACT: background_label: We present a syntax-infused variational autoencoder (SIVAE), that integrates sentences with their syntactic trees to improve the grammar of generated sentences.
background_label: Distinct from existing VAE-based text generative models, SIVAE contains two separate latent spaces, for sentences and syntactic trees.
objective_label: The evidence lower bound objective is redesigned correspondingly, by optimizing a joint distribution that accommodates two encoders and two decoders.
method_label: SIVAE works with long short-term memory architectures to simultaneously generate sentences and syntactic trees.
method_label: Two versions of SIVAE are proposed: one captures the dependencies between the latent variables through a conditional prior network, and the other treats the latent variables independently such that syntactically-controlled sentence generation can be performed.
result_label: Experimental results demonstrate the generative superiority of SIVAE on both reconstruction and targeted syntactic evaluations.
result_label: Finally, we show that the proposed models can be used for unsupervised paraphrasing given different syntactic tree templates.

===================================
paper_id: 157238803; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200
TITLE: From Isolation to Radicalization: Anti-Muslim Hostility and Support for ISIS in the West
ABSTRACT: background_label: What explains online radicalization and support for ISIS in Europe?
background_label: In the past few years, thousands in the West have become radicalized by consuming extremist content online, and many have left for Syria to join the Islamic State.
objective_label: This study examines whether anti-Muslim hostility in Europe might drive pro-ISIS radicalization.
method_label: From December 2015 to May 2016, I collected real-time and historical data on the activity of thousands of ISIS activists and the full social network of their followers on Twitter, a central platform for the organization’s recruitment efforts.
method_label: I captured and analyzed the online activity and textual content produced by ISIS supporters before their accounts were deleted from the Internet.
method_label: Using data on the geographic location of Twitter users, I matched online radicalization indicators with offline data on vote share for far-right, anti-Muslim political parties to test whether the intensity of anti-Muslim hostility at the local (neighborhood/municipality) level predicts support for ISIS on Twitter.
result_label: Results show that local-level support for far-right parties is a significant and substantively meaningful predictor of online radicalization, including posting tweets sympathizing with ISIS, describing life in ISIS-controlled territories, discussing foreign fighters, and expressing anti-West sentiment.
result_label: An analysis of high-frequency data surrounding events that stir anti-Muslim hostility, such as terrorist attacks and anti-Muslim protests in Europe, shows the same pattern.

===================================
paper_id: 201694322; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: title_tfidf - abs_tfidf
TITLE: Incorporating Source Syntax into Transformer-Based Neural Machine Translation
ABSTRACT: background_label: AbstractTransformer-based neural machine translation (NMT) has recently achieved state-ofthe-art performance on many machine translation tasks.
background_label: However, recent work (Raganato and Tiedemann, 2018; Tang et al., 2018; Tran et al., 2018) has indicated that Transformer models may not learn syntactic structures as well as their recurrent neural network-based counterparts, particularly in low-resource cases.
method_label: In this paper, we incorporate constituency parse information into a Transformer NMT model.
method_label: We leverage linearized parses of the source training sentences in order to inject syntax into the Transformer architecture without modifying it.We introduce two methods: a multi-task machine translation and parsing model with a single encoder and decoder, and a mixed encoder model that learns to translate directly from parsed and unparsed source sentences.
result_label: We evaluate our methods on low-resource translation from English into twenty target languages, showing consistent improvements of 1.3 BLEU on average across diverse target languages for the multi-task technique.
result_label: We further evaluate the models on full-scale WMT tasks, finding that the multi-task model aids low-and medium-resource NMT but degenerates high-resource English→German translation.

===================================
paper_id: 4790050; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200
TITLE: ISIS at its apogee: the Arabic discourse on Twitter and what we can learn from that about ISIS support and Foreign Fighters
ABSTRACT: background_label: We analyze 26.2 million comments published in Arabic language on Twitter, from July 2014 to January 2015, when ISIS' strength reached its peak and the group was prominently expanding the territorial area under its control.
method_label: By doing that, we are able to measure the share of support and aversion toward the Islamic State within the online Arab communities.
method_label: We then investigate two specific topics.
method_label: First, by exploiting the time-granularity of the tweets, we link the opinions with daily events to understand the main determinants of the changing trend in support toward ISIS.
result_label: Second, by taking advantage of the geographical locations of tweets, we explore the relationship between online opinions across countries and the number of foreign fighters joining ISIS.

===================================
paper_id: 36117198; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: DeepMind_Commentary
ABSTRACT: background_label: We agree with Lake and colleagues on their list of key ingredients for building humanlike intelligence, including the idea that model-based reasoning is essential.
background_label: However, we favor an approach that centers on one additional ingredient: autonomy.
objective_label: In particular, we aim toward agents that can both build and exploit their own internal models, with minimal human hand-engineering.
method_label: We believe an approach centered on autonomous learning has the greatest chance of success as we scale toward real-world complexity, tackling domains for which ready-made formal models are not available.
result_label: Here we survey several important examples of the progress that has been made toward building autonomous agents with humanlike abilities, and highlight some outstanding challenges.

===================================
paper_id: 60771008; YEAR: 2005
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200
TITLE: Verbnet: a broad-coverage, comprehensive verb lexicon
ABSTRACT: background_label: Despite the proliferation of approaches to lexicon development, the field of natural language processing has yet to develop a clear consensus on guidelines for computational verb lexicons, which has severely limited their utility in information processing applications.
background_label: James Pustejovsky's Generative Lexicon has concentrated on nouns rather than verbs.
background_label: WordNet does not provide a comprehensive account of possible syntactic frames and predicate argument structures associated with individual verb senses and ComLex provides syntactic frames but ignores sense distinctions.
background_label: Dorr's LCS lexicon attempts to address these limitations, but does not provide broad coverage of syntactic frames or different senses or links to actual instances in corpora.
method_label: In order to address this gap, we created VerbNet, a verb lexicon compatible with Word-Net but with explicitly stated syntactic and semantic information, using Levin verb classes to systematically construct lexical entries.
method_label: Classes are hierarchically organized to ensure that all their members have common semantic and syntactic properties.
method_label: Each class in the hierarchy is characterized extensionally by its set of verbs, and intensionally by syntactic frames and semantic predicates and a list of typical verb arguments.
method_label: One of VerbNet's primary applications has been as a basis for Parameterized Action Representations (PARs), which are used to animate the actions of virtual human agents in a simulated 3D environment.
result_label: In order to support the animation of the actions, PARs have to make explicit many details that are often underspecified in the language.
background_label: This detailed level of representation also provides a suitable pivot representation for generation in other natural languages, i.e., a form of interlingua.
background_label: To evaluate VerbNet's syntactic coverage it has been mapped to the Proposition Bank.
background_label: VerbNet syntactic frames account for over 84% exact matches to the frames found in PropBank.
background_label: VerbNet provides mappings between its verbs and WordNet senses and between its verbs and FameNet II frames, and mappings between the syntactic frames and Xtag tree families.
background_label: All these resources are complementary and can be used as extensions of each other.
method_label: The original set of classes described by Levin has been refined and extended in many ways through systematic efforts: the coverage experiment against PropBank corpus instances proposed a large set of new syntactic frames and a better treatment of prepositions; new classes from Korhonen and Briscoe's resource were integrated into the lexicon; and new members from the LCS database were added.
method_label: Taking advantage of VerbNet's class-based approach automatic acquisition methods were investigated.
method_label: Additional verbs derived from Kingsbury's clustering experiments and from Loper's VerbNet-WordNet correlation experiment were integrated into the lexicon.
result_label: These experiments show that it is possible to semi-automatically supplement and tune VerbNet with novel information from corpus data.
result_label: These approaches reduce the manual classification and enable easy adaptation of the lexicon to specific tasks and applications.

===================================
paper_id: 14941047; YEAR: 2007
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Discriminative word alignment by learning the alignment structure and syntactic divergence between a language pair
ABSTRACT: background_label: Discriminative approaches for word alignment have gained popularity in recent years because of the flexibility that they offer for using a large variety of features and combining information from various sources.
background_label: But, the models proposed in the past have not been able to make much use of features that capture the likelihood of an alignment structure (the set of alignment links) and the syntactic divergence between sentences in the parallel text.
background_label: This is primarily because of the limitation of their search techniques.
method_label: In this paper, we propose a generic discriminative re-ranking approach for word alignment which allows us to make use of structural features effectively.
method_label: These features are particularly useful for language pairs with high structural divergence (like English-Hindi, English-Japanese).
method_label: We have shown that by using the structural features, we have obtained a decrease of 2.3% in the absolute value of alignment error rate (AER).
result_label: When we add the cooccurence probabilities obtained from IBM model-4 to our features, we achieved the best AER (50.50) for the English-Hindi parallel corpus.

===================================
paper_id: 62172538; YEAR: 2006
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf
TITLE: A Fully-Lexicalized Probabilistic Model for Japanese Syntactic and Case Structure Analysis
ABSTRACT: background_label: We present an integrated probabilistic model for Japanese syntactic and case structure analysis.
method_label: Syntactic and case structure are simultaneously analyzed based on wide-coverage case frames that are constructed from a huge raw corpus in an unsupervised manner.
method_label: This model selects the syntactic and case structure that has the highest generative probability.
method_label: We evaluate both syntactic structure and case structure.
result_label: In particular, the experimental results for syntactic analysis on web sentences show that the proposed model significantly outperforms known syntactic analyzers.

===================================
paper_id: 9745687; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter
TITLE: Recursive Autoencoders for ITG-Based Translation
ABSTRACT: background_label: AbstractWhile inversion transduction grammar (ITG) is well suited for modeling ordering shifts between languages, how to make applying the two reordering rules (i.e., straight and inverted) dependent on actual blocks being merged remains a challenge.
objective_label: Unlike previous work that only uses boundary words, we propose to use recursive autoencoders to make full use of the entire merging blocks alternatively.
method_label: The recursive autoencoders are capable of generating vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling's perspective.
result_label: Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points.

===================================
paper_id: 462553; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf
TITLE: Translating Phrases in Neural Machine Translation
ABSTRACT: background_label: Phrases play an important role in natural language understanding and machine translation (Sag et al., 2002; Villavicencio et al., 2005).
background_label: However, it is difficult to integrate them into current neural machine translation (NMT) which reads and generates sentences word by word.
method_label: In this work, we propose a method to translate phrases in NMT by integrating a phrase memory storing target phrases from a phrase-based statistical machine translation (SMT) system into the encoder-decoder architecture of NMT.
method_label: At each decoding step, the phrase memory is first re-written by the SMT model, which dynamically generates relevant target phrases with contextual information provided by the NMT model.
method_label: Then the proposed model reads the phrase memory to make probability estimations for all phrases in the phrase memory.
method_label: If phrase generation is carried on, the NMT decoder selects an appropriate phrase from the memory to perform phrase translation and updates its decoding state by consuming the words in the selected phrase.
method_label: Otherwise, the NMT decoder generates a word from the vocabulary as the general NMT decoder does.
result_label: Experiment results on the Chinese to English translation show that the proposed model achieves significant improvements over the baseline on various test sets.

===================================
paper_id: 2180008; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: specter
TITLE: Bi-directional Attention with Agreement for Dependency Parsing
ABSTRACT: background_label: We develop a novel bi-directional attention model for dependency parsing, which learns to agree on headword predictions from the forward and backward parsing directions.
method_label: The parsing procedure for each direction is formulated as sequentially querying the memory component that stores continuous headword embeddings.
method_label: The proposed parser makes use of {\it soft} headword embeddings, allowing the model to implicitly capture high-order parsing history without dramatically increasing the computational complexity.
result_label: We conduct experiments on English, Chinese, and 12 other languages from the CoNLL 2006 shared task, showing that the proposed model achieves state-of-the-art unlabeled attachment scores on 6 languages.

===================================
paper_id: 28681432; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200
TITLE: Explainable AI: Beware of Inmates Running the Asylum Or: How I Learnt to Stop Worrying and Love the Social and Behavioural Sciences
ABSTRACT: background_label: In his seminal book `The Inmates are Running the Asylum: Why High-Tech Products Drive Us Crazy And How To Restore The Sanity' [2004, Sams Indianapolis, IN, USA], Alan Cooper argues that a major reason why software is often poorly designed (from a user perspective) is that programmers are in charge of design decisions, rather than interaction designers.
background_label: As a result, programmers design software for themselves, rather than for their target audience, a phenomenon he refers to as the `inmates running the asylum'.
objective_label: This paper argues that explainable AI risks a similar fate.
method_label: While the re-emergence of explainable AI is positive, this paper argues most of us as AI researchers are building explanatory agents for ourselves, rather than for the intended users.
result_label: But explainable AI is more likely to succeed if researchers and practitioners understand, adopt, implement, and improve models from the vast and valuable bodies of research in philosophy, psychology, and cognitive science, and if evaluation of these models is focused more on people than on technology.
result_label: From a light scan of literature, we demonstrate that there is considerable scope to infuse more results from the social and behavioural sciences into explainable AI, and present some key results from these fields that are relevant to explainable AI.

===================================
paper_id: 106868491; YEAR: 2002
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Learning to Eat Soup with a Knife: Counterinsurgency Lessons from Malaya and Vietnam
ABSTRACT: background_label: Invariably, armies are accused of preparing to fight the previous war.
background_label: In "Learning to Eat Soup with a Knife", Lieutenant Colonel John A. Nagl - a veteran of both Operation Desert Storm and the current conflict in Iraq - considers the now-crucial question of how armies adapt to changing circumstances during the course of conflicts for which they are initially unprepared.
method_label: Through the use of archival sources and interviews with participants in both engagements, Nagl compares the development of counterinsurgency doctrine and practice in the Malayan Emergency from 1948 to 1960 with what developed in the Vietnam War from 1950 to 1975.
method_label: In examining these two events, Nagl - the subject of a recent New York Times Magazine cover story by Peter Maass - argues that organizational culture is key to the ability to learn from unanticipated conditions, a variable which explains why the British army successfully conducted counterinsurgency in Malaya but why the American army failed to do so in Vietnam, treating the war instead as a conventional conflict.
method_label: Nagl concludes that the British army, because of its role as a colonial police force and the organizational characteristics created by its history and national culture, was better able to quickly learn and apply the lessons of counterinsurgency during the course of the Malayan Emergency.
result_label: With a new preface reflecting on the author's combat experience in Iraq, "Learning to Eat Soup with a Knife" is a timely examination of the lessons of previous counterinsurgency campaigns that will be hailed by both military leaders and interested civilians.

===================================
paper_id: 119425731; YEAR: 1972
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: Unzerlegbare Darstellungen I
ABSTRACT: background_label: LetK be the structure got by forgetting the composition law of morphisms in a given category.
background_label: A linear representation ofK is given by a map V associating with any morphism ϕ: a→e ofK a linear vector space map V(ϕ): V(a)→V(e).
method_label: We classify thoseK having only finitely many isomorphy classes of indecomposable linear representations.
other_label: This classification is related to an old paper by Yoshii [3].

===================================
paper_id: 5929878; YEAR: 2010
adju relevance: Irrelevant (0)
difference: 1; annotator4: -1; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: From Lawvere to Brandenburger-Keisler: interactive forms of diagonalization and self-reference
ABSTRACT: background_label: We analyze the Brandenburger-Keisler paradox in epistemic game theory, which is a `two-person version of Russell's paradox'.
objective_label: Our aim is to understand how it relates to standard one-person arguments, and why the `believes-assumes' modality used in the argument arises.
method_label: We recast it as a fixpoint result, which can be carried out in any regular category, and show how it can be reduced to a relational form of the one-person diagonal argument due to Lawvere.
method_label: We give a compositional account, which leads to simple multi-agent generalizations.
method_label: We also outline a general approach to the construction of assumption complete models.

===================================
paper_id: 2382276; YEAR: 2008
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: Sentence Simplification for Semantic Role Labeling
ABSTRACT: background_label: AbstractParse-tree paths are commonly used to incorporate information from syntactic parses into NLP systems.
background_label: These systems typically treat the paths as atomic (or nearly atomic) features; these features are quite sparse due to the immense variety of syntactic expression.
method_label: In this paper, we propose a general method for learning how to iteratively simplify a sentence, thus decomposing complicated syntax into small, easy-to-process pieces.
method_label: Our method applies a series of hand-written transformation rules corresponding to basic syntactic patternsfor example, one rule "depassivizes" a sentence.
method_label: The model is parameterized by learned weights specifying preferences for some rules over others.
method_label: After applying all possible transformations to a sentence, we are left with a set of candidate simplified sentences.
method_label: We apply our simplification system to semantic role labeling (SRL).
method_label: As we do not have labeled examples of correct simplifications, we use labeled training data for the SRL task to jointly learn both the weights of the simplification model and of an SRL model, treating the simplification as a hidden variable.
method_label: By extracting and labeling simplified sentences, this combined simplification/SRL system better generalizes across syntactic variation.
result_label: It achieves a statistically significant 1.2% F1 measure increase over a strong baseline on the Conll-2005 SRL task, attaining near-state-of-the-art performance.

===================================
paper_id: 67856251; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter
TITLE: Bridging the Gap: Attending to Discontinuity in Identification of Multiword Expressions
ABSTRACT: method_label: We introduce a new method to tag Multiword Expressions (MWEs) using a linguistically interpretable language-independent deep learning architecture.
method_label: We specifically target discontinuity, an under-explored aspect that poses a significant challenge to computational treatment of MWEs.
method_label: Two neural architectures are explored: Graph Convolutional Network (GCN) and multi-head self-attention.
method_label: GCN leverages dependency parse information, and self-attention attends to long-range relations.
method_label: We finally propose a combined model that integrates complementary information from both through a gating mechanism.
result_label: The experiments on a standard multilingual dataset for verbal MWEs show that our model outperforms the baselines not only in the case of discontinuous MWEs but also in overall F-score.

===================================
paper_id: 6219199; YEAR: 2003
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited - abs_tfidfcbow200 - specter
TITLE: Syntactic Annotation of a German Newspaper Corpus
ABSTRACT: background_label: We report on the syntactic annotation of a German newspaper corpus.
background_label: The annotations consist of context-free structures, additionally allowing crossing branches, with labeled nodes (phrases) and edges (grammatical functions).
method_label: Furthermore, we present a new, interactive semi-automatic annotation process that allows efficient and reliable annotations.
method_label: The annotation process is sped up by incrementally presenting structures and by automatically highlighting unreliable assignments.

===================================
paper_id: 18061968; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200 - abs_tfidf
TITLE: Supervised Learning of Syntactic Contexts for Uncovering Definitions and Extracting Hypernym Relations in Text Databases
ABSTRACT: other_label: Abstract.
background_label: In this paper we address the problem of automatically constructing structured knowledge from plain texts.
method_label: In particular, we present a supervised learning technique to first identify definitions in text data, while then finding hypernym relations within them making use of extracted syntactic structures.
method_label: Instead of using pattern matching methods that rely on lexico-syntactic patterns, we propose a method which only uses syntactic dependencies between terms extracted with a syntactic parser.
method_label: Our assumption is that syntax is more robust than patterns when coping with the length and the complexity of the texts.
method_label: Then, we transform the syntactic contexts of each noun in a coarse-grained textual representation, that is later fed into hyponym/hypernym-centered Support Vector Machine classifiers.
result_label: The results on an annotated dataset of definitional sentences demonstrate the validity of our approach overtaking the current state of the art.

===================================
paper_id: 118988729; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: A Microphotonic Astrocomb
ABSTRACT: background_label: One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers.
background_label: It has already been realized that the large number of exactly known optical frequencies provided by laser frequency combs ('astrocombs') can significantly surpass conventionally used hollow-cathode lamps as calibration light sources.
background_label: A remaining challenge, however, is generation of frequency combs with lines resolvable by astronomical spectrometers.
method_label: Here we demonstrate an astrocomb generated via soliton formation in an on-chip microphotonic resonator ('microresonator') with a resolvable line spacing of 23.7 GHz.
method_label: This comb is providing wavelength calibration on the 10 cm/s radial velocity level on the GIANO-B high-resolution near-infrared spectrometer.
result_label: As such, microresonator frequency combs have the potential of providing broadband wavelength calibration for the next-generation of astronomical instruments in planet-hunting and cosmological research.

===================================
paper_id: 13867055; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited - abs_tfidfcbow200 - specter
TITLE: Neural Monkey: The Current State and Beyond
ABSTRACT: background_label: AbstractNeural Monkey is an open-source toolkit for sequence-to-sequence learning.
objective_label: The focus of this paper is to present the current state of the toolkit to the intended audience, which includes students and researchers, both active in the deep learning community and newcomers.
method_label: For each of these target groups, we describe the most relevant features of the toolkit, including the simple configuration scheme, methods of model inspection that promote useful intuitions, or a modular design for easy prototyping.
method_label: We summarize relevant contributions to the research community which were made using this toolkit and discuss the characteristics of our toolkit with respect to other existing systems.
result_label: We conclude with a set of proposals for future development.

===================================
paper_id: 57613254; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: From Knowledge Representation to Argumentation in AI, Law and Policy Making. a Festscrift in Honour of Trevor Bench-Capon on the Occasion of His 60th
ABSTRACT: background_label: Trevor Bench-Capon is well recognised as an outstanding figure in Artificial Intelligence and Law, having published extensively over the long course of his career on legal knowledge representation, engineering methods for knowledge-based systems, theoretical and applied argumentation, case-based reasoning, policy-making, reasoning about evidence, and many other related topics.
background_label: He has deeply influenced many of his colleagues, particularly with his earlier work on principled methods for legal knowledge representation and the engineering of knowledge-based systems, and later with the introduction of values in the study of argumentation.
method_label: This Festschrift is in honour of Bench-Capon's work and its seminal influence.
method_label: The articles contained here by his colleagues extensively review, comment on, and extend Bench-Capon's work.
result_label: As a whole, the volume is a substantive introduction to the main topics and issues to which Bench-Capon has contributed so much.

===================================
paper_id: 914127; YEAR: 2010
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Where to Look Next? Combining Static and Dynamic Proto-objects in a TVA-based Model of Visual Attention
ABSTRACT: background_label: To decide “Where to look next ?” is a central function of the attention system of humans, animals and robots.
background_label: Control of attention depends on three factors, that is, low-level static and dynamic visual features of the environment (bottom-up), medium-level visual features of proto-objects and the task (top-down).
method_label: We present a novel integrated computational model that includes all these factors in a coherent architecture based on findings and constraints from the primate visual system.
method_label: The model combines spatially inhomogeneous processing of static features, spatio-temporal motion features and task-dependent priority control in the form of the first computational implementation of saliency computation as specified by the “Theory of Visual Attention” (TVA, [7]).
method_label: Importantly, static and dynamic processing streams are fused at the level of visual proto-objects, that is, ellipsoidal visual units that have the additional medium-level features of position, size, shape and orientation of the principal axis.
method_label: Proto-objects serve as input to the TVA process that combines top-down and bottom-up information for computing attentional priorities so that relatively complex search tasks can be implemented.
method_label: To this end, separately computed static and dynamic proto-objects are filtered and subsequently merged into one combined map of proto-objects.
method_label: For each proto-object, attentional priorities in the form of attentional weights are computed according to TVA.
method_label: The target of the next saccade is the center of gravity of the proto-object with the highest weight according to the task.
result_label: We illustrate the approach by applying it to several real world image sequences and show that it is robust to parameter variations.

===================================
paper_id: 37555979; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200
TITLE: Schemas of Marital Change: From Arranged Marriages to Eloping for Love.
ABSTRACT: background_label: In recent decades, arranged marriages have become less common in many parts of Asia.
objective_label: This paper explores people's schemas surrounding just such a marital change in one Indian village using semi-structured interviews (N=30) and ethnographic fieldwork.
method_label: Respondents categorize marriages into two main types: arranged marriages and elopements, also called love marriages.
background_label: Arranged marriages were common in the past, while elopements are now dominant.
result_label: Both types of marriages have characteristics that are perceived positively and the ideal marriage is a hybrid of the two.
result_label: Respondents ascribe the rise of love marriages to educational expansion, technological change, and foreign influence.
result_label: Many also see it as an inevitable part of a larger process of socio-economic change.
result_label: These schemas are strongly shaped by global influences, but also reflect multiple layers of local beliefs and cultures.

===================================
paper_id: 6099034; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: Spatial Transformer Networks
ABSTRACT: background_label: Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner.
objective_label: In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network.
method_label: This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process.
result_label: We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.

===================================
paper_id: 11728957; YEAR: 2008
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: Efficient sentence segmentation using syntactic features
ABSTRACT: background_label: To enable downstream language processing,automatic speech recognition output must be segmented into its individual sentences.
background_label: Previous sentence segmentation systems have typically been very local,using low-level prosodic and lexical features to independently decide whether or not to segment at each word boundary position.
background_label: In this work,we leverage global syntactic information from a syntactic parser, which is better able to capture long distance dependencies.
method_label: While some previous work has included syntactic features, ours is the first to do so in a tractable, lattice-based way, which is crucial for scaling up to long-sentence contexts.
method_label: Specifically, an initial hypothesis lattice is constructed using local features.
method_label: Candidate sentences are then assigned syntactic language model scores.
method_label: These global syntactic scores are combined with local low-level scores in a log-linear model.
result_label: The resulting system significantly outperforms the most popular long-span model for sentence segmentation (the hidden event language model) on both reference text and automatic speech recognizer output from news broadcasts.

===================================
paper_id: 13292366; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: specter
TITLE: Neural Machine Translation with Supervised Attention
ABSTRACT: background_label: The attention mechanisim is appealing for neural machine translation, since it is able to dynam- ically encode a source sentence by generating a alignment between a target word and source words.
background_label: Unfortunately, it has been proved to be worse than conventional alignment models in aligment accuracy.
objective_label: In this paper, we analyze and explain this issue from the point view of re- ordering, and propose a supervised attention which is learned with guidance from conventional alignment models.
result_label: Experiments on two Chinese-to-English translation tasks show that the super- vised attention mechanism yields better alignments leading to substantial gains over the standard attention based NMT.

===================================
paper_id: 5698842; YEAR: 2011
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited - abs_tfidfcbow200 - specter
TITLE: Apertium: a free/open-source platform for rule-based machine translation
ABSTRACT: background_label: Apertium is a free/open-source platform for rule-based machine translation.
background_label: It is being widely used to build machine translation systems for a variety of language pairs, especially in those cases (mainly with related-language pairs) where shallow transfer suffices to produce good quality translations, although it has also proven useful in assimilation scenarios with more distant pairs involved.
method_label: This article summarises the Apertium platform: the translation engine, the encoding of linguistic data, and the tools developed around the platform.
method_label: The present limitations of the platform and the challenges posed for the coming years are also discussed.
result_label: Finally, evaluation results for some of the most active language pairs are presented.
result_label: An appendix describes Apertium as a free/open-source project.

===================================
paper_id: 10015691; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: Morphological and Syntactic Case in Statistical Dependency Parsing
ABSTRACT: background_label: Most morphologically rich languages with free word order use case systems to mark the grammatical function of nominal elements, especially for the core argument functions of a verb.
background_label: The standard pipeline approach in syntactic dependency parsing assumes a complete disambiguation of morphological (case) information prior to automatic syntactic analysis.
background_label: Parsing experiments on Czech, German, and Hungarian show that this approach is susceptible to propagating morphological annotation errors when parsing languages displaying syncretism in their morphological case paradigms.
method_label: We develop a different architecture where we use case as a possibly underspecified filtering device restricting the options for syntactic analysis.
method_label: Carefully designed morpho-syntactic constraints can delimit the search space of a statistical dependency parser and exclude solutions that would violate the restrictions overtly marked in the morphology of the words in a given sentence.
result_label: The constrained system outperforms a state-of-the-art data-driven pipeline architecture, as we show experimentally, and, in addition, the parser output comes with guarantees about local and global morpho-syntactic wellformedness, which can be useful for downstream applications.

===================================
paper_id: 53629257; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 1; annotator4: 0; annotator3: 1
sources: specter
TITLE: Investigating NP-Chunking with Universal Dependencies for English
ABSTRACT: background_label: AbstractChunking is a pre-processing task generally dedicated to improving constituency parsing.
objective_label: In this paper, we want to show that universal dependency (UD) parsing can also leverage the information provided by the task of chunking even though annotated chunks are not provided with universal dependency trees.
method_label: In particular, we introduce the possibility of deducing noun-phrase (NP) chunks from universal dependencies, focusing on English as a first example.
result_label: We then demonstrate how the task of NP-chunking can benefit PoS-tagging in a multi-task learning setting -comparing two different strategies -and how it can be used as a feature for dependency parsing in order to learn enriched models.

===================================
paper_id: 53733849; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: Implanting Rational Knowledge into Distributed Representation at Morpheme Level
ABSTRACT: background_label: Previously, researchers paid no attention to the creation of unambiguous morpheme embeddings independent from the corpus, while such information plays an important role in expressing the exact meanings of words for parataxis languages like Chinese.
objective_label: In this paper, after constructing the Chinese lexical and semantic ontology based on word-formation, we propose a novel approach to implanting the structured rational knowledge into distributed representation at morpheme level, naturally avoiding heavy disambiguation in the corpus.
method_label: We design a template to create the instances as pseudo-sentences merely from the pieces of knowledge of morphemes built in the lexicon.
method_label: To exploit hierarchical information and tackle the data sparseness problem, the instance proliferation technique is applied based on similarity to expand the collection of pseudo-sentences.
method_label: The distributed representation for morphemes can then be trained on these pseudo-sentences using word2vec.
result_label: For evaluation, we validate the paradigmatic and syntagmatic relations of morpheme embeddings, and apply the obtained embeddings to word similarity measurement, achieving significant improvements over the classical models by more than 5 Spearman scores or 8 percentage points, which shows very promising prospects for adoption of the new source of knowledge.

===================================
paper_id: 91183938; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter
TITLE: A Multi-Task Approach for Disentangling Syntax and Semantics in Sentence Representations
ABSTRACT: background_label: We propose a generative model for a sentence that uses two latent variables, with one intended to represent the syntax of the sentence and the other to represent its semantics.
method_label: We show we can achieve better disentanglement between semantic and syntactic representations by training with multiple losses, including losses that exploit aligned paraphrastic sentences and word-order information.
method_label: We also investigate the effect of moving from bag-of-words to recurrent neural network modules.
result_label: We evaluate our models as well as several popular pretrained embeddings on standard semantic similarity tasks and novel syntactic similarity tasks.
result_label: Empirically, we find that the model with the best performing syntactic and semantic representations also gives rise to the most disentangled representations.

===================================
paper_id: 6771196; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited - abs_tfidfcbow200 - specter
TITLE: Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks
ABSTRACT: background_label: There is a lot of research interest in encoding variable length sentences into fixed length vectors, in a way that preserves the sentence meanings.
method_label: Two common methods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs.
method_label: The sentence vectors are used as features for subsequent machine learning tasks or for pre-training in the context of deep learning.
background_label: However, not much is known about the properties that are encoded in these sentence representations and about the language information they capture.
objective_label: We propose a framework that facilitates better understanding of the encoded representations.
method_label: We define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when using the representation as input.
method_label: We demonstrate the potential contribution of the approach by analyzing different sentence representation mechanisms.
result_label: The analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks, and on the effect of the encoded vector's dimensionality on the resulting representations.

===================================
paper_id: 1361675; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200
TITLE: From data to analysis: linking NWChem and Avogadro with the syntax and semantics of Chemical Markup Language
ABSTRACT: background_label: BACKGROUND Multidisciplinary integrated research requires the ability to couple the diverse sets of data obtained from a range of complex experiments and computer simulations.
background_label: Integrating data requires semantically rich information.
method_label: In this paper an end-to-end use of semantically rich data in computational chemistry is demonstrated utilizing the Chemical Markup Language (CML) framework.
method_label: Semantically rich data is generated by the NWChem computational chemistry software with the FoX library and utilized by the Avogadro molecular editor for analysis and visualization.
result_label: RESULTS The NWChem computational chemistry software has been modified and coupled to the FoX library to write CML compliant XML data files.
result_label: The FoX library was expanded to represent the lexical input files and molecular orbitals used by the computational chemistry software.
background_label: Draft dictionary entries and a format for molecular orbitals within CML CompChem were developed.
method_label: The Avogadro application was extended to read in CML data, and display molecular geometry and electronic structure in the GUI allowing for an end-to-end solution where Avogadro can create input structures, generate input files, NWChem can run the calculation and Avogadro can then read in and analyse the CML output produced.
method_label: The developments outlined in this paper will be made available in future releases of NWChem, FoX, and Avogadro.
method_label: CONCLUSIONS The production of CML compliant XML files for computational chemistry software such as NWChem can be accomplished relatively easily using the FoX library.
method_label: The CML data can be read in by a newly developed reader in Avogadro and analysed or visualized in various ways.
result_label: A community-based effort is needed to further develop the CML CompChem convention and dictionary.
result_label: This will enable the long-term goal of allowing a researcher to run simple "Google-style" searches of chemistry and physics and have the results of computational calculations returned in a comprehensible form alongside articles from the published literature.

===================================
paper_id: 147528281; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200
TITLE: From EPIC to EPTIC — Exploring simplification in interpreting and translation from an intermodal perspective
ABSTRACT: background_label: This article introduces EPTIC (the European Parliament Translation and Interpreting Corpus), a new bidirectional (English Italian) corpus of interpreted and translated EU Parliament proceedings.
background_label: Built as an extension of the English Italian subsection of EPIC (the European Parliament Interpreting Corpus), EPTIC is an intermodal corpus featuring the pseudo-parallel outputs of interpreting and translation processes, aligned to each other and to the corresponding source texts (speeches by MEPs and their written up versions).
method_label: As a first attempt at unearthing the potential of EPTIC, we investigate lexical simplification replicating the methodology proposed by Laviosa ( 1998a ; 1998b ), but extending it to encompass both a monolingual comparable and an intermodal perspective.
result_label: Our results indicate that the mediation process reduces complexity in both modes of language production and both language directions, with interpreters simplifying the input more than translators, and evidence of simplification being more lexical in English and more lexico-syntactic in Italian.

===================================
paper_id: 118046049; YEAR: 2004
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: The syntax of Chichewa
ABSTRACT: other_label: 1.
other_label: Introduction 2.
other_label: Phonetics and phonology 3.
background_label: Clause structure 4.
background_label: Relative clauses, clefts and question formation 5.
other_label: Argument structure and verb stem morphology 6.
other_label: Argument structure reducing suffixes 7.
result_label: The verb stem as a domain of linguistic processes.

===================================
paper_id: 52072951; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200
TITLE: Weakly-supervised Neural Semantic Parsing with a Generative Ranker
ABSTRACT: background_label: Weakly-supervised semantic parsers are trained on utterance-denotation pairs, treating logical forms as latent.
background_label: The task is challenging due to the large search space and spuriousness of logical forms.
objective_label: In this paper we introduce a neural parser-ranker system for weakly-supervised semantic parsing.
method_label: The parser generates candidate tree-structured logical forms from utterances using clues of denotations.
method_label: These candidates are then ranked based on two criterion: their likelihood of executing to the correct denotation, and their agreement with the utterance semantics.
method_label: We present a scheduled training procedure to balance the contribution of the two objectives.
method_label: Furthermore, we propose to use a neurally encoded lexicon to inject prior domain knowledge to the model.
result_label: Experiments on three Freebase datasets demonstrate the effectiveness of our semantic parser, achieving results within the state-of-the-art range.

===================================
paper_id: 6692380; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: Identifying missing dictionary entries with frequency-conserving context models
ABSTRACT: objective_label: In an effort to better understand meaning from natural language texts, we explore methods aimed at organizing lexical objects into contexts.
method_label: A number of these methods for organization fall into a family defined by word ordering.
background_label: Unlike demographic or spatial partitions of data, these collocation models are of special importance for their universal applicability.
background_label: While we are interested here in text and have framed our treatment appropriately, our work is potentially applicable to other areas of research (e.g., speech, genomics, and mobility patterns) where one has ordered categorical data, (e.g., sounds, genes, and locations).
method_label: Our approach focuses on the phrase (whether word or larger) as the primary meaning-bearing lexical unit and object of study.
method_label: To do so, we employ our previously developed framework for generating word-conserving phrase-frequency data.
method_label: Upon training our model with the Wiktionary---an extensive, online, collaborative, and open-source dictionary that contains over 100,000 phrasal-definitions---we develop highly effective filters for the identification of meaningful, missing phrase-entries.
result_label: With our predictions we then engage the editorial community of the Wiktionary and propose short lists of potential missing entries for definition, developing a breakthrough, lexical extraction technique, and expanding our knowledge of the defined English lexicon of phrases.

===================================
paper_id: 194866932; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200
TITLE: North and South as Jane Austen Fanfiction: How Gaskell’s Use of Austen’s Characters and Structure Strengthen Her Social Protest Novel
ABSTRACT: background_label: The genre of fanfiction has, arguably, existed for centuries, with many well-known pieces of literature matching the definition of “fanfiction”.
background_label: While countless classics meet the requirements of a “fanfiction” text by retelling the stories of classic figures such as King Arthur or Julius Caesar, others offer more subtle examples of early fanfiction, using characters and storylines from earlier works.
background_label: Elizabeth Gaskell’s North and South, which largely parallels Jane Austen’s Pride and Prejudice is, arguably, an example of fanfiction writing prior to the official recognition of the genre in the early 20th century.
objective_label: This paper explores Gaskell’s use of Jane Austen’s characters (namely, Fitzwilliam Darcy and Elizabeth Bennet, as well as minor archetypal characters) and story structure in her novel, North and South.
result_label: I will argue that Gaskell’s amount of “borrowing” from Austen could potentially categorize North and South as an alternate universe (AU) fanfiction of Pride and Prejudice, and will explore the ways in which this severe amount of “borrowed material” from Austen strengthens, rather than weakens, Gaskell’s novel and serves her politically motivated purposes in writing North and South.

===================================
paper_id: 63432627; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - abs_cbow200
TITLE: Clause Analysis: Using Syntactic Information to Automatically Extract Source, Subject, and Predicate from Texts with an Application to the 2008–2009 Gaza War
ABSTRACT: background_label: This article presents a new method and open source R package that uses syntactic information to automatically extract source–subject–predicate clauses .
method_label: This improves on frequency-based text analysis methods by dividing text into predicates with an identified subject and optional source, extracting the statements and actions of (political) actors as mentioned in the text.
method_label: The content of these predicates can be analyzed using existing frequency-based methods, allowing for the analysis of actions, issue positions and framing by different actors within a single text.
method_label: We show that a small set of syntactic patterns can extract clauses and identify quotes with good accuracy, significantly outperforming a baseline system based on word order.
result_label: Taking the 2008–2009 Gaza war as an example, we further show how corpus comparison and semantic network analysis applied to the results of the clause analysis can show differences in citation and framing patterns between U.S. and English-language Chinese coverage of this war.

===================================
paper_id: 11117517; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: Structured Attentions for Visual Question Answering
ABSTRACT: background_label: Visual attention, which assigns weights to image regions according to their relevance to a question, is considered as an indispensable part by most Visual Question Answering models.
background_label: Although the questions may involve complex relations among multiple regions, few attention models can effectively encode such cross-region relations.
objective_label: In this paper, we demonstrate the importance of encoding such relations by showing the limited effective receptive field of ResNet on two datasets, and propose to model the visual attention as a multivariate distribution over a grid-structured Conditional Random Field on image regions.
method_label: We demonstrate how to convert the iterative inference algorithms, Mean Field and Loopy Belief Propagation, as recurrent layers of an end-to-end neural network.
result_label: We empirically evaluated our model on 3 datasets, in which it surpasses the best baseline model of the newly released CLEVR dataset by 9.5%, and the best published model on the VQA dataset by 1.25%.
other_label: Source code is available at https: //github.com/zhuchen03/vqa-sva.

===================================
paper_id: 7113872; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Classify or Select: Neural Architectures for Extractive Document Summarization
ABSTRACT: background_label: We present two novel and contrasting Recurrent Neural Network (RNN) based architectures for extractive summarization of documents.
background_label: The Classifier based architecture sequentially accepts or rejects each sentence in the original document order for its membership in the final summary.
method_label: The Selector architecture, on the other hand, is free to pick one sentence at a time in any arbitrary order to piece together the summary.
method_label: Our models under both architectures jointly capture the notions of salience and redundancy of sentences.
method_label: In addition, these models have the advantage of being very interpretable, since they allow visualization of their predictions broken up by abstract features such as information content, salience and redundancy.
result_label: We show that our models reach or outperform state-of-the-art supervised models on two different corpora.
result_label: We also recommend the conditions under which one architecture is superior to the other based on experimental evidence.

===================================
paper_id: 84186736; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Neural Check-Worthiness Ranking with Weak Supervision: Finding Sentences for Fact-Checking
ABSTRACT: background_label: Automatic fact-checking systems detect misinformation, such as fake news, by (i) selecting check-worthy sentences for fact-checking, (ii) gathering related information to the sentences, and (iii) inferring the factuality of the sentences.
background_label: Most prior research on (i) uses hand-crafted features to select check-worthy sentences, and does not explicitly account for the recent finding that the top weighted terms in both check-worthy and non-check-worthy sentences are actually overlapping [15].
method_label: Motivated by this, we present a neural check-worthiness sentence ranking model that represents each word in a sentence by \textit{both} its embedding (aiming to capture its semantics) and its syntactic dependencies (aiming to capture its role in modifying the semantics of other terms in the sentence).
method_label: Our model is an end-to-end trainable neural network for check-worthiness ranking, which is trained on large amounts of unlabelled data through weak supervision.
result_label: Thorough experimental evaluation against state of the art baselines, with and without weak supervision, shows our model to be superior at all times (+13% in MAP and +28% at various Precision cut-offs from the best baseline with statistical significance).
result_label: Empirical analysis of the use of weak supervision, word embedding pretraining on domain-specific data, and the use of syntactic dependencies of our model reveals that check-worthy sentences contain notably more identical syntactic dependencies than non-check-worthy sentences.

===================================
paper_id: 56657817; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited - abs_tfidfcbow200 - specter
TITLE: Analysis Methods in Neural Language Processing: A Survey
ABSTRACT: background_label: The field of natural language processing has seen impressive progress in recent years, with neural network models replacing many of the traditional systems.
background_label: A plethora of new models have been proposed, many of which are thought to be opaque compared to their feature-rich counterparts.
background_label: This has led researchers to analyze, interpret, and evaluate neural networks in novel and more fine-grained ways.
method_label: In this survey paper, we review analysis methods in neural language processing, categorize them according to prominent research trends, highlight existing limitations, and point to potential directions for future work.

===================================
paper_id: 2735247; YEAR: 2012
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidfcbow200 - abs_tfidf
TITLE: Improving NLP through Marginalization of Hidden Syntactic Structure
ABSTRACT: background_label: AbstractMany NLP tasks make predictions that are inherently coupled to syntactic relations, but for many languages the resources required to provide such syntactic annotations are unavailable.
objective_label: For others it is unclear exactly how much of the syntactic annotations can be effectively leveraged with current models, and what structures in the syntactic trees are most relevant to the current task.We propose a novel method which avoids the need for any syntactically annotated data when predicting a related NLP task.
method_label: Our method couples latent syntactic representations, constrained to form valid dependency graphs or constituency parses, with the prediction task via specialized factors in a Markov random field.
method_label: At both training and test time we marginalize over this hidden structure, learning the optimal latent representations for the problem.
result_label: Results show that this approach provides significant gains over a syntactically uninformed baseline, outperforming models that observe syntax on an English relation extraction task, and performing comparably to them in semantic role labeling.

===================================
paper_id: 96440785; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: The market for paintings in the Venetian Republic from Renaissance to Rococò
ABSTRACT: background_label: We study the art market in the Venetian Republic from 1550 to 1750 analyzing the determinants of the prices (adjusted for the cost of living measured by the price of wheat) of figurative paintings.
background_label: Reputation of the painters, size of the paintings and other quantifiable factors affect prices as expected.
method_label: Other relevant factors include the placement of the paintings (on altars, ceilings or walls), whose impact reflects differences in demand elasticities.
result_label: We find evidence of the law of one price confirming price equalization between high and low demand geographical destinations and between different subjects.
result_label: Finally, in a Schumpeterian perspective, we relate the temporal trend of the price of a representative painting with waves of artistic innovations, whose peacks were in the 1500s and in the 1700s with a dark Baroque age in the intermediate century.

===================================
paper_id: 174776087; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: On the Compositionality Prediction of Noun Phrases using Poincar\'e Embeddings
ABSTRACT: background_label: The compositionality degree of multiword expressions indicates to what extent the meaning of a phrase can be derived from the meaning of its constituents and their grammatical relations.
background_label: Prediction of (non)-compositionality is a task that has been frequently addressed with distributional semantic models.
objective_label: We introduce a novel technique to blend hierarchical information with distributional information for predicting compositionality.
method_label: In particular, we use hypernymy information of the multiword and its constituents encoded in the form of the recently introduced Poincar\'e embeddings in addition to the distributional information to detect compositionality for noun phrases.
method_label: Using a weighted average of the distributional similarity and a Poincar\'e similarity function, we obtain consistent and substantial, statistically significant improvement across three gold standard datasets over state-of-the-art models based on distributional information only.
method_label: Unlike traditional approaches that solely use an unsupervised setting, we have also framed the problem as a supervised task, obtaining comparable improvements.
result_label: Further, we publicly release our Poincar\'e embeddings, which are trained on the output of handcrafted lexical-syntactic patterns on a large corpus.

===================================
paper_id: 10118836; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 1; annotator4: 0; annotator3: 1
sources: abs_tfidfcbow200 - abs_tfidf
TITLE: Word Representations, Tree Models and Syntactic Functions
ABSTRACT: background_label: Word representations induced from models with discrete latent variables (e.g.\ HMMs) have been shown to be beneficial in many NLP applications.
method_label: In this work, we exploit labeled syntactic dependency trees and formalize the induction problem as unsupervised learning of tree-structured hidden Markov models.
method_label: Syntactic functions are used as additional observed variables in the model, influencing both transition and emission components.
method_label: Such syntactic information can potentially lead to capturing more fine-grain and functional distinctions between words, which, in turn, may be desirable in many NLP applications.
method_label: We evaluate the word representations on two tasks -- named entity recognition and semantic frame identification.
result_label: We observe improvements from exploiting syntactic function information in both cases, and the results rivaling those of state-of-the-art representation learning methods.
result_label: Additionally, we revisit the relationship between sequential and unlabeled-tree models and find that the advantage of the latter is not self-evident.

===================================
paper_id: 14351497; YEAR: 1997
adju relevance: Irrelevant (0)
difference: 1; annotator4: 0; annotator3: 1
sources: title_tfidf
TITLE: Abstract syntax from concrete syntax
ABSTRACT: background_label: ABSTRACTModem Software Engineering practice advocates the development of domain-specific specification languages to characterize formally the idioms of discourse and jargon of specific problem domains.
background_label: With poorly-understood domains it is best to construct an abstract syntax to characterize the domain concepts and abstractions before developing a concrete syntax.
background_label: Often, however, a good concrete syntax exists a priori: sometimes in sophisticated formal languages characterizing (often mathematical) domains but more often in miniature, legacy-code languages, sorely in need of reverse engineering.
background_label: In such cases, it is necessary to derive an appropriate abstract syntax -or its first cousin, an object-oriented model -from the concrete syntax.
method_label: This report describes a transformation process that produces a good abstract representation from a low-level concrete syntax specification.

===================================
paper_id: 85990031; YEAR: 2005
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Castles built on sand : dysfunctionality in plankton models and the inadequacy of dialogue between biologists and modellers
ABSTRACT: background_label: Although lip service is often paid to the involvement of modellers in the design of biological experiments and to a lesser degree to a role for biologists in construction of dynamic models, on closer examination the ultimate communication failings and associated waste of effort are all too obvious.
background_label: Biologists need to work with modellers to ensure that data collected are more amenable to modelling (notably C-N-P biomass, rather than just Chl, or organism numbers), to measure the fate of non- or lesser-limiting nutrients, and the release/production of particulate and dissolved organics from organisms.
method_label: Modellers should not omit representations of biological behaviour unless it is demonstrated (empirically and/or mathematically) that it is safe to do so; the performance of each part of an ecosystem model should be demonstrated as being fit for purpose and not dysfunctional.
result_label: Modelling should be accepted as a research tool within biology and ecology with just as much emphasis as enjoyed by statistical and molecular methods.

===================================
paper_id: 52074926; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf
TITLE: Exploiting Rich Syntactic Information for Semantic Parsing with Graph-to-Sequence Model
ABSTRACT: background_label: Existing neural semantic parsers mainly utilize a sequence encoder, i.e., a sequential LSTM, to extract word order features while neglecting other valuable syntactic information such as dependency graph or constituent trees.
method_label: In this paper, we first propose to use the \textit{syntactic graph} to represent three types of syntactic information, i.e., word order, dependency and constituency features.
method_label: We further employ a graph-to-sequence model to encode the syntactic graph and decode a logical form.
result_label: Experimental results on benchmark datasets show that our model is comparable to the state-of-the-art on Jobs640, ATIS and Geo880.
result_label: Experimental results on adversarial examples demonstrate the robustness of the model is also improved by encoding more syntactic information.

===================================
paper_id: 13666623; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter
TITLE: Obligation and Prohibition Extraction Using Hierarchical RNNs
ABSTRACT: background_label: We consider the task of detecting contractual obligations and prohibitions.
method_label: We show that a self-attention mechanism improves the performance of a BILSTM classifier, the previous state of the art for this task, by allowing it to focus on indicative tokens.
method_label: We also introduce a hierarchical BILSTM, which converts each sentence to an embedding, and processes the sentence embeddings to classify each sentence.
result_label: Apart from being faster to train, the hierarchical BILSTM outperforms the flat one, even when the latter considers surrounding sentences, because the hierarchical model has a broader discourse view.

===================================
paper_id: 76666182; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200
TITLE: Looking for the Devil in the Details: Learning Trilinear Attention Sampling Network for Fine-grained Image Recognition
ABSTRACT: background_label: Learning subtle yet discriminative features (e.g., beak and eyes for a bird) plays a significant role in fine-grained image recognition.
background_label: Existing attention-based approaches localize and amplify significant parts to learn fine-grained details, which often suffer from a limited number of parts and heavy computational cost.
objective_label: In this paper, we propose to learn such fine-grained features from hundreds of part proposals by Trilinear Attention Sampling Network (TASN) in an efficient teacher-student manner.
method_label: Specifically, TASN consists of 1) a trilinear attention module, which generates attention maps by modeling the inter-channel relationships, 2) an attention-based sampler which highlights attended parts with high resolution, and 3) a feature distiller, which distills part features into a global one by weight sharing and feature preserving strategies.
result_label: Extensive experiments verify that TASN yields the best performance under the same settings with the most competitive approaches, in iNaturalist-2017, CUB-Bird, and Stanford-Cars datasets.

===================================
paper_id: 17947758; YEAR: 2008
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: A co-located interface for narration to support reconciliation in a conflict: initial results from Jewish and Palestinian youth
ABSTRACT: background_label: So called intractable conflicts may benefit from more modest and socially oriented approaches than those based on classical conflict resolution techniques.
background_label: This paper is inspired by theories on small group intervention in a conflict.
background_label: The general claim is that participants may achieve a greater understanding of and appreciation for the other's viewpoint under conditions that support partaking in a tangible joint task and creating a shared narration.
objective_label: Our goal was to design a methodology wherein the extent to which technology contributes to conflict negotiation and resolution could be assessed.
method_label: Specifically, a co-located interface for producing a joint narration as a tool for favouring reconciliation is presented and discussed.
result_label: The results of an initial set of studies where the interface was used by Arab and Jewish youth in Israel provided insight into the usability of the various components of the technology and of the paradigm.

===================================
paper_id: 2265207; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200 - specter - abs_tfidf
TITLE: Structural Embedding of Syntactic Trees for Machine Comprehension
ABSTRACT: background_label: Deep neural networks for machine comprehension typically utilizes only word or character embeddings without explicitly taking advantage of structured linguistic information such as constituency trees and dependency trees.
objective_label: In this paper, we propose structural embedding of syntactic trees (SEST), an algorithm framework to utilize structured information and encode them into vector representations that can boost the performance of algorithms for the machine comprehension.
result_label: We evaluate our approach using a state-of-the-art neural attention model on the SQuAD dataset.
result_label: Experimental results demonstrate that our model can accurately identify the syntactic boundaries of the sentences and extract answers that are syntactically coherent over the baseline methods.

===================================
paper_id: 9931139; YEAR: 2010
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Gamed-based iSTART Practice: From MiBoard to Self-Explanation Showdown
ABSTRACT: background_label: MiBoard (Multiplayer Interactive Board Game) is an online, turnbased board game that was developed to assess the integration of game characteristics (point rewards, game-like interaction, and peer feedback) and how that might affect student engagement and learning efficacy.
background_label: This online board game was designed to fit within the Extended Practice module of iSTART (Interactive Strategy Training for Active Reading and Thinking).
method_label: Unfortunately, preliminary research shows that MiBoard actually reduces engagement and does not benefit the quality of student self-explanations when compared to the original Extended Practice module.
method_label: Consequently the MiBoard framework has been revamped to create Self-Explanation Showdown, a faster-paced, less analytically oriented game that adds competition to the creation of self-explanations.
result_label: Students are evaluated on the quality of their self-explanations using the same assessment algorithms from iSTART Extended Practice module (this includes both word-based and LSA-based assessments).
result_label: The technical issues involved in development of MiBoard and Self- Explanation Showdown are described.
result_label: The lessons learned from the MiBoard experience are also discussed in this paper.

===================================
paper_id: 16977542; YEAR: 2005
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter
TITLE: Head-Driven PCFGs With Latent-Head Statistics
ABSTRACT: background_label: Although state-of-the-art parsers for natural language are lexicalized, it was recently shown that an accurate unlexicalized parser for the Penn tree-bank can be simply read off a manually refined tree-bank.
background_label: While lexicalized parsers often suffer from sparse data, manual mark-up is costly and largely based on individual linguistic intuition.
objective_label: Thus, across domains, languages, and tree-bank annotations, a fundamental question arises: Is it possible to automatically induce an accurate parser from a tree-bank without resorting to full lexicalization?
method_label: In this paper, we show how to induce head-driven probabilistic parsers with latent heads from a tree-bank.
result_label: Our automatically trained parser has a performance of 85.7% (LP/LR F1), which is already better than that of early lexicalized ones.

===================================
paper_id: 144498228; YEAR: 1991
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200
TITLE: Totalitarian Language: Orwell's Newspeak and Its Nazi and Communist Antecedents
ABSTRACT: background_label: In this analysis of the language of totalitarianism, John Wesley Young examines the manipulation of language by Nazi and Communist regimes.
background_label: Relating the language of totalitarian regimes to the language Newspeak in George Orwell's satirical novel "1984", Young addresses the similarities and differences between the real and fictional languages, demonstrates the accuracy of Newspeak, and explores the degree of control that language can exert over the thought and behaviour of a people.
background_label: Based on Orwell's own perceptions of political, literary and linguistic practices, Newspeak is used in the novel by a totalitarian dictatorship to create a distorted view of reality and to control the thought and behaviour of its subjects.
background_label: It is a language in which heretical ideas cannot be expressed or even concieved.
method_label: Young's analysis of the speeches and writings of actual dictators, particularly in Nazi Germany and the Soviet Union, reveals a similar propagandistic structure in the use, or misuse, of language, the purpose of which was to enslave and control the populace.
result_label: Young's analysis of the effects of these languages on the subjects of totalitarian dictatorships reveals that although Orwell's Newspeak was a potent instrument of control over the thought and behaviour of the citizens, the language of actual totalitarian regimes was not nearly so successful.
result_label: When official rhetoric and reality did not agree, the skepticism and cynicism of the people gave rise to the phenomenon of counterlanguages that attempted to reinscribe the reality of life under dictorial rule.
result_label: Young evaluates the extent to which thought control succeeded under the Nazi and Soviet regimes and considers the implications that these findings offer for linguistic determinism.

===================================
paper_id: 7014930; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: The Role of Context in Neural Morphological Disambiguation
ABSTRACT: background_label: AbstractLanguages with rich morphology often introduce sparsity in language processing tasks.
background_label: While morphological analyzers can reduce this sparsity by providing morpheme-level analyses for words, they will often introduce ambiguity by returning multiple analyses for the same surface form.
background_label: The problem of disambiguating between these morphological parses is further complicated by the fact that a correct parse for a word is not only dependent on the surface form but also on other words in its context.
objective_label: In this paper, we present a language-agnostic approach to morphological disambiguation.
method_label: We address the problem of using context in morphological disambiguation by presenting several LSTM-based neural architectures that encode long-range surface-level and analysis-level contextual dependencies.
method_label: We applied our approach to Turkish, Russian, and Arabic to compare effectiveness across languages, matching state-of-the-art in two of the three languages.
result_label: Our results also demonstrate that while context plays a role in learning how to disambiguate, the type and amount of context needed varies between languages based on their morphological and syntactic properties.

===================================
paper_id: 1114678; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited - abs_tfidfcbow200 - specter
TITLE: Neural Machine Translation of Rare Words with Subword Units
ABSTRACT: background_label: Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem.
background_label: Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary.
objective_label: In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units.
method_label: This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations).
result_label: We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.

===================================
paper_id: 14068874; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited - abs_tfidfcbow200 - specter
TITLE: The Stanford CoreNLP Natural Language Processing Toolkit
ABSTRACT: background_label: We describe the design and use of the Stanford CoreNLP toolkit, an extensible pipeline that provides core natural language analysis.
background_label: This toolkit is quite widely used, both in the research NLP community and also among commercial and government users of open source NLP technology.
result_label: We suggest that this follows from a simple, approachable design, straightforward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage.

===================================
paper_id: 44117283; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter
TITLE: Sluice Resolution without Hand-Crafted Features over Brittle Syntax Trees
ABSTRACT: background_label: AbstractSluice resolution in English is the problem of finding antecedents of wh-fronted ellipses.
background_label: Previous work has relied on handcrafted features over syntax trees that scale poorly to other languages and domains; in particular, to dialogue, which is one of the most interesting applications of sluice resolution.
method_label: Syntactic information is arguably important for sluice resolution, but we show that multi-task learning with partial parsing as auxiliary tasks effectively closes the gap and buys us an additional 9% error reduction over previous work.
result_label: Since we are not directly relying on features from partial parsers, our system is more robust to domain shifts, giving a 26% error reduction on embedded sluices in dialogue.

===================================
paper_id: 15149661; YEAR: 2000
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited - abs_tfidfcbow200 - specter
TITLE: Building a Treebank for French
ABSTRACT: background_label: We present a treebank project for French.
background_label: We have annotated a newspaper corpus of 1 Million words with part of speech, inflection, compounds, lemmas and constituency.
method_label: We describe the tagging and parsing phases of the project, and for each, the automatic tools, the guidelines and the validation process.
method_label: We then present some uses of the corpus as well as some directions for future work.

===================================
paper_id: 29304770; YEAR: 1985
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200
TITLE: Syllabic Consonants and Syllabification in Imdlawn Tashlhiyt Berber
ABSTRACT: background_label: Improved means for releasably locking a typewriter ball-type print head to a driving head shaft are disclosed.
method_label: The print head has a downwardly-projecting hub which is slidably received on the head shaft and is axially locked thereto by means of a spring-loaded locking pin which snaps out from its retracted position within the head shaft to rest on the top wall of the print head, the print head being axially releasable by operating a cam lever located on the print head top wall.
method_label: A key pin extending from the head shaft engages a key slot at the lower edge of the hub when the print head is seated on the drive shaft to angularly lock the print head to the head shaft.

