======================================================================
paper_id: 1791179; YEAR: 2001
TITLE: A Sequential Model for Multi-Class Classification
ABSTRACT: background_label: Many classification problems require decisions among a large number of competing classes.
background_label: These tasks, however, are not handled well by general purpose learning methods and are usually addressed in an ad-hoc fashion.
method_label: We suggest a general approach -- a sequential learning model that utilizes classifiers to sequentially restrict the number of competing classes while maintaining, with high probability, the presence of the true outcome in the candidates set.
result_label: Some theoretical and computational properties of the model are discussed and we argue that these are important in NLP-like domains.
result_label: The advantages of the model are illustrated in an experiment in part-of-speech tagging.
===================================
paper_id: 63643710; YEAR: 2016
adju relevance: Identical (+3)
difference: 1; annotator4: 2; annotator3: 3
sources: abs_tfidfcbow200 - abs_cbow200 - specter - abs_tfidf
TITLE: Multiclass Classification Through Multidimensional Clustering
ABSTRACT: background_label: Classification is one of the most important machine learning tasks in science and engineering.
background_label: However, it can be a difficult task, in particular when a high number of classes is involved.
background_label: Genetic Programming, despite its recognized successfulness in so many different domains, is one of the machine learning methods that typically struggles, and often fails, to provide accurate solutions for multiclass classification problems.
method_label: We present a novel algorithm for tree based GP that incorporates some ideas on the representation of the solution space in higher dimensions, and can be generalized to other types of GP.
result_label: We test three variants of this new approach on a large set of benchmark problems from several different sources, and observe their competitiveness against the most successful state-of-the-art classifiers like Random Forests, Random Subspaces and Multilayer Perceptron.

===================================
paper_id: 34085479; YEAR: 2015
adju relevance: Similar (+2)
difference: 0; annotator4: 2; annotator3: 2
sources: abs_tfidf - specter
TITLE: Classification with many classes: challenges and pluses
ABSTRACT: objective_label: The objective of the paper is to study accuracy of multi-class classification in high-dimensional setting, where the number of classes is also large ("large $L$, large $p$, small $n$"model).
background_label: While this problem arises in many practical applications and many techniques have been recently developed for its solution, to the best of our knowledge nobody provided a rigorous theoretical analysis of this important setup.
objective_label: The purpose of the present paper is to fill in this gap.
method_label: We consider one of the most common settings, classification of high-dimensional normal vectors where, unlike standard assumptions, the number of classes could be large.
method_label: We derive non-asymptotic conditions on effects of significant features, and the low and the upper bounds for distances between classes required for successful feature selection and classification with a given accuracy.
method_label: Furthermore, we study an asymptotic setup where the number of classes is diverging with the dimension of feature space and while the number of samples per class is possibly limited.
result_label: We point out on an interesting and, at first glance, somewhat counter-intuitive phenomenon that a large number of classes may be a"blessing"rather than a"curse"since, in certain settings, the precision of classification can improve as the number of classes grows.
result_label: This is due to more accurate feature selection since even weaker significant features, which are not sufficiently strong to be manifested in a coarse classification, being shared across the classes, have a stronger impact as the number of classes increases.
result_label: We supplement our theoretical investigation by a simulation study and a real data example where we again observe the above phenomenon.

===================================
paper_id: 11729594; YEAR: 2007
adju relevance: Similar (+2)
difference: 3; annotator4: 0; annotator3: 3
sources: title_cbow200 - title_tfidfcbow200 - specter
TITLE: Model-shared subspace boosting for multi-label classification
ABSTRACT: background_label: Typical approaches to the multi-label classification problem require learning an independent classifier for every label from all the examples and features.
background_label: This can become a computational bottleneck for sizeable datasets with a large label space.
objective_label: In this paper, we propose an efficient and effective multi-label learning algorithm called model-shared subspace boosting (MSSBoost) as an attempt to reduce the information redundancy in the learning process.
method_label: This algorithm automatically finds, shares and combines a number of base models across multiple labels, where each model is learned from random feature subspace and boots trap data samples.
method_label: The decision functions for each label are jointly estimated and thus a small number of shared subspace models can support the entire label space.
result_label: Our experimental results on both synthetic data and real multimedia collections have demonstrated that the proposed algorithm can achieve better classification performance than the non-ensemble baselineclassifiers with a significant speedup in the learning and prediction processes.
result_label: It can also use a smaller number of base models to achieve the same classification performance as its non-model-shared counterpart.

===================================
paper_id: 16456996; YEAR: 2012
adju relevance: Similar (+2)
difference: 1; annotator4: 1; annotator3: 2
sources: title_tfidf - title_cbow200 - title_tfidfcbow200 - specter
TITLE: Learning Compact Class Codes for Fast Inference in Large Multi Class Classification
ABSTRACT: other_label: Abstract.
background_label: We describe a new approach for classification with a very large number of classes where we assume some class similarity information is available, e.g.
objective_label: through a hierarchical organization.
method_label: The proposed method learns a compact binary code using such an existing similarity information defined on classes.
method_label: Binary classifiers are then trained using this code and decoding is performed using a simple nearest neighbor rule.
result_label: This strategy, related to Error Correcting Output Codes methods, is shown to perform similarly or better than the standard and efficient one-vs-all approach, with much lower inference complexity.

===================================
paper_id: 489951; YEAR: 2012
adju relevance: Similar (+2)
difference: 1; annotator4: 1; annotator3: 2
sources: title_tfidfcbow200 - title_cbow200 - title_tfidf
TITLE: Multi-class cosegmentation
ABSTRACT: background_label: Bottom-up, fully unsupervised segmentation remains a daunting challenge for computer vision.
background_label: In the cosegmentation context, on the other hand, the availability of multiple images assumed to contain instances of the same object classes provides a weak form of supervision that can be exploited by discriminative approaches.
background_label: Unfortunately, most existing algorithms are limited to a very small number of images and/or object classes (typically two of each).
objective_label: This paper proposes a novel energy-minimization approach to cosegmentation that can handle multiple classes and a significantly larger number of images.
method_label: The proposed cost function combines spectral- and discriminative-clustering terms, and it admits a probabilistic interpretation.
method_label: It is optimized using an efficient EM method, initialized using a convex quadratic approximation of the energy.
result_label: Comparative experiments show that the proposed approach matches or improves the state of the art on several standard datasets.

===================================
paper_id: 49191384; YEAR: 2018
adju relevance: Related (+1)
difference: 1; annotator4: 0; annotator3: 1
sources: title_cbow200 - title_tfidfcbow200 - specter
TITLE: SGM: Sequence Generation Model for Multi-label Classification
ABSTRACT: background_label: Multi-label classification is an important yet challenging task in natural language processing.
background_label: It is more complex than single-label classification in that the labels tend to be correlated.
background_label: Existing methods tend to ignore the correlations between labels.
background_label: Besides, different parts of the text can contribute differently for predicting different labels, which is not considered by existing models.
method_label: In this paper, we propose to view the multi-label classification task as a sequence generation problem, and apply a sequence generation model with a novel decoder structure to solve it.
result_label: Extensive experimental results show that our proposed methods outperform previous work by a substantial margin.
result_label: Further analysis of experimental results demonstrates that the proposed methods not only capture the correlations between labels, but also select the most informative words automatically when predicting different labels.

===================================
paper_id: 9790719; YEAR: 2000
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: cited - specter
TITLE: Reducing Multiclass to Binary: A Unifying Approach for Margin Classifiers
ABSTRACT: background_label: We present a unifying framework for studying the solution of multiclass categorization problems by reducing them to multiple binary problems that are then solved using a margin-based binary learning algorithm.
method_label: The proposed framework unifies some of the most popular approaches in which each class is compared against all others, or in which all pairs of classes are compared to each other, or in which output codes with error-correcting properties are used.
method_label: We propose a general method for combining the classifiers generated on the binary problems, and we prove a general empirical multiclass loss bound given the empirical loss of the individual binary learning algorithms.
method_label: The scheme and the corresponding bounds apply to many popular classification learning algorithms including support-vector machines, AdaBoost, regression, logistic regression and decision-tree algorithms.
method_label: We also give a multiclass generalization error analysis for general output codes with AdaBoost as the binary learner.
result_label: Experimental results with SVM and AdaBoost show that our scheme provides a viable alternative to the most commonly used multiclass algorithms.

===================================
paper_id: 22480768; YEAR: 2006
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: title_cbow200 - title_tfidfcbow200 - title_tfidf
TITLE: A Class-Incremental Learning Method for Multi-Class Support Vector Machines in Text Classification
ABSTRACT: objective_label: To solve multi-class problems of support vector machines (SVM) more efficiently, a novel framework, which we call class-incremental learning (CIL), is proposed in this paper.
method_label: CIL consists of two phases: incremental feature selection and incremental training, for updating the knowledge of old SVM classifiers in text classification when new classes are added to the system.
method_label: CIL reuses the old models of the classifier and learns only one binary sub-classifier with an additional phase of feature selection when a new class comes.
method_label: In the testing phase, current classifier is applied to the vectors' projections on the sub-spaces concerned.
method_label: CIL can serve as a flexible approach for all binary classification algorithms in text classification.
result_label: Our experiment shows that the CIL-based SVM was not only substantially faster in training time than the popular batch SVM learning methods such as 1-against-rest, 1-against-1 and divide-by-2 but also almost competed to the best performances in effectiveness of them

===================================
paper_id: 10782967; YEAR: 2016
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: specter - title_cbow200 - title_tfidfcbow200 - title_tfidf
TITLE: A simple technique for improving multi-class classification with neural networks
ABSTRACT: objective_label: We present a novel method to perform multi-class pattern classification with neural networks and test it on a challenging 3D hand gesture recognition problem.
method_label: Our method consists of a standard one-against-all (OAA) classification, followed by another network layer classifying the resulting class scores, possibly augmented by the original raw input vector.
method_label: This allows the network to disambiguate hard-to-separate classes as the distribution of class scores carries considerable information as well, and is in fact often used for assessing the confidence of a decision.
result_label: We show that by this approach we are able to significantly boost our results, overall as well as for particular difficult cases, on the hard 10-class gesture classification task.

===================================
paper_id: 261545; YEAR: 2008
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200 - title_tfidf
TITLE: A sequential dual method for large scale multi-class linear svms
ABSTRACT: background_label: Efficient training of direct multi-class formulations of linear Support Vector Machines is very useful in applications such as text classification with a huge number examples as well as features.
objective_label: This paper presents a fast dual method for this training.
method_label: The main idea is to sequentially traverse through the training set and optimize the dual variables associated with one example at a time.
method_label: The speed of training is enhanced further by shrinking and cooling heuristics.
result_label: Experiments indicate that our method is much faster than state of the art solvers such as bundle, cutting plane and exponentiated gradient methods.

===================================
paper_id: 3872220; YEAR: 2016
adju relevance: Related (+1)
difference: 1; annotator4: 0; annotator3: 1
sources: title_cbow200 - title_tfidfcbow200 - title_tfidf
TITLE: An Online Universal Classifier for Binary, Multi-class and Multi-label Classification
ABSTRACT: background_label: Classification involves the learning of the mapping function that associates input samples to corresponding target label.
background_label: There are two major categories of classification problems: Single-label classification and Multi-label classification.
background_label: Traditional binary and multi-class classifications are sub-categories of single-label classification.
background_label: Several classifiers are developed for binary, multi-class and multi-label classification problems, but there are no classifiers available in the literature capable of performing all three types of classification.
method_label: In this paper, a novel online universal classifier capable of performing all the three types of classification is proposed.
method_label: Being a high speed online classifier, the proposed technique can be applied to streaming data applications.
method_label: The performance of the developed classifier is evaluated using datasets from binary, multi-class and multi-label problems.
result_label: The results obtained are compared with state-of-the-art techniques from each of the classification types.

===================================
paper_id: 10201889; YEAR: 2014
adju relevance: Related (+1)
difference: 1; annotator4: 0; annotator3: 1
sources: specter
TITLE: Deep Learning for Multi-label Classification
ABSTRACT: background_label: In multi-label classification, the main focus has been to develop ways of learning the underlying dependencies between labels, and to take advantage of this at classification time.
background_label: Developing better feature-space representations has been predominantly employed to reduce complexity, e.g., by eliminating non-helpful feature attributes from the input space prior to (or during) training.
background_label: This is an important task, since many multi-label methods typically create many different copies or views of the same input data as they transform it, and considerable memory can be saved by taking advantage of redundancy.
method_label: In this paper, we show that a proper development of the feature space can make labels less interdependent and easier to model and predict at inference time.
method_label: For this task we use a deep learning approach with restricted Boltzmann machines.
method_label: We present a deep network that, in an empirical evaluation, outperforms a number of competitive methods from the literature

===================================
paper_id: 5241746; YEAR: 2005
adju relevance: Related (+1)
difference: 1; annotator4: 0; annotator3: 1
sources: specter
TITLE: Collective multi-label classification
ABSTRACT: background_label: Common approaches to multi-label classification learn independent classifiers for each category, and employ ranking or thresholding schemes for classification.
background_label: Because they do not exploit dependencies between labels, such techniques are only well-suited to problems in which categories are independent.
background_label: However, in many domains labels are highly interdependent.
objective_label: This paper explores multi-label conditional random field (CRF)classification models that directly parameterize label co-occurrences in multi-label classification.
result_label: Experiments show that the models outperform their single-label counterparts on standard text corpora.
result_label: Even when multi-labels are sparse, the models improve subset classification error by as much as 40%.

===================================
paper_id: 2284272; YEAR: 2014
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: specter
TITLE: Multi-class Open Set Recognition Using Probability of Inclusion
ABSTRACT: background_label: Abstract.The perceived success of recent visual recognition approaches has largely been derived from their performance on classification tasks, where all possible classes are known at training time.
background_label: But what about open set problems, where unknown classes appear at test time?
background_label: Intuitively, if we could accurately model just the positive data for any known class without overfitting, we could reject the large set of unknown classes even under an assumption of incomplete class knowledge.
method_label: In this paper, we formulate the problem as one of modeling positive training data at the decision boundary, where we can invoke the statistical extreme value theory.
method_label: A new algorithm called the PI -SVM is introduced for estimating the unnormalized posterior probability of class inclusion.

===================================
paper_id: 12156882; YEAR: 2009
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidf - specter
TITLE: A review on the combination of binary classifiers in multiclass problems
ABSTRACT: background_label: Several real problems involve the classification of data into categories or classes.
background_label: Given a data set containing data whose classes are known, Machine Learning algorithms can be employed for the induction of a classifier able to predict the class of new data from the same domain, performing the desired discrimination.
background_label: Some learning techniques are originally conceived for the solution of problems with only two classes, also named binary classification problems.
background_label: However, many problems require the discrimination of examples into more than two categories or classes.
objective_label: This paper presents a survey on the main strategies for the generalization of binary classifiers to problems with more than two classes, known as multiclass classification problems.
method_label: The focus is on strategies that decompose the original multiclass problem into multiple binary subtasks, whose outputs are combined to obtain the final prediction.

===================================
paper_id: 59843641; YEAR: 2007
adju relevance: Related (+1)
difference: 2; annotator4: 0; annotator3: 2
sources: abs_cbow200 - specter
TITLE: Structured Gradient Boosting
ABSTRACT: background_label: The goal of many machine learning problems can be formalized as the creation of a function that can properly classify an input vector, given a set of examples of that function.
background_label: While this formalism has produced a number of success stories, there are notable situations in which it fails.
background_label: One such situation arises when the class labels are composed of multiple variables, each of which may be correlated with all or part of the input or output vectors.
background_label: Such problems, known as structured prediction problems, are common in the fields of information retrieval, computational linguistics, and computer vision, among others.
method_label: In this dissertation, I will discuss structured prediction problems and some of the previous approaches to solving them.
method_label: I will then present a new algorithm, structured gradient boosting, that combines strong points of previous approaches while retaining their generality.
method_label: More specifically, the algorithm will combine some of the notions of margin maximization present in support vector methods with the speed and flexibility of the structured perceptron algorithm.
result_label: Finally, I will show a number of novel ways in which this algorithm can be applied effectively, highlighting applications in learning by demonstration and music information retrieval.

===================================
paper_id: 12188377; YEAR: 2008
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: title_tfidf - title_cbow200 - title_tfidfcbow200
TITLE: Adaptive Base Class Boost for Multi-class Classification
ABSTRACT: background_label: We develop the concept of ABC-Boost (Adaptive Base Class Boost) for multi-class classification and present ABC-MART, a concrete implementation of ABC-Boost.
method_label: The original MART (Multiple Additive Regression Trees) algorithm has been very successful in large-scale applications.
method_label: For binary classification, ABC-MART recovers MART.
result_label: For multi-class classification, ABC-MART considerably improves MART, as evaluated on several public data sets.

===================================
paper_id: 2876102; YEAR: 2004
adju relevance: Related (+1)
difference: 1; annotator4: 0; annotator3: 1
sources: title_cbow200 - title_tfidfcbow200 - title_tfidf
TITLE: MMAC: A New Multi-Class, Multi-Label Associative Classification Approach
ABSTRACT: background_label: Building fast and accurate classifiers for large-scale databases is an important task in data mining.
background_label: There is growing evidence that integrating classification and association rule mining together can produce more efficient and accurate classifiers than traditional classification techniques.
objective_label: In this paper, the problem of producing rules with multiple labels is investigated.
objective_label: We propose a new associative classification approach called multi-class, multi-label associative classification (MMAC).
method_label: This paper also presents three measures for evaluating the accuracy of data mining classification approaches to a wide range of traditional and multi-label classification problems.
result_label: Results for 28 different datasets show that the MMAC approach is an accurate and effective classification technique, highly competitive and scalable in comparison with other classification approaches.

===================================
paper_id: 8749298; YEAR: 2016
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: abs_tfidf
TITLE: Online Open World Recognition
ABSTRACT: background_label: As we enter into the big data age and an avalanche of images have become readily available, recognition systems face the need to move from close, lab settings where the number of classes and training data are fixed, to dynamic scenarios where the number of categories to be recognized grows continuously over time, as well as new data providing useful information to update the system.
background_label: Recent attempts, like the open world recognition framework, tried to inject dynamics into the system by detecting new unknown classes and adding them incrementally, while at the same time continuously updating the models for the known classes.
background_label: incrementally adding new classes and detecting instances from unknown classes, while at the same time continuously updating the models for the known classes.
method_label: In this paper we argue that to properly capture the intrinsic dynamic of open world recognition, it is necessary to add to these aspects (a) the incremental learning of the underlying metric, (b) the incremental estimate of confidence thresholds for the unknown classes, and (c) the use of local learning to precisely describe the space of classes.
method_label: We extend three existing metric learning algorithms towards these goals by using online metric learning.
result_label: Experimentally we validate our approach on two large-scale datasets in different learning scenarios.
method_label: For all these scenarios our proposed methods outperform their non-online counterparts.
result_label: We conclude that local and online learning is important to capture the full dynamics of open world recognition.

===================================
paper_id: 4502442; YEAR: 2005
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: A framework for kernel-based multi-category classification
ABSTRACT: background_label: AbstractA geometric framework for understanding multi-category classification is introduced, through which many existing 'all-together' algorithms can be understood.
background_label: The structure enables parsimonious optimisation, through a direct extension of the binary methodology.
method_label: The focus is on Support Vector Classification, with parallels drawn to related methods.The ability of the framework to compare algorithms is illustrated by a brief discussion of Fisher consistency.
method_label: Its utility in improving understanding of multi-category analysis is demonstrated through a derivation of improved generalisation bounds.It is also described how this architecture provides insights regarding how to further improve on the speed of existing multi-category classification algorithms.
method_label: An initial example of how this might be achieved is developed in the formulation of a straightforward multi-category Sequential Minimal Optimisation algorithm.
result_label: Proof-of-concept experimental results have shown that this, combined with the mapping of pairwise results, is comparable with benchmark optimisation speeds.

===================================
paper_id: 8896798; YEAR: 2006
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: title_tfidfcbow200 - title_cbow200 - title_tfidf
TITLE: Semi-Supervised Boosting for Multi-Class Classification
ABSTRACT: other_label: Abstract.
background_label: Most semi-supervised learning algorithms have been designed for binary classification, and are extended to multi-class classification by approaches such as one-against-the-rest.
background_label: The main shortcoming of these approaches is that they are unable to exploit the fact that each example is only assigned to one class.
background_label: Additional problems with extending semisupervised binary classifiers to multi-class problems include imbalanced classification and different output scales of different binary classifiers.
method_label: We propose a semi-supervised boosting framework, termed Multi-Class Semi-Supervised Boosting (MCSSB), that directly solves the semisupervised multi-class learning problem.
method_label: Compared to the existing semisupervised boosting methods, the proposed framework is advantageous in that it exploits both classification confidence and similarities among examples when deciding the pseudo-labels for unlabeled examples.
result_label: Empirical study with a number of UCI datasets shows that the proposed MCSSB algorithm performs better than the state-of-the-art boosting algorithms for semi-supervised learning.

===================================
paper_id: 67770291; YEAR: 2019
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: title_cbow200 - title_tfidfcbow200 - title_tfidf
TITLE: N-ary decomposition for multi-class classification
ABSTRACT: background_label: A common way of solving a multi-class classification problem is to decompose it into a collection of simpler two-class problems.
background_label: One major disadvantage is that with such a binary decomposition scheme it may be difficult to represent subtle between-class differences in many-class classification problems due to limited choices of binary-value partitions.
method_label: To overcome this challenge, we propose a new decomposition method called N-ary decomposition that decomposes the original multi-class problem into a set of simpler multi-class subproblems.
method_label: We theoretically show that the proposed N-ary decomposition could be unified into the framework of error correcting output codes and give the generalization error bound of an N-ary decomposition for multi-class classification.
result_label: Extensive experimental results demonstrate the state-of-the-art performance of our approach.

===================================
paper_id: 674508; YEAR: 2016
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: specter - title_cbow200 - title_tfidfcbow200 - title_tfidf
TITLE: A pragmatic approach to multi-class classification
ABSTRACT: background_label: We present a novel hierarchical approach to multi-class classification which is generic in that it can be applied to different classification models (e.g., support vector machines, perceptrons), and makes no explicit assumptions about the probabilistic structure of the problem as it is usually done in multi-class classification.
method_label: By adding a cascade of additional classifiers, each of which receives the previous classifier's output in addition to regular input data, the approach harnesses unused information that manifests itself in the form of, e.g., correlations between predicted classes.
result_label: Using multilayer perceptrons as a classification model, we demonstrate the validity of this approach by testing it on a complex ten-class 3D gesture recognition task.

===================================
paper_id: 16373719; YEAR: 2006
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: title_cbow200 - title_tfidfcbow200 - title_tfidf
TITLE: Dendogram based SVM for multi-class classification
ABSTRACT: objective_label: This paper presents a new approach called dendogram based support vector machines (DSVM), to treat multi-class problems.
method_label: First, the method consists to build a taxonomy of classes in an ascendant manner done by ascendant hierarchical clustering method (AHC).
method_label: Second, SVM is injected at each internal node of the taxonomy in order to separate the two subsets of the current node.
method_label: Finally, for classifying a pattern query, we present it to the "root" SVM, and then according to the output, the pattern is presented to one of the two SVMs of the subsets, and so on through the "leaf" nodes.
method_label: Therefore, the classification procedure is done in a descendant way in the taxonomy from the root through the end level which represents the classes.
method_label: The pattern is thus associated to one of last SVMs associated class.
method_label: AHC decomposition uses distance measures to investigate the class grouping in binary form at each level in the hierarchy.
method_label: SVM method requires little tuning and yields both high accuracy levels and good generalization for binary classification.
result_label: Therefore, DSVM method gives good results for multi class problems by both, training an optimal number of SVMs and rapidly classifying patterns in a descendant way by selecting an optimal set of SVMs which participate to the final decision.
result_label: The proposed method is compared to other multi-class SVM methods over several complex problems

===================================
paper_id: 938434; YEAR: 2006
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: cited - specter
TITLE: Solving Multiclass Learning Problems via Error-Correcting Output Codes
ABSTRACT: background_label: This paper presents a novel framework of error-correcting output coding (ECOC) addressing the problem of multi-class classification.
method_label: By weighting the output space of each base classifier which is trained independently, the distance function of decoding is adapted so that the samples are more discriminative.
method_label: A criterion generated over the Extended Pair Samples (EPS) is proposed to train the weights of output space.
method_label: Some properties still hold in the new framework: any classifier, as well as distance function, is still applicable.
method_label: We first conduct empirical studies on UCI datasets to verify the presented framework with four frequently used coding matrixes and then apply it in RoboCup domain to enhance the performance of agent control.
result_label: Experimental results show that our supervised learned decoding scheme improves the accuracy of classification significantly and betters the ball control of agents in a soccer game after learning from experience.

===================================
paper_id: 15781707; YEAR: 2015
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: title_tfidfcbow200
TITLE: A Novel Approach to Distributed Multi-Class SVM
ABSTRACT: background_label: With data sizes constantly expanding, and with classical machine learning algorithms that analyze such data requiring larger and larger amounts of computation time and storage space, the need to distribute computation and memory requirements among several computers has become apparent.
background_label: Although substantial work has been done in developing distributed binary SVM algorithms and multi-class SVM algorithms individually, the field of multi-class distributed SVMs remains largely unexplored.
objective_label: This research proposes a novel algorithm that implements the Support Vector Machine over a multi-class dataset and is efficient in a distributed environment (here, Hadoop).
method_label: The idea is to divide the dataset into half recursively and thus compute the optimal Support Vector Machine for this half during the training phase, much like a divide and conquer approach.
method_label: While testing, this structure has been effectively exploited to significantly reduce the prediction time.
method_label: Our algorithm has shown better computation time during the prediction phase than the traditional sequential SVM methods (One vs. One, One vs. Rest) and out-performs them as the size of the dataset grows.
method_label: This approach also classifies the data with higher accuracy than the traditional multi-class algorithms.

===================================
paper_id: 42296784; YEAR: 2019
adju relevance: Related (+1)
difference: 2; annotator4: 1; annotator3: 3
sources: title_cbow200 - title_tfidfcbow200 - specter - title_tfidf
TITLE: Binary Stochastic Representations for Large Multi-class Classification
ABSTRACT: background_label: Classification with a large number of classes is a key problem in machine learning and corresponds to many real-world applications like tagging of images or textual documents in social networks.
background_label: If one-vs-all methods usually reach top performance in this context, these approaches suffer from a high inference complexity, linear w.r.t the number of categories.
background_label: Different models based on the notion of binary codes have been proposed to overcome this limitation, achieving in a sublinear inference complexity.
background_label: But they a priori need to decide which binary code to associate to which category before learning using more or less complex heuristics.
method_label: We propose a new end-to-end model which aims at simultaneously learning to associate binary codes with categories, but also learning to map inputs to binary codes.
method_label: This approach called Deep Stochastic Neural Codes (DSNC) keeps the sublinear inference complexity but do not need any a priori tuning.
result_label: Experimental results on different datasets show the effectiveness of the approach w.r.t baseline methods.

===================================
paper_id: 57375742; YEAR: 2019
adju relevance: Related (+1)
difference: 1; annotator4: 0; annotator3: 1
sources: title_tfidf - specter
TITLE: Multi-class Classification without Multi-class Labels
ABSTRACT: objective_label: This work presents a new strategy for multi-class classification that requires no class-specific labels, but instead leverages pairwise similarity between examples, which is a weaker form of annotation.
method_label: The proposed method, meta classification learning, optimizes a binary classifier for pairwise similarity prediction and through this process learns a multi-class classifier as a submodule.
method_label: We formulate this approach, present a probabilistic graphical model for it, and derive a surprisingly simple loss function that can be used to learn neural network-based models.
method_label: We then demonstrate that this same framework generalizes to the supervised, unsupervised cross-task, and semi-supervised settings.
result_label: Our method is evaluated against state of the art in all three learning paradigms and shows a superior or comparable accuracy, providing evidence that learning multi-class classification without multi-class labels is a viable learning option.

===================================
paper_id: 12510650; YEAR: 2016
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: title_tfidfcbow200 - title_cbow200 - title_tfidf
TITLE: A Novel Progressive Learning Technique for Multi-class Classification
ABSTRACT: background_label: In this paper, a progressive learning technique for multi-class classification is proposed.
background_label: This newly developed learning technique is independent of the number of class constraints and it can learn new classes while still retaining the knowledge of previous classes.
method_label: Whenever a new class (non-native to the knowledge learnt thus far) is encountered, the neural network structure gets remodeled automatically by facilitating new neurons and interconnections, and the parameters are calculated in such a way that it retains the knowledge learnt thus far.
method_label: This technique is suitable for real-world applications where the number of classes is often unknown and online learning from real-time data is required.
method_label: The consistency and the complexity of the progressive learning technique are analyzed.
method_label: Several standard datasets are used to evaluate the performance of the developed technique.
result_label: A comparative study shows that the developed technique is superior.

===================================
paper_id: 6475282; YEAR: 2016
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: title_tfidfcbow200
TITLE: Distributed Optimization of Multi-Class SVMs
ABSTRACT: background_label: Training of one-vs.-rest SVMs can be parallelized over the number of classes in a straight forward way.
background_label: Given enough computational resources, one-vs.-rest SVMs can thus be trained on data involving a large number of classes.
background_label: The same cannot be stated, however, for the so-called all-in-one SVMs, which require solving a quadratic program of size quadratically in the number of classes.
other_label: We develop distributed algorithms for two all-in-one SVM formulations (Lee et al.
method_label: and Weston and Watkins) that parallelize the computation evenly over the number of classes.
method_label: This allows us to compare these models to one-vs.-rest SVMs on unprecedented scale.
result_label: The results indicate superior accuracy on text classification data.

===================================
paper_id: 8441645; YEAR: 2017
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: title_tfidfcbow200 - title_tfidf
TITLE: Multi-Class Support Vector Machine via Maximizing Multi-Class Margins
ABSTRACT: background_label: AbstractSupport Vector Machine (SVM) is originally proposed as a binary classification model with achieving great success in many applications.
background_label: In reality, it is more often to solve a problem which has more than two classes.
background_label: So, it is natural to extend SVM to a multi-class classifier.
background_label: There have been many works proposed to construct a multi-class classifier based on binary SVM, such as one versus rest strategy (OvsR), one versus one strategy (OvsO) and Weston's multi-class SVM.
method_label: The first two split the multi-class problem to multiple binary classification subproblems, and we need to train multiple binary classifiers.
method_label: Weston's multi-class SVM is formed by ensuring risk constraints and imposing a specific regularization, like Frobenius norm.
method_label: It is not derived by maximizing the margin between hyperplane and training data which is the motivation in SVM.
method_label: In this paper, we propose a multiclass SVM model from the perspective of maximizing margin between training points and hyperplane, and analyze the relation between our model and other related methods.
result_label: In the experiment, it shows that our model can get better or compared results when comparing with other related methods.

===================================
paper_id: 11917361; YEAR: 2006
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: title_tfidf - title_cbow200 - title_tfidfcbow200 - specter
TITLE: Implementing Multi-class Classifiers by One-class Classification Methods
ABSTRACT: objective_label: In this paper we address the problem of how to implement a multi-class classifier by an ensemble of one-class classifiers.
method_label: One-class classifiers are first trained for each class and then a decision function is formulated based on minimum distance rules.
method_label: Two kinds of one-class classifiers are explored: the support vector domain description and a kernel principle component analysis based method.
method_label: Both of the two methods can work in the feature space and deal with nonlinear classification problems.
result_label: Experiments on some benchmark datasets show that the proposed methods with carefully tuned parameters have comparable generalization ability with support vector machines while having some other advantages.

===================================
paper_id: 15526621; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 1; annotator4: 0; annotator3: 1
sources: title_cbow200 - title_tfidfcbow200 - specter
TITLE: A Generalized Mixture Framework for Multi-label Classification.
ABSTRACT: background_label: We develop a novel probabilistic ensemble framework for multi-label classification that is based on the mixtures-of-experts architecture.
method_label: In this framework, we combine multi-label classification models in the classifier chains family that decompose the class posterior distribution P(Y1, …, Yd |X) using a product of posterior distributions over components of the output space.
method_label: Our approach captures different input-output and output-output relations that tend to change across data.
method_label: As a result, we can recover a rich set of dependency relations among inputs and outputs that a single multi-label classification model cannot capture due to its modeling simplifications.
method_label: We develop and present algorithms for learning the mixtures-of-experts models from data and for performing multi-label predictions on unseen data instances.
result_label: Experiments on multiple benchmark datasets demonstrate that our approach achieves highly competitive results and outperforms the existing state-of-the-art multi-label classification methods.

===================================
paper_id: 62500203; YEAR: 2009
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf - title_cbow200 - title_tfidfcbow200
TITLE: Multi-class active learning for image classification
ABSTRACT: background_label: One of the principal bottlenecks in applying learning techniques to classification problems is the large amount of labeled training data required.
background_label: Especially for images and video, providing training data is very expensive in terms of human time and effort.
objective_label: In this paper we propose an active learning approach to tackle the problem.
method_label: Instead of passively accepting random training examples, the active learning algorithm iteratively selects unlabeled examples for the user to label, so that human effort is focused on labeling the most “useful” examples.
method_label: Our method relies on the idea of uncertainty sampling, in which the algorithm selects unlabeled examples that it finds hardest to classify.
method_label: Specifically, we propose an uncertainty measure that generalizes margin-based uncertainty to the multi-class case and is easy to compute, so that active learning can handle a large number of classes and large data sizes efficiently.
result_label: We demonstrate results for letter and digit recognition on datasets from the UCI repository, object recognition results on the Caltech-101 dataset, and scene categorization results on a dataset of 13 natural scene categories.
result_label: The proposed method gives large reductions in the number of training examples required over random selection to achieve similar classification accuracy, with little computational overhead.

===================================
paper_id: 4189528; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf - title_cbow200 - title_tfidfcbow200
TITLE: Multi-Stream Multi-Class Fusion of Deep Networks for Video Classification
ABSTRACT: objective_label: This paper studies deep network architectures to address the problem of video classification.
objective_label: A multi-stream framework is proposed to fully utilize the rich multimodal information in videos.
method_label: Specifically, we first train three Convolutional Neural Networks to model spatial, short-term motion and audio clues respectively.
method_label: Long Short Term Memory networks are then adopted to explore long-term temporal dynamics.
method_label: With the outputs of the individual streams on multiple classes, we propose to mine class relationships hidden in the data from the trained models.
method_label: The automatically discovered relationships are then leveraged in the multi-stream multi-class fusion process as a prior, indicating which and how much information is needed from the remaining classes, to adaptively determine the optimal fusion weights for generating the final scores of each class.
method_label: Our contributions are two-fold.
method_label: First, the multi-stream framework is able to exploit multimodal features that are more comprehensive than those previously attempted.
method_label: Second, our proposed fusion method not only learns the best weights of the multiple network streams for each class, but also takes class relationship into account, which is known as a helpful clue in multi-class visual classification tasks.
result_label: Our framework produces significantly better results than the state of the arts on two popular benchmarks, 92.2% on UCF-101 (without using audio) and 84.9% on Columbia Consumer Videos.

===================================
paper_id: 2250085; YEAR: 2004
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter - title_cbow200 - title_tfidfcbow200 - title_tfidf
TITLE: A consistency-based model selection for one-class classification
ABSTRACT: background_label: Model selection in unsupervised learning is a hard problem.
objective_label: In this paper, a simple selection criterion for hyper-parameters in one-class classifiers (OCCs) is proposed.
method_label: It makes use of the particular structure of the one-class problem.
method_label: The mean idea is that the complexity of the classifier is increased until the classifier becomes inconsistent on the target class.
method_label: This defines the most complex classifier, which can still reliably be trained on the data.
result_label: Experiments indicated the usefulness of the approach.

===================================
paper_id: 10997380; YEAR: 2006
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter
TITLE: A New Data Selection Approach for Semi-Supervised Acoustic Modeling
ABSTRACT: background_label: Current approaches to semi-supervised incremental learning prefer to select unlabeled examples predicted with high confidence for model re-training.
background_label: However, this strategy can degrade the classification performance rather than improve it.
method_label: We present an analysis for the reasons of this phenomenon, showing that only relying on high confidence for data selection can lead to an erroneous estimate to the true distribution when the confidence annotator is highly correlated with the classifier in the information they use.
method_label: We propose a new data selection approach to address this problem and apply it to a variety of applications, including machine learning and speech recognition.
result_label: Encouraging improvements in recognition accuracy are observed in our experiments

===================================
paper_id: 118156376; YEAR: 1989
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: Partially Identified Econometric Models
ABSTRACT: background_label: This paper studies a class of models where full identification is not necessarily assumed.
background_label: We term such models partially identified.
background_label: It is argued that partially identified systems are of practical importance since empirical investigators frequently proceed under conditions that are best described as apparent identification.
objective_label: One objective of the paper is to explore the properties of conventional statistical procedures in the context of identification failure.
method_label: Our analysis concentrates on two major types of partially identified model: the classic simultaneous equations model under rank condition failures; and time series spurious regressions.
method_label: Both types serve to illustrate the extensions that are needed to conventional asymptotic theory if the theory is to accommodate partially identified systems.
method_label: In many of the cases studied, the limit distributions fall within the class of compound normal distributions.
method_label: They are simply represented as covariance matrix or scalar mixtures of normals.
result_label: This includes time series spurious regressions, where representations in terms of functionals of vector Brownian motion are more conventional in recent research following earlier work by the author.

===================================
paper_id: 19155038; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf - title_cbow200 - title_tfidfcbow200
TITLE: Sequential Multi-Class Labeling in Crowdsourcing
ABSTRACT: background_label: We consider a crowdsourcing platform where workers' responses to questions posed by a crowdsourcer are used to determine the hidden state of a multi-class labeling problem.
objective_label: As workers may be unreliable, we propose to perform sequential questioning in which the questions posed to the workers are designed based on previous questions and answers.
method_label: We propose a Partially-Observable Markov Decision Process (POMDP) framework to determine the best questioning strategy, subject to the crowdsourcer's budget constraint.
method_label: As this POMDP formulation is in general intractable, we develop a suboptimal approach based on a $q$-ary Ulam-R\'enyi game.
method_label: We also propose a sampling heuristic, which can be used in tandem with standard POMDP solvers, using our Ulam-R\'enyi strategy.
result_label: We demonstrate through simulations that our approaches outperform a non-sequential strategy based on error correction coding and which does not utilize workers' previous responses.

===================================
paper_id: 43792; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter
TITLE: Training for Fast Sequential Prediction Using Dynamic Feature Selection
ABSTRACT: background_label: We present paired learning and inference algorithms for significantly reducing computation and increasing speed of the vector dot products in the classifiers that are at the heart of many NLP components.
method_label: This is accomplished by partitioning the features into a sequence of templates which are ordered such that high confidence can often be reached using only a small fraction of all features.
method_label: Parameter estimation is arranged to maximize accuracy and early confidence in this sequence.
result_label: We present experiments in left-to-right part-of-speech tagging on WSJ, demonstrating that we can preserve accuracy above 97% with over a five-fold reduction in run-time.

===================================
paper_id: 14919201; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 1; annotator4: 0; annotator3: 1
sources: title_tfidfcbow200 - title_cbow200
TITLE: Single-class SVM for dynamic scene modeling
ABSTRACT: background_label: Scene modeling is the starting point and thus the most crucial stage for many vision-based systems involving tracking or recognition.
background_label: Most of the existing approaches attempt at solving this problem by making some simplifying assumptions such as that of a stationary background.
background_label: However, this might not always be the case, as swaying trees or ripples in the water often violate these assumptions.
method_label: In this paper, we present a novel method for modeling background of a dynamic scene, i.e., scenes that contain “non-stationary” background motions, such as periodic motions (e.g., pendulums or escalators) or dynamic textures (e.g., water fountain in the background, swaying trees, or water ripples, etc.).
method_label: The paper proposes single-class support vector machine (SVM), and we show why it is preferable to other scene modeling techniques currently in use for this particular problem.
method_label: Using a rectangular region around a pixel, spatial and appearance-based features are extracted from limited amount of training data, used for learning the SVMs.
method_label: These features are unique, easy to compute and immune to rotation, and changes in scale and illumination.
result_label: We experiment on a diverse set of dynamic scenes and present both qualitative and quantitative results, indicating the practicality and the effectiveness of the proposed method.

===================================
paper_id: 14872665; YEAR: 2008
adju relevance: Irrelevant (0)
difference: 1; annotator4: 0; annotator3: 1
sources: title_cbow200 - title_tfidfcbow200 - specter - abs_tfidf - title_tfidf
TITLE: A Generative Probabilistic Model for Multi-label Classification
ABSTRACT: background_label: Traditional discriminative classification method makes little attempt to reveal the probabilistic structure and the correlation within both input and output spaces.
background_label: In the scenario of multi-label classification, most of the classifiers simply assume the predefined classes are independently distributed, which would definitely hinder the classification performance when there are intrinsic correlations between the classes.
objective_label: In this article, we propose a generative probabilistic model, the Correlated Labeling Model (CoL Model), to formulate the correlation between different classes.
method_label: The CoL model is presented to capture the correlation between classes and the underlying structures via the latent random variables in a supervised manner.
method_label: We develop a variational procedure to approximate the posterior distribution and employ the EM algorithm for the empirical Bayes parameter estimation.
result_label: In our evaluations, the proposed model achieved promising results on various data sets.

===================================
paper_id: 15368138; YEAR: 2000
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited - specter
TITLE: Handwritten digit recognition with a novel vision model that extracts linearly separable features
ABSTRACT: background_label: We use well-established results in biological vision to construct a novel vision model for handwritten digit recognition.
method_label: We show empirically that the features extracted by our model are linearly separable over a large training set (MNIST).
result_label: Using only a linear classifier on these features, our model is relatively simple yet outperforms other models on the same data set.

===================================
paper_id: 53051208; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Closing Brackets with Recurrent Neural Networks
ABSTRACT: background_label: AbstractMany natural and formal languages contain words or symbols that require a matching counterpart for making an expression wellformed.
background_label: The combination of opening and closing brackets is a typical example of such a construction.
background_label: Due to their commonness, the ability to follow such rules is important for language modeling.
background_label: Currently, recurrent neural networks (RNNs) are extensively used for this task.
method_label: We investigate whether they are capable of learning the rules of opening and closing brackets by applying them to synthetic Dyck languages that consist of different types of brackets.
result_label: We provide an analysis of the statistical properties of these languages as a baseline and show strengths and limits of Elman-RNNs, GRUs and LSTMs in experiments on random samples of these languages.
result_label: In terms of perplexity and prediction accuracy, the RNNs get close to the theoretical baseline in most cases.

===================================
paper_id: 23387485; YEAR: 2008
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200
TITLE: Bayesian regularization of neural networks.
ABSTRACT: background_label: Bayesian regularized artificial neural networks (BRANNs) are more robust than standard back-propagation nets and can reduce or eliminate the need for lengthy cross-validation.
background_label: Bayesian regularization is a mathematical process that converts a nonlinear regression into a "well-posed" statistical problem in the manner of a ridge regression.
background_label: The advantage of BRANNs is that the models are robust and the validation process, which scales as O(N2) in normal regression methods, such as back propagation, is unnecessary.
method_label: These networks provide solutions to a number of problems that arise in QSAR modeling, such as choice of model, robustness of model, choice of validation set, size of validation effort, and optimization of network architecture.
result_label: They are difficult to overtrain, since evidence procedures provide an objective Bayesian criterion for stopping training.
background_label: They are also difficult to overfit, because the BRANN calculates and trains on a number of effective network parameters or weights, effectively turning off those that are not relevant.
background_label: This effective number is usually considerably smaller than the number of weights in a standard fully connected back-propagation neural net.
method_label: Automatic relevance determination (ARD) of the input variables can be used with BRANNs, and this allows the network to "estimate" the importance of each input.
method_label: The ARD method ensures that irrelevant or highly correlated indices used in the modeling are neglected as well as showing which are the most important variables for modeling the activity data.
method_label: This chapter outlines the equations that define the BRANN method plus a flowchart for producing a BRANN-QSAR model.
result_label: Some results of the use of BRANNs on a number of data sets are illustrated and compared with other linear and nonlinear models.

===================================
paper_id: 9874135; YEAR: 2009
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200 - title_tfidf
TITLE: Efficient multi-label ranking for multi-class learning: Application to object recognition
ABSTRACT: background_label: Multi-label learning is useful in visual object recognition when several objects are present in an image.
background_label: Conventional approaches implement multi-label learning as a set of binary classification problems, but they suffer from imbalanced data distributions when the number of classes is large.
objective_label: In this paper, we address multi-label learning with many classes via a ranking approach, termed multi-label ranking.
method_label: Given a test image, the proposed scheme aims to order all the object classes such that the relevant classes are ranked higher than the irrelevant ones.
method_label: We present an efficient algorithm for multi-label ranking based on the idea of block coordinate descent.
method_label: The proposed algorithm is applied to visual object recognition.
result_label: Empirical results on the PASCAL VOC 2006 and 2007 data sets show promising results in comparison to the state-of-the-art algorithms for multi-label learning.

===================================
paper_id: 23824762; YEAR: 2008
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: MMG: a probabilistic tool to identify submodules of metabolic pathways.
ABSTRACT: background_label: MOTIVATION A fundamental task in systems biology is the identification of groups of genes that are involved in the cellular response to particular signals.
background_label: At its simplest level, this often reduces to identifying biological quantities (mRNA abundance, enzyme concentrations, etc.)
background_label: which are differentially expressed in two different conditions.
background_label: Popular approaches involve using t-test statistics, based on modelling the data as arising from a mixture distribution.
background_label: A common assumption of these approaches is that the data are independent and identically distributed; however, biological quantities are usually related through a complex (weighted) network of interactions, and often the more pertinent question is which subnetworks are differentially expressed, rather than which genes.
result_label: Furthermore, in many interesting cases (such as high-throughput proteomics and metabolomics), only very partial observations are available, resulting in the need for efficient imputation techniques.
background_label: RESULTS We introduce Mixture Model on Graphs (MMG), a novel probabilistic model to identify differentially expressed submodules of biological networks and pathways.
method_label: The method can easily incorporate information about weights in the network, is robust against missing data and can be easily generalized to directed networks.
method_label: We propose an efficient sampling strategy to infer posterior probabilities of differential expression, as well as posterior probabilities over the model parameters.
method_label: We assess our method on artificial data demonstrating significant improvements over standard mixture model clustering.
result_label: Analysis of our model results on quantitative high-throughput proteomic data leads to the identification of biologically significant subnetworks, as well as the prediction of the expression level of a number of enzymes, some of which are then verified experimentally.
other_label: AVAILABILITY MATLAB code is available from http://www.dcs.shef.ac.uk/~guido/software.html

===================================
paper_id: 3503170; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200 - abs_tfidf
TITLE: A Reevaluation and Benchmark of Hidden Markov Models
ABSTRACT: background_label: Hidden Markov models are frequently used in handwriting-recognition applications.
background_label: While a large number of methodological variants have been developed to accommodate different use cases, the core concepts have not been changed much.
objective_label: In this paper, we develop a number of datasets to benchmark our own implementation as well as various other tool kits.
method_label: We introduce a gradual scale of difficulty that allows comparison of datasets in terms of separability of classes.
method_label: Two experiments are performed to review the basic HMM functions, especially aimed at evaluating the role of the transition probability matrix.
method_label: We found that the transition matrix may be far less important than the observation probabilities.
method_label: Furthermore, the traditional training methods are not always able to find the proper (true) topology of the transition matrix.
result_label: These findings support the view that the quality of the features may require more attention than the aspect of temporal modelling addressed by HMMs.

===================================
paper_id: 17567112; YEAR: 1992
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited - specter
TITLE: A method for disambiguating word senses in a large corpus
ABSTRACT: background_label: Word sense disambiguation has been recognized as a major problem in natural language processing research for over forty years.
background_label: Both quantitive and qualitative methods have been tried, but much of this work has been stymied by difficulties in acquiring appropriate lexical resources.
objective_label: The availability of this testing and training material has enabled us to develop quantitative disambiguation methods that achieve 92% accuracy in discriminating between two very distinct senses of a noun.
method_label: In the training phase, we collect a number of instances of each sense of the polysemous noun.
method_label: Then in the testing phase, we are given a new instance of the noun, and are asked to assign the instance to one of the senses.
method_label: We attempt to answer this question by comparing the context of the unknown instance with contexts of known instances using a Bayesian argument that has been applied successfully in related tasks such as author identification and information retrieval.
method_label: The proposed method is probably most appropriate for those aspects of sense disambiguation that are closest to the information retrieval task.
method_label: In particular, the proposed method was designed to disambiguate senses that are usually associated with different topics.

===================================
paper_id: 7408595; YEAR: 2000
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: Decontamination of Training Samples for Supervised Pattern Recognition Methods
ABSTRACT: background_label: The present work discusses what have been called 'imperfectly supervised situations': pattern recognition applications where the assumption of label correctness does not hold for all the elements of the training sample.
objective_label: A methodology for contending with these practical situations and to avoid their negative impact on the performance of supervised methods is presented.
method_label: This methodology can be regarded as a cleaning process removing some suspicious instances of the training sample or correcting the class labels of some others while retaining them.
method_label: It has been conceived for doing classification with the Nearest Neighbor rule, a supervised nonparametric classifier that combines conceptual simplicity and an asymptotic error rate bounded in terms of the optimal Bayes error.
method_label: However, initial experiments concerning the learning phase of a Multilayer Perceptron (not reported in the present work) seem to indicate a broader applicability.
result_label: Results with both simulated and real data sets are presented to support the methodology and to clarify the ideas behind it.
result_label: Related works are briefly reviewed and some issues deserving further research are also exposed.

===================================
paper_id: 120221455; YEAR: 1985
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200
TITLE: A New Class of Market Share Models
ABSTRACT: background_label: Applications of market share models which implicitly rely on Luce's choice axiom have been widely criticized because they cannot account for the effects of differential product substitutability and product dominance.
background_label: Three types of choice models---Tversky's Elimination-By-Aspects model, Tree Models, and Generalized PROBIT---have been offered as solutions to the problems identified with the Luce model, but they each suffer from limitations which have prevented their widespread application in marketing contexts.
background_label: Tversky's elegant EBA model has not been widely used because it requires a large number of parameters and no special-purpose parameter estimation software has yet emerged.
method_label: Tree models have been offered as more parsimonious special cases of EBA, but they are more restrictive in that they presume: 1 that products, and the process of choosing from among them, can be characterized in terms of hierarchical, attribute-based trees; and 2 that the aspects governing choice are well-known.
result_label: Generalized PROBIT can paramorphically handle the problems with the Luce model, but parameter estimation software has proved problematic because it cannot guarantee a globally optimum solution.
background_label: This paper proposes a new class of market share models.
background_label: Rather than model the choice process explicitly, the new models simply scale the effects competing products have on each other's market share.
method_label: These competitive effects are scaled in the context of a class of market-share models which: 1 do not assume a tree-like structure for the competing products; 2 do not presume any a priori knowledge about the attributes governing choice; 3 are characterized in terms of parameters that can be estimated using ordinary least-squares; and 4 provide clear managerial insight into the sources of competition.
method_label: The paper begins with a brief review of previous work.
method_label: Following the review, the paper offers a theorem and proof which guarantees the existence of the new class of models.
result_label: The empirical validity and strategic utility of the models are demonstrated using two separate sets of data.

===================================
paper_id: 153690277; YEAR: 1980
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200
TITLE: MACROECONOMICS AND REALITY
ABSTRACT: background_label: Existing strategies for econometric analysis related to macroeconomics are subject to a number of serious objections, some recently formulated, some old.
background_label: These objections are summarized in this paper, and it is argued that taken together they make it unlikely that macroeconomic models are in fact over identified, as the existing statistical theory usually assumes.
background_label: The implications of this conclusion are explored, and an example of econometric work in a non-standard style, taking account of the objections to the standard style, is presented.
background_label: THE STUDY OF THE BUSINESS cycle, fluctuations in aggregate measures of economic activity and prices over periods from one to ten years or so, constitutes or motivates a large part of what we call macroeconomics.
background_label: Most economists would agree that there are many macroeconomic variables whose cyclical fluctuations are of interest, and would agree further that fluctuations in these series are interrelated.
background_label: It would seem to follow almost tautologically that statistical models involving large numbers of macroeconomic variables ought to be the arena within which macroeconomic theories confront reality and thereby each other.
background_label: Instead, though large-scale statistical macroeconomic models exist and are by some criteria successful, a deep vein of skepticism about the value of these models runs through that part of the economics profession not actively engaged in constructing or using them.
result_label: It is still rare for empirical research in macroeconomics to be planned and executed within the framework of one of the large models.
background_label: In this lecture I intend to discuss some aspects of this situation, attempting both to offer some explanations and to suggest some means for improvement.
background_label: I will argue that the style in which their builders construct claims for a connection between these models and reality-the style in which "identification" is achieved for these models-is inappropriate, to the point at which claims for identification in these models cannot be taken seriously.
background_label: This is a venerable assertion; and there are some good old reasons for believing it;2 but there are also some reasons which have been more recently put forth.
objective_label: After developing the conclusion that the identification claimed for existing large-scale models is incredible, I will discuss what ought to be done in consequence.
method_label: The line of argument is: large-scale models do perform useful forecasting and policy-analysis functions despite their incredible identification; the restrictions imposed in the usual style of identification are neither essential to constructing a model which can perform these functions nor innocuous; an alternative style of identification is available and practical.
result_label: Finally we will look at some empirical work based on an alternative style of macroeconometrics.
other_label: A six-variable dynamic system is estimated without using 1 Research for this paper was supported by NSF Grant Soc-76-02482.
method_label: Lars Hansen executed the computations.
result_label: The paper has benefited from comments by many people, especially Thomas J. Sargent

===================================
paper_id: 28482216; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf
TITLE: A Generative Model For Zero Shot Learning Using Conditional Variational Autoencoders
ABSTRACT: background_label: Zero shot learning in Image Classification refers to the setting where images from some novel classes are absent in the training data but other information such as natural language descriptions or attribute vectors of the classes are available.
background_label: This setting is important in the real world since one may not be able to obtain images of all the possible classes at training.
method_label: While previous approaches have tried to model the relationship between the class attribute space and the image space via some kind of a transfer function in order to model the image space correspondingly to an unseen class, we take a different approach and try to generate the samples from the given attributes, using a conditional variational autoencoder, and use the generated samples for classification of the unseen classes.
result_label: By extensive testing on four benchmark datasets, we show that our model outperforms the state of the art, particularly in the more realistic generalized setting, where the training classes can also appear at the test time along with the novel classes.

===================================
paper_id: 17468577; YEAR: 2009
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter
TITLE: Semisupervised Multitask Learning
ABSTRACT: background_label: Context plays an important role when performing classification, and in this paper we examine context from two perspectives.
background_label: First, the classification of items within a single task is placed within the context of distinct concurrent or previous classification tasks (multiple distinct data collections).
method_label: This is referred to as multi-task learning (MTL), and is implemented here in a statistical manner, using a simplified form of the Dirichlet process.
method_label: In addition, when performing many classification tasks one has simultaneous access to all unlabeled data that must be classified, and therefore there is an opportunity to place the classification of any one feature vector within the context of all unlabeled feature vectors; this is referred to as semi-supervised learning.
method_label: In this paper we integrate MTL and semi-supervised learning into a single framework, thereby exploiting two forms of contextual information.
result_label: Example results are presented on a "toy" example, to demonstrate the concept, and the algorithm is also applied to three real data sets.

===================================
paper_id: 2894303; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200
TITLE: Modelling human preferences for ranking and collaborative filtering: a probabilistic ordered partition approach
ABSTRACT: background_label: Learning preference models from human generated data is an important task in modern information processing systems.
background_label: Its popular setting consists of simple input ratings, assigned with numerical values to indicate their relevancy with respect to a specific query.
background_label: Since ratings are often specified within a small range, several objects may have the same ratings, thus creating ties among objects for a given query.
background_label: Dealing with this phenomena presents a general problem of modelling preferences in the presence of ties and being query-specific.
method_label: To this end, we present in this paper a novel approach by constructing probabilistic models directly on the collection of objects exploiting the combinatorial structure induced by the ties among them.
method_label: The proposed probabilistic setting allows exploration of a super-exponential combinatorial state-space with unknown numbers of partitions and unknown order among them.
result_label: Learning and inference in such a large state-space are challenging, and yet we present in this paper efficient algorithms to perform these tasks.
background_label: Our approach exploits discrete choice theory, imposing generative process such that the finite set of objects is partitioned into subsets in a stagewise procedure, and thus reducing the state-space at each stage significantly.
method_label: Efficient Markov chain Monte Carlo algorithms are then presented for the proposed models.
method_label: We demonstrate that the model can potentially be trained in a large-scale setting of hundreds of thousands objects using an ordinary computer.
method_label: In fact, in some special cases with appropriate model specification, our models can be learned in linear time.
other_label: We evaluate the models on two application areas: (i) document ranking with the data from the Yahoo!
result_label: challenge and (ii) collaborative filtering with movie data.
result_label: We demonstrate that the models are competitive against state-of-the-arts.

===================================
paper_id: 14382246; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: On Loss Functions for Deep Neural Networks in Classification
ABSTRACT: background_label: Deep neural networks are currently among the most commonly used classifiers.
background_label: Despite easily achieving very good performance, one of the best selling points of these models is their modular design - one can conveniently adapt their architecture to specific needs, change connectivity patterns, attach specialised layers, experiment with a large amount of activation functions, normalisation schemes and many others.
background_label: While one can find impressively wide spread of various configurations of almost every aspect of the deep nets, one element is, in authors' opinion, underrepresented - while solving classification problems, vast majority of papers and applications simply use log loss.
objective_label: In this paper we try to investigate how particular choices of loss functions affect deep models and their learning dynamics, as well as resulting classifiers robustness to various effects.
method_label: We perform experiments on classical datasets, as well as provide some additional, theoretical insights into the problem.
result_label: In particular we show that L1 and L2 losses are, quite surprisingly, justified classification objectives for deep nets, by providing probabilistic interpretation in terms of expected misclassification.
result_label: We also introduce two losses which are not typically used as deep nets objectives and show that they are viable alternatives to the existing ones.

===================================
paper_id: 52986657; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 1; annotator4: 0; annotator3: 1
sources: abs_tfidf
TITLE: Incremental Few-Shot Learning with Attention Attractor Networks
ABSTRACT: background_label: Machine learning classifiers are often trained to recognize a set of pre-defined classes.
background_label: However, in many real applications, it is often desirable to have the flexibility of learning additional concepts, without re-training on the full training set.
method_label: This paper addresses this problem, incremental few-shot learning, where a regular classification network has already been trained to recognize a set of base classes; and several extra novel classes are being considered, each with only a few labeled examples.
method_label: After learning the novel classes, the model is then evaluated on the overall performance of both base and novel classes.
method_label: To this end, we propose a meta-learning model, the Attention Attractor Network, which regularizes the learning of novel classes.
method_label: In each episode, we train a set of new weights to recognize novel classes until they converge, and we show that the technique of recurrent back-propagation can back-propagate through the optimization process and facilitate the learning of the attractor network regularizer.
result_label: We demonstrate that the learned attractor network can recognize novel classes while remembering old classes without the need to review the original training set, outperforming baselines that do not rely on an iterative optimization process.

===================================
paper_id: 6859542; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200 - title_tfidf
TITLE: Multi-Class Multi-Scale Series Contextual Model for Image Segmentation
ABSTRACT: background_label: Contextual information has been widely used as a rich source of information to segment multiple objects in an image.
background_label: A contextual model uses the relationships between the objects in a scene to facilitate object detection and segmentation.
background_label: Using contextual information from different objects in an effective way for object segmentation, however, remains a difficult problem.
method_label: In this paper, we introduce a novel framework, called multiclass multiscale (MCMS) series contextual model, which uses contextual information from multiple objects and at different scales for learning discriminative models in a supervised setting.
method_label: The MCMS model incorporates cross-object and inter-object information into one probabilistic framework and thus is able to capture geometrical relationships and dependencies among multiple objects in addition to local information from each single object present in an image.
result_label: We demonstrate that our MCMS model improves object segmentation performance in electron microscopy images and provides a coherent segmentation of multiple objects.
result_label: Through speeding up the segmentation process, the proposed method will allow neurobiologists to move beyond individual specimens and analyze populations paving the way for understanding neurodegenerative diseases at the microscopic level.

===================================
paper_id: 10674718; YEAR: 2008
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf - title_cbow200 - title_tfidfcbow200
TITLE: Multi-Kernel Support Vector Clustering for Multi-Class Classification
ABSTRACT: background_label: Support vector clustering (SVC) has been successfully applied to solve multi-class classification problems.
background_label: However, it is usually hard to determine the hyper-parameters of RBF kernel functions.
background_label: A multiple kernel learning (MKL) algorithm is developed to solve this problem, by which the kernel matrix weights and Lagrange multipliers can be simultaneously obtained with semidefinite programming.
background_label: However, the amount of time and space required is very demanding.
method_label: We develop a two stage multiple kernel learning algorithm by incorporating sequential minimal optimization (SMO) with the gradient projection method.
result_label: Experimental results on data sets from UCI and Statlog show that the proposed approach outperforms single-kernel support vector clustering.

===================================
paper_id: 117710480; YEAR: 2009
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: Correlation models for paired comparison data
ABSTRACT: background_label: Binary paired comparison data are binary data that record which of two objects being compared is preferred.
background_label: Applications arise in many contexts including biology, acoustics, genetics, consumer behaviour and sports tournaments.
background_label: Most of traditional models, such as the Bradley-Terry model, are based on unrealistic independence assumptions.
background_label: However, in many instances it is sensible to believe that the results of two paired comparisons involving a common object will be correlated.
objective_label: This thesis focuses on the potential presence of dependence among paired comparison data.
method_label: Two novel models, either marginally or conditionally specified, are introduced in order to account for the dependence structure of the data.
method_label: Although the proposed models appear to be more realistic than usual independence models, ordinary likelihood inference is made difficult by the need to approximate high dimensional integrals.
method_label: Hence, in this thesis a composite likelihood inferential approach is proposed.
result_label: Simulation studies are performed in order to assess the behaviour of the maximum composite likelihood estimators for the parameters of the correlation models for paired comparison data.
background_label: The estimators, in case of single round robin tournaments, exhibit good properties.
background_label: Moreover, even if not all matches of a round robin tournament are played, the estimates are not much affected at least until fifty per cent of the competitions takes place.
method_label: The proposed methodology is illustrated by some applications to real data sets.
method_label: A first application regards sports data, specifically the results of the Italian A1 league which is at the top of the Italian volleyball league system.
objective_label: In this case the interest lies in determining whether the geographical origin of the teams or their physical features influence the strength of the teams themselves.
method_label: A further application is concerned with biological data.
method_label: Biologists are interested in investigating the role of colours in sexual signalling and in determining whether specific colours are associated with stronger animals.
method_label: For this purpose, they observed the results of contests between some lizards that belong to a particular species which displays three different patches of colours on their bodies.
result_label: Finally, the problem of the evaluation and ranking of scientific journals is considered with an application to the main statistical journals.

===================================
paper_id: 5170310; YEAR: 2005
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: A PAC-style model for learning from labeled and unlabeled data
ABSTRACT: background_label: There has been growing interest in practice in using unlabeled data together with labeled data in machine learning, and a number of different approaches have been developed.
background_label: However, the assumptions these methods are based on are often quite distinct and not captured by standard theoretical models.
method_label: In this paper we describe a PAC-style framework that can be used to model many of these assumptions, and analyze sample-complexity issues in this setting: that is, how much of each type of data one should expect to need in order to learn well, and what are the basic quantities that these numbers depend on.
method_label: Our model can be viewed as an extension of the standard PAC model, where in addition to a concept class C, one also proposes a type of compatibility that one believes the target concept should have with the underlying distribution.
method_label: In this view, unlabeled data can be helpful because it allows one to estimate compatibility over the space of hypotheses, and reduce the size of the search space to those that, according to one's assumptions, are a-priori reasonable with respect to the distribution.
method_label: We discuss a number of technical issues that arise in this context, and provide sample-complexity bounds both for uniform convergence and e-cover based algorithms.
result_label: We also consider algorithmic issues, and give an efficient algorithm for a special case of co-training.

===================================
paper_id: 6683105; YEAR: 2011
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf - specter
TITLE: Sequential Feature Selection for Classification
ABSTRACT: other_label: Abstract.
background_label: In most real-world information processing problems, data is not a free resource; its acquisition is rather time-consuming and/or expensive.
objective_label: We investigate how these two factors can be included in supervised classification tasks by deriving classification as a sequential decision process and making it accessible to Reinforcement Learning.
method_label: Our method performs a sequential feature selection that learns which features are most informative at each timestep, choosing the next feature depending on the already selected features and the internal belief of the classifier.
result_label: Experiments on a handwritten digits classification task show significant reduction in required data for correct classification, while a medical diabetes prediction task illustrates variable feature cost minimization as a further property of our algorithm.

===================================
paper_id: 1177419; YEAR: 1998
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited - specter
TITLE: Part of Speech Tagging Using a Network of Linear Separators
ABSTRACT: background_label: We present an architecture and an on-line learning algorithm and apply it to the problem of part-of-speech tagging.
background_label: The architecture presented, SNOW, is a network of linear separators in the feature space, utilizing the Winnow update algorithm.Multiplicative weight-update algorithms such as Winnow have been shown to have exceptionally good behavior when applied to very high dimensional problems, and especially when the target concepts depend on only a small subset of the features in the feature space.
method_label: In this paper we describe an architecture that utilizes this mistake-driven algorithm for multi-class prediction-selecting the part of speech of a word.
method_label: The experimental analysis presented here provides more evidence to that these algorithms are suitable for natural language problems.The algorithm used is an on-line algorithm: every example is used by the algorithm only once, and is then discarded.
result_label: This has significance in terms of efficiency, as well as quick adaptation to new contexts.We present an extensive experimental study of our algorithm under various conditions; in particular, it is shown that the algorithm performs comparably to the best known algorithms for POS.

===================================
paper_id: 14509422; YEAR: 2001
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited - specter
TITLE: The Use of Classifiers in Sequential Inference
ABSTRACT: background_label: We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints.
objective_label: In particular, we develop two general approaches for an important subproblem-identifying phrase structure.
method_label: The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state-observation dependencies.
method_label: The second is an extension of constraint satisfaction formalisms.
method_label: We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing.

===================================
paper_id: 12998593; YEAR: 2011
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200
TITLE: Inference of complex biological networks: distinguishability issues and optimization-based solutions
ABSTRACT: background_label: BACKGROUND The inference of biological networks from high-throughput data has received huge attention during the last decade and can be considered an important problem class in systems biology.
background_label: However, it has been recognized that reliable network inference remains an unsolved problem.
background_label: Most authors have identified lack of data and deficiencies in the inference algorithms as the main reasons for this situation.
method_label: RESULTS We claim that another major difficulty for solving these inference problems is the frequent lack of uniqueness of many of these networks, especially when prior assumptions have not been taken properly into account.
objective_label: Our contributions aid the distinguishability analysis of chemical reaction network (CRN) models with mass action dynamics.
method_label: The novel methods are based on linear programming (LP), therefore they allow the efficient analysis of CRNs containing several hundred complexes and reactions.
background_label: Using these new tools and also previously published ones to obtain the network structure of biological systems from the literature, we find that, often, a unique topology cannot be determined, even if the structure of the corresponding mathematical model is assumed to be known and all dynamical variables are measurable.
background_label: In other words, certain mechanisms may remain undetected (or they are falsely detected) while the inferred model is fully consistent with the measured data.
background_label: It is also shown that sparsity enforcing approaches for determining 'true' reaction structures are generally not enough without additional prior information.
background_label: CONCLUSIONS The inference of biological networks can be an extremely challenging problem even in the utopian case of perfect experimental information.
background_label: Unfortunately, the practical situation is often more complex than that, since the measurements are typically incomplete, noisy and sometimes dynamically not rich enough, introducing further obstacles to the structure/parameter estimation process.
result_label: In this paper, we show how the structural uniqueness and identifiability of the models can be guaranteed by carefully adding extra constraints, and that these important properties can be checked through appropriate computation methods.

===================================
paper_id: 173187918; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 1; annotator4: 0; annotator3: 1
sources: abs_tfidf
TITLE: Large Scale Incremental Learning
ABSTRACT: background_label: Modern machine learning suffers from catastrophic forgetting when learning new classes incrementally.
background_label: The performance dramatically degrades due to the missing data of old classes.
method_label: Incremental learning methods have been proposed to retain the knowledge acquired from the old classes, by using knowledge distilling and keeping a few exemplars from the old classes.
method_label: However, these methods struggle to scale up to a large number of classes.
method_label: We believe this is because of the combination of two factors: (a) the data imbalance between the old and new classes, and (b) the increasing number of visually similar classes.
method_label: Distinguishing between an increasing number of visually similar classes is particularly challenging, when the training data is unbalanced.
method_label: We propose a simple and effective method to address this data imbalance issue.
method_label: We found that the last fully connected layer has a strong bias towards the new classes, and this bias can be corrected by a linear model.
result_label: With two bias parameters, our method performs remarkably well on two large datasets: ImageNet (1000 classes) and MS-Celeb-1M (10000 classes), outperforming the state-of-the-art algorithms by 11.1% and 13.2% respectively.

===================================
paper_id: 18011791; YEAR: 1999
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Techniques for learning and tuning fuzzy rule-based systems for linguistic modeling and their application
ABSTRACT: background_label: Nowadays, Linguistic Modeling is considered to be one of the most important areas of application for Fuzzy Logic.
background_label: Linguistic Mamdani-type Fuzzy Rule-Based Systems (FRBSs), the ones used to perform this task, provide a human-readable description of the model in the form of linguistic rules, which is a desirable characteristic in many problems.
method_label: In this Chapter we are going to accomplish a short revision of the FRBSs where we shall see the different types that currently exist, along with their structures and characteristics, centering our attention on linguistic Mamdani-type FRBS.
method_label: The performance of a linguistic FRBS depends on its Rule Base and the membership functions associated to the fuzzy partitions.
method_label: Due to the complexity in the design of these components, a large quantity of automatic techniques has been proposed to put it into effect.
method_label: Thereafter, we are going to review several learning (when it sets the Rule Base and sometimes the Data Base as well) and tuning (when it only sets the Data Base) methods.
method_label: These methods are inspired in the three most well known approaches: ad hoc data covering, neural networks, and genetic algorithms.
method_label: We shall introduce a brief description of these techniques and their synergy with FRBSs.
method_label: The accuracy of the reviewed methods will be compared when solving two real-world applications.
result_label: Some interesting conclusions will be obtained about the behavior of the methods, approaches, and techniques.

===================================
paper_id: 1785; YEAR: 1998
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited - specter
TITLE: Learning to Resolve Natural Language Ambiguities: A Unified Approach
ABSTRACT: background_label: We analyze a few of the commonly used statistics based and machine learning algorithms for natural language disambiguation tasks and observe that they can be re-cast as learning linear separators in the feature space.
background_label: Each of the methods makes a priori assumptions, which it employs, given the data, when searching for its hypothesis.
method_label: Nevertheless, as we show, it searches a space that is as rich as the space of all linear separators.
method_label: We use this to build an argument for a data driven approach which merely searches for a good linear separator in the feature space, without further assumptions on the domain or a specific problem.
method_label: We present such an approach - a sparse network of linear separators, utilizing the Winnow learning algorithm - and show how to use it in a variety of ambiguity resolution problems.
method_label: The learning approach presented is attribute-efficient and, therefore, appropriate for domains having very large number of attributes.
method_label: In particular, we present an extensive experimental comparison of our approach with other methods on several well studied lexical disambiguation tasks such as context-sensitive spelling correction, prepositional phrase attachment and part of speech tagging.
result_label: In all cases we show that our approach either outperforms other methods tried for these tasks or performs comparably to the best.

===================================
paper_id: 1742928; YEAR: 1997
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited - specter
TITLE: Resolving PP attachment Ambiguities with Memory-Based Learning
ABSTRACT: background_label: AbstractIn this paper we describe the application of Memory-Based Learning to the problem of Prepositional Phrase attachment disambiguation.
method_label: We compare Memory-Based Learning, which stores examples in memory and generalizes by using intelligent similarity metrics, with a number of recently proposed statistical methods that are well suited to large numbers of features.
result_label: We evaluate our methods on a common benchmark dataset and show that our method compares favorably to previous methods, and is well-suited to incorporating various unconventional representations of word patterns such as value difference metrics and Lexical Space.

===================================
paper_id: 60446966; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf
TITLE: Active Learning Via Sequential Design and Uncertainty Sampling
ABSTRACT: background_label: Classification is an important task in many fields including biomedical research and machine learning.
background_label: Traditionally, a classification rule is constructed based a bunch of labeled data.
background_label: Recently, due to technological innovation and automatic data collection schemes, we easily encounter with data sets containing large amounts of unlabeled samples.
background_label: Because to label each of them is usually costly and inefficient, how to utilize these unlabeled data in a classifier construction process becomes an important problem.
method_label: In machine learning literature, active learning or semi-supervised learning are popular concepts discussed under this situation, where classification algorithms recruit new unlabeled subjects sequentially based on the information learned from previous stages of its learning process, and these new subjects are then labeled and included as new training samples.
method_label: From a statistical aspect, these methods can be recognized as a hybrid of the sequential design and stochastic approximation procedure.
method_label: In this paper, we study sequential learning procedures for building efficient and effective classifiers, where only the selected subjects are labeled and included in its learning stage.
method_label: The proposed algorithm combines the ideas of Bayesian sequential optimal design and uncertainty sampling.
method_label: Computational issues of the algorithm are discussed.
result_label: Numerical results using both synthesized data and real examples are reported.

===================================
paper_id: 59578351; YEAR: 2012
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: Phraseology in the language, in the dictionary, and in the computer
ABSTRACT: background_label: Two main families of phrasemes (= non-free phrases) are distinguish ed: lexical phrasemes and semantic-lexical phrasemes; the phrasemes of the first family are constrained only in their form (their meaning being free), those of the second family are constrained both in their meaning and in their form.
method_label: Two basic concepts are introduced: compositionality of complex linguistic signs and the pivot of a meaning.
method_label: Three major classes of phrasemes are presented: non- compositional idioms and compositional collocations and cliches .
method_label: A new type of general dictionary is proposed, and the lexicographic presentation of the three classes of phrasemes is illustrated.
result_label: To show how the proposed approach to phraseology can be used in Automatic Language Processing, three fully-fledged examples are examined in detail.

===================================
paper_id: 17814659; YEAR: 2005
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf - title_cbow200 - title_tfidfcbow200
TITLE: Multi-Class and Single-Class Classification Approaches to Vehicle Model Recognition from Images
ABSTRACT: background_label: This paper investigates the use of machine learning classification techniques applied to the task of recognising the make and model of vehicles.
background_label: Although a number of vehicle classification systems already exist, most of them seek only to distinguish between vehicle categories, e.g.
background_label: identifying whether a vehicle is a bus, truck or car.
method_label: The system presented here demonstrates that a set of features extracted from the frontal view of a vehicle may be used to determine the vehicle type (make and model) with high accuracy.
result_label: The performance of some standard multi-class classification algorithms is compared for this problem.
result_label: A one-class k-Nearest Neighbour classification algorithm is also implemented and tested.

===================================
paper_id: 2370; YEAR: 2005
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: ATNoSFERES revisited
ABSTRACT: background_label: ATNoSFERES is a Pittsburgh style Learning Classifier System (LCS) in which the rules are represented as edges of an Augmented Transition Network.
background_label: Genotypes are strings of tokens of a stack-based language, whose execution builds the labeled graph.
background_label: The original ATNoSFERES, using a bitstring to represent the language tokens, has been favorably compared in previous work to several Michigan style LCSs architectures in the context of Non Markov problems.
method_label: Several modifications of ATNoSFERES are proposed here: the most important one conceptually being a representational change: each token is now represented by an integer, hence the genotype is a string of integers; several other modifications of the underlying grammar language are also proposed.
result_label: The resulting ATNoSFERES-II is validated on several standard animat Non Markov problems, on which it outperforms all previously published results in the LCS literature.
result_label: The reasons for these improvement are carefully analyzed, and some assumptions are proposed on the underlying mechanisms in order to explain these good results.

===================================
paper_id: 14406743; YEAR: 2000
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter - abs_tfidfcbow200
TITLE: Learning from Imbalanced Data Sets: A Comparison of Various Strategies
ABSTRACT: background_label: Although the majority of concept-learning systems previously designed usually assume that their training sets are well-balanced, this assumption is not necessarily correct.
background_label: Indeed, there exists many domains for which one class is represented by a large number of examples while the other is represented by only a few.
objective_label: The purpose of this paper is 1) to demonstrate experimentally that, at least in the case of connectionist systems, class imbalances hinder the performance of standard classifiers and 2) to compare the performance of several approaches previously proposed to deal with the problem.

===================================
paper_id: 12897739; YEAR: 2009
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Training set expansion: an approach to improving the reconstruction of biological networks from limited and uneven reliable interactions
ABSTRACT: background_label: MOTIVATION An important problem in systems biology is reconstructing complete networks of interactions between biological objects by extrapolating from a few known interactions as examples.
background_label: While there are many computational techniques proposed for this network reconstruction task, their accuracy is consistently limited by the small number of high-confidence examples, and the uneven distribution of these examples across the potential interaction space, with some objects having many known interactions and others few.
objective_label: RESULTS To address this issue, we propose two computational methods based on the concept of training set expansion.
method_label: They work particularly effectively in conjunction with kernel approaches, which are a popular class of approaches for fusing together many disparate types of features.
method_label: Both our methods are based on semi-supervised learning and involve augmenting the limited number of gold-standard training instances with carefully chosen and highly confident auxiliary examples.
method_label: The first method, prediction propagation, propagates highly confident predictions of one local model to another as the auxiliary examples, thus learning from information-rich regions of the training network to help predict the information-poor regions.
method_label: The second method, kernel initialization, takes the most similar and most dissimilar objects of each object in a global kernel as the auxiliary examples.
result_label: Using several sets of experimentally verified protein-protein interactions from yeast, we show that training set expansion gives a measurable performance gain over a number of representative, state-of-the-art network reconstruction methods, and it can correctly identify some interactions that are ranked low by other methods due to the lack of training examples of the involved proteins.

===================================
paper_id: 5782250; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: A generalized flow for multi-class and binary classification tasks: An Azure ML approach
ABSTRACT: background_label: The constant growth in the present day real-world databases pose computational challenges for a single computer.
background_label: Cloud-based platforms, on the other hand, are capable of handling large volumes of information manipulation tasks, thereby necessitating their use for large real-world data set computations.
objective_label: This work focuses on creating a novel Generalized Flow within the cloud-based computing platform: Microsoft Azure Machine Learning Studio (MAMLS) that accepts multi-class and binary classification data sets alike and processes them to maximize the overall classification accuracy.
method_label: First, each data set is split into training and testing data sets, respectively.
method_label: Then, linear and nonlinear classification model parameters are estimated using the training data set.
method_label: Data dimensionality reduction is then performed to maximize classification accuracy.
method_label: For multi-class data sets, data centric information is used to further improve overall classification accuracy by reducing the multi-class classification to a series of hierarchical binary classification tasks.
method_label: Finally, the performance of optimized classification model thus achieved is evaluated and scored on the testing data set.
method_label: The classification characteristics of the proposed flow are comparatively evaluated on 3 public data sets and a local data set with respect to existing state-of-the-art methods.
result_label: On the 3 public data sets, the proposed flow achieves 78-97.5% classification accuracy.
result_label: Also, the local data set, created using the information regarding presence of Diabetic Retinopathy lesions in fundus images, results in 85.3-95.7% average classification accuracy, which is higher than the existing methods.
result_label: Thus, the proposed generalized flow can be useful for a wide range of application-oriented"big data sets".

===================================
paper_id: 3204825; YEAR: 1996
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited - specter
TITLE: A Bayesian hybrid method for context-sensitive spelling correction
ABSTRACT: background_label: Two classes of methods have been shown to be useful for resolving lexical ambiguity.
method_label: The first relies on the presence of particular words within some distance of the ambiguous target word; the second uses the pattern of words and part-of-speech tags around the target word.
background_label: These methods have complementary coverage: the former captures the lexical ``atmosphere'' (discourse topic, tense, etc.
method_label: ), while the latter captures local syntax.
method_label: Yarowsky has exploited this complementarity by combining the two methods using decision lists.
method_label: The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be.
method_label: This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction.
method_label: Decision lists are found, by and large, to outperform either component method.
result_label: However, it is found that further improvements can be obtained by taking into account not just the single strongest piece of evidence, but ALL the available evidence.
result_label: A new hybrid method, based on Bayesian classifiers, is presented for doing this, and its performance improvements are demonstrated.

===================================
paper_id: 47081191; YEAR: 1989
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited - specter
TITLE: Continuous Speech Recognition From Phonetic Transcription
ABSTRACT: background_label: Previous research by the authors has been directed toward phonetic transcription of fluent speech.
method_label: We have applied our techniques to speech recognition on the DARPA Resource Management Task.
method_label: In order to perform speech recognition, however, the phonetic transcription must be interpreted as a sequence of words.
method_label: A central component of this process is lexical access for which a novel method is proposed.

===================================
paper_id: 125136660; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200
TITLE: Rigorous and compliant approaches to one-class classification
ABSTRACT: background_label: Abstract A wide number of real problems requiring qualitative answers should be addressed by one-class classification (OCC), as in the case of authentication studies, verification of particular claims and quality control.
background_label: The key feature of OCC is that models are developed using only samples from the target class, so that a representative sampling is not strictly required for non-target classes.
background_label: On the contrary, in the discriminant analysis (DA) approach, all of the classes considered (at least two) have a non-negligible influence in the definition of the delimiter.
background_label: It follows that faults in the definition of the classes involved and in representative sampling for each of them may determine a bias in the classification rules.
objective_label: A key aspect in one-class classification concerns model optimisation.
method_label: When the optimal modelling conditions are searched by considering parameters such as type II error or specificity (‘compliant’ approach), information from the non-target class is being used and may therefore determine a bias in the model.
method_label: In order to build pure class models (‘rigorous’ approach), only information from the target class should be regarded: in other words, optimisation should be performed only considering type I error, or sensitivity.
result_label: In the present study, ‘compliant’ and ‘rigorous’ approaches are critically compared on real case studies, by applying two novel modelling techniques: partial least squares density modelling (PLS-DM) and data driven soft independent modelling of class analogy (DD-SIMCA).

===================================
paper_id: 3117752; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf - title_cbow200 - title_tfidfcbow200
TITLE: Deep Decision Network for Multi-class Image Classification
ABSTRACT: background_label: In this paper, we present a novel Deep Decision Network (DDN) that provides an alternative approach towards building an efficient deep learning network.
method_label: During the learning phase, starting from the root network node, DDN automatically builds a network that splits the data into disjoint clusters of classes which would be handled by the subsequent expert networks.
method_label: This results in a tree-like structured network driven by the data.
method_label: The proposed method provides an insight into the data by identifying the group of classes that are hard to classify and require more attention when compared to others.
method_label: DDN also has the ability to make early decisions thus making it suitable for timesensitive applications.
result_label: We validate DDN on two publicly available benchmark datasets: CIFAR-10 and CIFAR-100 and it yields state-of-the-art classification performance on both the datasets.
result_label: The proposed algorithm has no limitations to be applied to any generic classification problems.

===================================
paper_id: 52184207; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Constrained Generation of Semantically Valid Graphs via Regularizing Variational Autoencoders
ABSTRACT: background_label: Deep generative models have achieved remarkable success in various data domains, including images, time series, and natural languages.
background_label: There remain, however, substantial challenges for combinatorial structures, including graphs.
background_label: One of the key challenges lies in the difficulty of ensuring semantic validity in context.
background_label: For examples, in molecular graphs, the number of bonding-electron pairs must not exceed the valence of an atom; whereas in protein interaction networks, two proteins may be connected only when they belong to the same or correlated gene ontology terms.
background_label: These constraints are not easy to be incorporated into a generative model.
objective_label: In this work, we propose a regularization framework for variational autoencoders as a step toward semantic validity.
method_label: We focus on the matrix representation of graphs and formulate penalty terms that regularize the output distribution of the decoder to encourage the satisfaction of validity constraints.
result_label: Experimental results confirm a much higher likelihood of sampling valid graphs in our approach, compared with others reported in the literature.

===================================
paper_id: 1890316; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200 - title_tfidf
TITLE: Joint Binary Classifier Learning for ECOC-Based Multi-Class Classification
ABSTRACT: background_label: Error-correcting output coding (ECOC) is one of the most widely used strategies for dealing with multi-class problems by decomposing the original multi-class problem into a series of binary sub-problems.
background_label: In traditional ECOC-based methods, binary classifiers corresponding to those sub-problems are usually trained separately without considering the relationships among these classifiers.
background_label: However, as these classifiers are established on the same training data, there may be some inherent relationships among them.
method_label: Exploiting such relationships can potentially improve the generalization performances of individual classifiers, and, thus, boost ECOC learning algorithms.
method_label: In this paper, we explore to mine and utilize such relationship through a joint classifier learning method, by integrating the training of binary classifiers and the learning of the relationship among them into a unified objective function.
method_label: We also develop an efficient alternating optimization algorithm to solve the objective function.
method_label: To evaluate the proposed method, we perform a series of experiments on eleven datasets from the UCI machine learning repository as well as two datasets from real-world image recognition tasks.
result_label: The experimental results demonstrate the efficacy of the proposed method, compared with state-of-the-art methods for ECOC-based multi-class classification.

===================================
paper_id: 53776855; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf
TITLE: Learning without Memorizing
ABSTRACT: background_label: Incremental learning (IL) is an important task aimed at increasing the capability of a trained model, in terms of the number of classes recognizable by the model.
background_label: The key problem in this task is the requirement of storing data (e.g.
background_label: images) associated with existing classes, while teaching the classifier to learn new classes.
background_label: However, this is impractical as it increases the memory requirement at every incremental step, which makes it impossible to implement IL algorithms on edge devices with limited memory.
objective_label: Hence, we propose a novel approach, called `Learning without Memorizing (LwM)', to preserve the information about existing (base) classes, without storing any of their data, while making the classifier progressively learn the new classes.
method_label: In LwM, we present an information preserving penalty: Attention Distillation Loss ($L_{AD}$), and demonstrate that penalizing the changes in classifiers' attention maps helps to retain information of the base classes, as new classes are added.
result_label: We show that adding $L_{AD}$ to the distillation loss which is an existing information preserving loss consistently outperforms the state-of-the-art performance in the iILSVRC-small and iCIFAR-100 datasets in terms of the overall accuracy of base and incrementally learned classes.

===================================
paper_id: 53281152; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200 - title_tfidf
TITLE: Adversarial Learning of Label Dependency: A Novel Framework for Multi-class Classification
ABSTRACT: background_label: Recent work has shown that exploiting relations between labels improves the performance of multi-label classification.
objective_label: We propose a novel framework based on generative adversarial networks (GANs) to model label dependency.
method_label: The discriminator learns to model label dependency by discriminating real and generated label sets.
method_label: To fool the discriminator, the classifier, or generator, learns to generate label sets with dependencies close to real data.
result_label: Extensive experiments and comparisons on two large-scale image classification benchmark datasets (MS-COCO and NUS-WIDE) show that the discriminator improves generalization ability for different kinds of models

===================================
paper_id: 6748437; YEAR: 1996
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf
TITLE: Estimating Markov model structures
ABSTRACT: background_label: The author investigates the derivation of Markov model structures from text corpora.
background_label: The structure of a Markov model is its number of states plus the set of outputs and transitions with non-zero probability.
background_label: The domain of the investigated models is part-of-speech tagging.
method_label: The investigations concern two methods to derive Markov models and their structures.
method_label: Both are able to form categories and allow words to belong to more than one of them.
method_label: The first method is model merging, which starts with a large and corpus-specific model and successively merges states to generate smaller and more general models.
method_label: The second method is model splitting, which is the inverse procedure and starts with a small and general model.
method_label: States are successively split to generate larger and more specific models.
result_label: In an experiment, the author shows that the combination of these techniques yields tagging accuracies that are at least equivalent to those of standard approaches.

===================================
paper_id: 14285303; YEAR: 1980
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf
TITLE: Nosing Around the Neighborhood: A New System Structure and Classification Rule for Recognition in Partially Exposed Environments
ABSTRACT: background_label: The scope of the classical k-NN classification techniques is enlarged under this study to cover partially exposed environments.
background_label: The modified classification system structure required for successful operation in environments, wherein all the inherent pattern classes are not exposed to the system prior to deployment, is developed and illustrated with the aid of a specific classification rule-the neighborhood census rule (NCR).
background_label: Admittedly, alternative rules can be visualized to fit this modified structure.
objective_label: However, this study concentrates on the use of NCR to bring out the underlying philosophy and develops optimum thresholds for admittance of unknown samples into the set of presently known classes.
method_label: These thresholds are learned from the available training samples of these classes.
method_label: This learning represents a new dimensionality of the learning system structure in that estimates of the domains of the known classes are developed in addition to learning of the discrimination among these classes.
method_label: This facilitates identification of samples belonging to the classes previously unexposed to the recognition system.
result_label: Experimental results are also presented in support of the proposed concepts and methodology for operation in partially exposed environments.

===================================
paper_id: 6764656; YEAR: 2002
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf - title_cbow200 - title_tfidfcbow200
TITLE: Multi-Class Active Learning for Image Classification
ABSTRACT: background_label: One of the principal bottlenecks in applying learning techniques to classification problems is the large amount of labeled training data required.
background_label: Especially for images and video, providing training data is very expensive in terms of human time and effort.
objective_label: In this paper we propose an active learning approach to tackle the problem.
method_label: Instead of passively accepting random training examples, the active learning algorithm iteratively selects unlabeled examples for the user to label, so that human effort is focused on labeling the most “useful” examples.
method_label: Our method relies on the idea of uncertainty sampling, in which the algorithm selects unlabeled examples that it finds hardest to classify.
method_label: Specifically, we propose an uncertainty measure that generalizes margin-based uncertainty to the multi-class case and is easy to compute, so that active learning can handle a large number of classes and large data sizes efficiently.
result_label: We demonstrate results for letter and digit recognition on datasets from the UCI repository, object recognition results on the Caltech-101 dataset, and scene categorization results on a dataset of 13 natural scene categories.
result_label: The proposed method gives large reductions in the number of training examples required over random selection to achieve similar classification accuracy, with little computational overhead.

===================================
paper_id: 34254969; YEAR: 2001
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Self-Similar Layered Hidden Markov Models
ABSTRACT: other_label: Abstract.
background_label: Hidden Markov Models (HMM) have proven to be useful in a variety of real world applications where considerations for uncertainty are crucial.
background_label: Such an advantage can be more leveraged if HMM can be scaled up to deal with complex problems.
objective_label: In this paper, we introduce, analyze and demonstrate SelfSimilar Layered HMM (SSLHMM), for a certain group of complex problems which show self-similar property, and exploit this property to reduce the complexity of model construction.
method_label: We show how the embedded knowledge of selfsimilar structure can be used to reduce the complexity of learning and increase the accuracy of the learned model.
method_label: Moreover, we introduce three different types of self-similarity in SSLHMM, and investigate their performance in the context of synthetic data and real-world network databases.
result_label: We show that SSLHMM has several advantages comparing to conventional HMM techniques and it is more efficient and accurate than one-step, flat method for model construction.
result_label: IntroductionThere is a vast amount of natural structures and physical systems which contain selfsimilar structures that are made through recurrent processes.
background_label: To name a few: ocean flows, changes in the yearly flood levels of rivers, voltages across nerve membranes, musical melodies, human brains, economic markets, Internet web logs and network data create enormously complex self-similar data [21] .
background_label: While there have been much effort on observing self-similar structures in scientific databases and natural structures, there are few works on using self-similar structure and fractal dimension for the purpose of data mining and predictive modeling.
background_label: Among these works, using fractal dimension and self-similarity to reduce the dimensionally curse [21], learning association rules [2] and applications in spatial joint selectivity in databases [9] are considerable.
method_label: In this paper we introduce a novel technique which uses the self-similar structure for predictive modeling using a Self-Similar Layered Hidden Markov Model (SSLHMM).
method_label: Despite the broad range of application areas shown for classic HMMs, they do have limitations and do not easily handle problems with certain characteristics.
background_label: For instance, classic HMM has difficulties to model complex problems with large states spaces.
method_label: Among the recognized limitations, we only focus on complexity of HMM for a certain category of problems with the following characteristics: 1) The uncertainty and complexity embedded in these applications make it difficult and impractical to construct the model in one step.
result_label: 2) Systems are self-similar, contain self-similar struc

===================================
paper_id: 117059123; YEAR: 1993
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200
TITLE: Generativity and Systematicity in Neural Network Combinatorial Learning
ABSTRACT: background_label: This thesis addresses a set of problems faced by connectionist learning that have originated from the observation that connectionist cognitive models lack two fundamental properties of the mind: Generativity, stemming from the boundless cognitive competence one can exhibit, and systematicity, due to the existence of symmetries within them.
background_label: Such properties have seldom been seen in neural networks models, which have typically suffered from problems of inadequate generalization, as examplified both by small number of generalizations relative to training set sizes and heavy interference between newly learned items and previously learned information.
background_label: Symbolic theories, arguing that mental representations have syntactic and semantic structure built from structured combinations of symbolic constituents, can in principle account for these properties (both arise from the sensitivity of structured semantic content with a generative and systematic syntax).
objective_label: This thesis studies the question of whether connectionism, arguing that symbolic theories can only provide approximative cognitive descriptions which can only be made precise at a sub-symbolic level, can also account for these properties.
method_label: Taking a cue from the domains in which human learning most dramatically displays generativity and systematicity, the answer is hypothesized to be positive for domains with combinatorial structure.
method_label: A study of such domains is performed, and a measure of combinatorial complexity in terms of information/entropy is used.
method_label: Experiments are then designed to confirm the hypothesis.
background_label: It is found that a basic connectionist model trained on a very small percentage of a simple combinatorial domain of recognizing letter sequences can correctly generalize to large numbers of novel sequences.
background_label: These numbers are found to grow exponentially when the combinatorial complexity of the domain grows.
background_label: The same behavior is even more dramatically obtained with virtual generalizations: new items which, although not correctly generalized, can be learned in a few presentations while leaving performance on the previously learned items intact.
result_label: Experiments are repeated with fully-distributed representations, and results imply that performance is not degraded.
method_label: When weight elimination is added, perfect systematicity is obtained.
method_label: A formal analysis is then attempted in a simpler case.
result_label: The more general case is treated with contribution analysis.

===================================
paper_id: 58036507; YEAR: 1990
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: Syntactic Neural Networks
ABSTRACT: background_label: We introduce a new connectionist paradigm which views neural networks as implementations of syntactic pattern recognition algorithms.
background_label: Thus, learning is seen as a process of grammatical inference and recognition as a process of parsing.
method_label: Naturally, the possible realizations of this theme are diverse; in this paper we present some initial explorations of the case where the pattern grammar is context-free, inferred (from examples) by a separate procedure, and then mapped onto a connectionist parser.
method_label: Unlike most neural networks for which structure is pre-defined, the resulting network has as many levels as are necessary and arbitrary connections between levels.
method_label: Furthermore, by the addition of a delay element, the network becomes capable of dealing with time-varying patterns in a simple and efficient manner.
result_label: Since grammatical inference algorithms are notoriously expensive computationally, we place an important restriction on the type of context-free grammars which can be inferred.
result_label: This dramatically reduces complexity.
background_label: The resulting grammars are called ‘strictly-hierarchical’ and map straightforwardly onto a temporal connectionist parser (TCP) using a relatively small number of neurons.
background_label: The new paradigm is applicable to a variety of pattern-processing tasks such as speech recognition and character recognition.
background_label: We concentrate here on hand-written character recognition; performance in other problem domains will be reported in future publications.
method_label: Results are presented to illustrate the performance of the system with respect to a number of parameters, namely, the inherent variability of the data, the nature of the learning (supervised or unsupervised) and the details of the clustering procedure used to limit the number of non-terminals inferred.
result_label: In each of these cases (eight in total), we contrast the performance of a stochastic and a non-stochastic TCP.
result_label: The stochastic TCP does have greater powers of discrimination, but in many cases the results were very similar.
result_label: If this result holds in practical situations it is important, because the non-stochastic version has a straightforward implementation in silicon.

===================================
paper_id: 15648012; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter
TITLE: Training a Restricted Boltzmann Machine for Classification by Labeling Model Samples
ABSTRACT: objective_label: We propose an alternative method for training a classification model.
method_label: Using the MNIST set of handwritten digits and Restricted Boltzmann Machines, it is possible to reach a classification performance competitive to semi-supervised learning if we first train a model in an unsupervised fashion on unlabeled data only, and then manually add labels to model samples instead of training data samples with the help of a GUI.
method_label: This approach can benefit from the fact that model samples can be presented to the human labeler in a video-like fashion, resulting in a higher number of labeled examples.
result_label: Also, after some initial training, hard-to-classify examples can be distinguished from easy ones automatically, saving manual work.

===================================
paper_id: 8847651; YEAR: 2011
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter
TITLE: Classification of Sets using Restricted Boltzmann Machines
ABSTRACT: background_label: We consider the problem of classification when inputs correspond to sets of vectors.
background_label: This setting occurs in many problems such as the classification of pieces of mail containing several pages, of web sites with several sections or of images that have been pre-segmented into smaller regions.
method_label: We propose generalizations of the restricted Boltzmann machine (RBM) that are appropriate in this context and explore how to incorporate different assumptions about the relationship between the input sets and the target class within the RBM.
result_label: In experiments on standard multiple-instance learning datasets, we demonstrate the competitiveness of approaches based on RBMs and apply the proposed variants to the problem of incoming mail classification.

