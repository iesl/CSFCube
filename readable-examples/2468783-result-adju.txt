======================================================================
paper_id: 2468783; YEAR: 2006
TITLE: Similarity of Semantic Relations
ABSTRACT: background_label: There are at least two kinds of similarity.
background_label: Relational similarity is correspondence between relations, in contrast with attributional similarity, which is correspondence between attributes.
background_label: When two words have a high degree of attributional similarity, we call them synonyms.
background_label: When two pairs of words have a high degree of relational similarity, we say that their relations are analogous.
background_label: For example, the word pair mason:stone is analogous to the pair carpenter:wood.
method_label: This paper introduces Latent Relational Analysis (LRA), a method for measuring relational similarity.
background_label: LRA has potential applications in many areas, including information extraction, word sense disambiguation, and information retrieval.
background_label: Recently the Vector Space Model (VSM) of information retrieval has been adapted to measuring relational similarity, achieving a score of 47% on a collection of 374 college-level multiple-choice word analogy questions.
background_label: In the VSM approach, the relation between a pair of words is characterized by a vector of frequencies of predefined patterns in a large corpus.
method_label: LRA extends the VSM approach in three ways: (1) the patterns are derived automatically from the corpus, (2) the Singular Value Decomposition (SVD) is used to smooth the frequency data, and (3) automatically generated synonyms are used to explore variations of the word pairs.
result_label: LRA achieves 56% on the 374 analogy questions, statistically equivalent to the average human score of 57%.
result_label: On the related problem of classifying semantic relations, LRA achieves similar gains over the VSM.
===================================
paper_id: 405; YEAR: 2006
adju relevance: Identical (+3)
difference: 2; annotator1: 1; annotator3: 3
sources: abs_cbow200 - abs_tfidf - title_tfidf - specter
TITLE: Expressing Implicit Semantic Relations without Supervision
ABSTRACT: background_label: We present an unsupervised learning algorithm that mines large text corpora for patterns that express implicit semantic relations.
background_label: For a given input word pair X:Y with some unspecified semantic relations, the corresponding output list of patterns<P1,...,Pm>is ranked according to how well each pattern Pi expresses the relations between X and Y.
method_label: For example, given X=ostrich and Y=bird, the two highest ranking output patterns are"X is the largest Y"and"Y such as the X".
method_label: The output patterns are intended to be useful for finding further pairs with the same relations, to support the construction of lexicons, ontologies, and semantic networks.
method_label: The patterns are sorted by pertinence, where the pertinence of a pattern Pi for a word pair X:Y is the expected relational similarity between the given pair and typical pairs for Pi.
result_label: The algorithm is empirically evaluated on two tasks, solving multiple-choice SAT word analogy questions and classifying semantic relations in noun-modifier pairs.
result_label: On both tasks, the algorithm achieves state-of-the-art results, performing significantly better than several alternative pattern ranking algorithms, based on tf-idf.

===================================
paper_id: 10202222; YEAR: 2013
adju relevance: Identical (+3)
difference: 2; annotator1: 1; annotator3: 3
sources: abs_cbow200 - abs_tfidf - abs_tfidfcbow200 - specter
TITLE: Distributional semantics beyond words: Supervised learning of analogy and paraphrase
ABSTRACT: background_label: There have been several efforts to extend distributional semantics beyond individual words, to measure the similarity of word pairs, phrases, and sentences (briefly, tuples; ordered sets of words, contiguous or noncontiguous).
background_label: One way to extend beyond words is to compare two tuples using a function that combines pairwise similarities between the component words in the tuples.
background_label: A strength of this approach is that it works with both relational similarity (analogy) and compositional similarity (paraphrase).
background_label: However, past work required hand-coding the combination function for different tasks.
objective_label: The main contribution of this paper is that combination functions are generated by supervised learning.
result_label: We achieve state-of-the-art results in measuring relational similarity between word pairs (SAT analogies and SemEval~2012 Task 2) and measuring compositional similarity between noun-modifier phrases and unigrams (multiple-choice paraphrase questions).

===================================
paper_id: 12428472; YEAR: 2004
adju relevance: Identical (+3)
difference: 0; annotator1: 3; annotator3: 3
sources: specter - abs_cbow200 - abs_tfidf - abs_tfidfcbow200
TITLE: Human-Level Performance on Word Analogy Questions by Latent Relational Analysis
ABSTRACT: background_label: This paper introduces Latent Relational Analysis (LRA), a method for measuring relational similarity.
background_label: LRA has potential applications in many areas, including information extraction, word sense disambiguation, machine translation, and information retrieval.
background_label: Relational similarity is correspondence between relations, in contrast with attributional similarity, which is correspondence between attributes.
method_label: When two words have a high degree of attributional similarity, we call them synonyms.
method_label: When two pairs of words have a high degree of relational similarity, we say that their relations are analogous.
result_label: For example, the word pair mason/stone is analogous to the pair carpenter/wood.
background_label: Past work on semantic similarity measures has mainly been concerned with attributional similarity.
background_label: Recently the Vector Space Model (VSM) of information retrieval has been adapted to the task of measuring relational similarity, achieving a score of 47% on a collection of 374 college-level multiple-choice word analogy questions.
method_label: In the VSM approach, the relation between a pair of words is characterized by a vector of frequencies of predefined patterns in a large corpus.
method_label: LRA extends the VSM approach in three ways: (1) the patterns are derived automatically from the corpus (they are not predefined), (2) the Singular Value Decomposition (SVD) is used to smooth the frequency data (it is also used this way in Latent Semantic Analysis), and (3) automatically generated synonyms are used to explore reformulations of the word pairs.
result_label: LRA achieves 56% on the 374 analogy questions, statistically equivalent to the average human score of 57%.
result_label: On the related problem of classifying noun-modifier relations, LRA achieves similar gains over the VSM, while using a smaller corpus.

===================================
paper_id: 5104622; YEAR: 2005
adju relevance: Identical (+3)
difference: 0; annotator1: 3; annotator3: 3
sources: cited - title_cbow200 - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Measuring Semantic Similarity by Latent Relational Analysis
ABSTRACT: background_label: This paper introduces Latent Relational Analysis (LRA), a method for measuring semantic similarity.
background_label: LRA measures similarity in the semantic relations between two pairs of words.
background_label: When two pairs have a high degree of relational similarity, they are analogous.
background_label: For example, the pair cat:meow is analogous to the pair dog:bark.
background_label: There is evidence from cognitive science that relational similarity is fundamental to many cognitive and linguistic tasks (e.g., analogical reasoning).
method_label: In the Vector Space Model (VSM) approach to measuring relational similarity, the similarity between two pairs is calculated by the cosine of the angle between the vectors that represent the two pairs.
method_label: The elements in the vectors are based on the frequencies of manually constructed patterns in a large corpus.
method_label: LRA extends the VSM approach in three ways: (1) patterns are derived automatically from the corpus, (2) Singular Value Decomposition is used to smooth the frequency data, and (3) synonyms are used to reformulate word pairs.
method_label: This paper describes the LRA algorithm and experimentally compares LRA to VSM on two tasks, answering college-level multiple-choice word analogy questions and classifying semantic relations in noun-modifier expressions.
result_label: LRA achieves state-of-the-art results, reaching human-level performance on the analogy questions and significantly exceeding VSM performance on both tasks.

===================================
paper_id: 5052538; YEAR: 2003
adju relevance: Identical (+3)
difference: 2; annotator1: 1; annotator3: 3
sources: title_tfidfcbow200 - title_cbow200 - abs_cbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Learning Analogies and Semantic Relations
ABSTRACT: background_label: We present an algorithm for learning from unlabeled text, based on the Vector Space Model (VSM) of information retrieval, that can solve verbal analogy questions of the kind found in the Scholastic Aptitude Test (SAT).
background_label: A verbal analogy has the form A:B::C:D, meaning"A is to B as C is to D"; for example, mason:stone::carpenter:wood.
method_label: SAT analogy questions provide a word pair, A:B, and the problem is to select the most analogous word pair, C:D, from a set of five choices.
method_label: The VSM algorithm correctly answers 47% of a collection of 374 college-level analogy questions (random guessing would yield 20% correct).
objective_label: We motivate this research by relating it to work in cognitive science and linguistics, and by applying it to a difficult problem in natural language processing, determining semantic relations in noun-modifier pairs.
objective_label: The problem is to classify a noun-modifier pair, such as"laser printer", according to the semantic relation between the noun (printer) and the modifier (laser).
method_label: We use a supervised nearest-neighbour algorithm that assigns a class to a given noun-modifier pair by finding the most analogous noun-modifier pair in the training data.
method_label: With 30 classes of semantic relations, on a collection of 600 labeled noun-modifier pairs, the learning algorithm attains an F value of 26.5% (random guessing: 3.3%).
result_label: With 5 classes of semantic relations, the F value is 43.2% (random: 20%).
result_label: The performance is state-of-the-art for these challenging problems.

===================================
paper_id: 15319681; YEAR: 2010
adju relevance: Similar (+2)
difference: 2; annotator1: 2; annotator3: 0
sources: abs_tfidf
TITLE: Improved Sentence Similarity Algorithm based on VSM and its application in Question Answering System
ABSTRACT: background_label: In the FAQ-based Chinese Question Answering System, the most critical issue is how to calculate the similarity between the user questions and the questions in the FAQ.
background_label: The traditional VSM-based Sentence Similarity Algorithm usually regards word as the basic linguistic unit of sentences and mainly considers the statistical information of words in questions, but doesn't take the word importance in the professional field and the semantic information of words into account.
objective_label: For these reasons, this paper proposes an Improved Sentence Similarity Algorithm Based on VSM, regarding notion as the basic linguistic unit of sentences, through conceptually abstracting and professionally classifying to improve the performance of Sentence Similarity Algorithm.
result_label: Testing in Chinese FAQ system of specific areas, experimental result shows that the performance of the improved algorithm is superior to the traditional VSM-based sentence similarity algorithm evidently.

===================================
paper_id: 14972026; YEAR: 2009
adju relevance: Similar (+2)
difference: 1; annotator1: 2; annotator3: 3
sources: specter - title_cbow200 - abs_cbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf
TITLE: Measuring the similarity between implicit semantic relations from the web
ABSTRACT: background_label: Measuring the similarity between semantic relations that hold among entities is an important and necessary step in various Web related tasks such as relation extraction, information retrieval and analogy detection.
background_label: For example, consider the case in which a person knows a pair of entities (e.g.
other_label: Google, YouTube), between which a particular relation holds (e.g.
background_label: acquisition).
other_label: The person is interested in retrieving other such pairs with similar relations (e.g.
result_label: Microsoft, Powerset).
objective_label: Existing keyword-based search engines cannot be applied directly in this case because, in keyword-based search, the goal is to retrieve documents that are relevant to the words used in a query -- not necessarily to the relations implied by a pair of words.
method_label: We propose a relational similarity measure, using a Web search engine, to compute the similarity between semantic relations implied by two pairs of words.
method_label: Our method has three components: representing the various semantic relations that exist between a pair of words using automatically extracted lexical patterns, clustering the extracted lexical patterns to identify the different patterns that express a particular semantic relation, and measuring the similarity between semantic relations using a metric learning approach.
method_label: We evaluate the proposed method in two tasks: classifying semantic relations between named entities, and solving word-analogy questions.
result_label: The proposed method outperforms all baselines in a relation classification task with a statistically significant average precision score of 0.74.
result_label: Moreover, it reduces the time taken by Latent Relational Analysis to process 374 word-analogy questions from 9 days to less than 6 hours, with an SAT score of 51%.

===================================
paper_id: 2281724; YEAR: 2013
adju relevance: Similar (+2)
difference: 2; annotator1: 2; annotator3: 0
sources: abs_tfidfcbow200 - abs_tfidf - specter
TITLE: Combining Heterogeneous Models for Measuring Relational Similarity
ABSTRACT: background_label: AbstractIn this work, we study the problem of measuring relational similarity between two word pairs (e.g., silverware:fork and clothing:shirt).
background_label: Due to the large number of possible relations, we argue that it is important to combine multiple models based on heterogeneous information sources.
method_label: Our overall system consists of two novel general-purpose relational similarity models and three specific word relation models.
result_label: When evaluated in the setting of a recently proposed SemEval-2012 task, our approach outperforms the previous best system substantially, achieving a 54.1% relative increase in Spearman's rank correlation.

===================================
paper_id: 9322367; YEAR: 2005
adju relevance: Similar (+2)
difference: 1; annotator1: 1; annotator3: 2
sources: cited - title_cbow200 - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Corpus-based Learning of Analogies and Semantic Relations
ABSTRACT: background_label: We present an algorithm for learning from unlabeled text, based on the Vector Space Model (VSM) of information retrieval, that can solve verbal analogy questions of the kind found in the SAT college entrance exam.
background_label: A verbal analogy has the form A:B::C:D, meaning"A is to B as C is to D"; for example, mason:stone::carpenter:wood.
method_label: SAT analogy questions provide a word pair, A:B, and the problem is to select the most analogous word pair, C:D, from a set of five choices.
method_label: The VSM algorithm correctly answers 47% of a collection of 374 college-level analogy questions (random guessing would yield 20% correct; the average college-bound senior high school student answers about 57% correctly).
objective_label: We motivate this research by applying it to a difficult problem in natural language processing, determining semantic relations in noun-modifier pairs.
objective_label: The problem is to classify a noun-modifier pair, such as"laser printer", according to the semantic relation between the noun (printer) and the modifier (laser).
method_label: We use a supervised nearest-neighbour algorithm that assigns a class to a given noun-modifier pair by finding the most analogous noun-modifier pair in the training data.
method_label: With 30 classes of semantic relations, on a collection of 600 labeled noun-modifier pairs, the learning algorithm attains an F value of 26.5% (random guessing: 3.3%).
result_label: With 5 classes of semantic relations, the F value is 43.2% (random: 20%).
result_label: The performance is state-of-the-art for both verbal analogies and noun-modifier relations.

===================================
paper_id: 18997693; YEAR: 2016
adju relevance: Related (+1)
difference: 0; annotator1: 1; annotator3: 1
sources: abs_tfidf
TITLE: An expressive dissimilarity measure for relational clustering using neighbourhood trees
ABSTRACT: background_label: Clustering is an underspecified task: there are no universal criteria for what makes a good clustering.
background_label: This is especially true for relational data, where similarity can be based on the features of individuals, the relationships between them, or a mix of both.
background_label: Existing methods for relational clustering have strong and often implicit biases in this respect.
objective_label: In this paper, we introduce a novel similarity measure for relational data.
method_label: It is the first measure to incorporate a wide variety of types of similarity, including similarity of attributes, similarity of relational context, and proximity in a hypergraph.
method_label: We experimentally evaluate how using this similarity affects the quality of clustering on very different types of datasets.
result_label: The experiments demonstrate that (a) using this similarity in standard clustering methods consistently gives good results, whereas other measures work well only on datasets that match their bias; and (b) on most datasets, the novel similarity outperforms even the best among the existing ones.

===================================
paper_id: 15637201; YEAR: 2012
adju relevance: Related (+1)
difference: 0; annotator1: 1; annotator3: 1
sources: specter - abs_cbow200 - abs_tfidf
TITLE: UTD: Determining Relational Similarity Using Lexical Patterns
ABSTRACT: objective_label: AbstractIn this paper we present our approach for assigning degrees of relational similarity to pairs of words in the SemEval-2012 Task 2.
method_label: To measure relational similarity we employed lexical patterns that can match against word pairs within a large corpus of 12 million documents.
method_label: Patterns are weighted by obtaining statistically estimated lower bounds on their precision for extracting word pairs from a given relation.
method_label: Finally, word pairs are ranked based on a model predicting the probability that they belong to the relation of interest.
result_label: This approach achieved the best results on the SemEval 2012 Task 2, obtaining a Spearman correlation of 0.229 and an accuracy on reproducing human answers to MaxDiff questions of 39.4%.

===================================
paper_id: 3101448; YEAR: 2015
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200 - title_tfidf - specter
TITLE: Evaluating semantic similarity and relatedness over the semantic grouping of clinical term pairs.
ABSTRACT: background_label: INTRODUCTION This article explores how measures of semantic similarity and relatedness are impacted by the semantic groups to which the concepts they are measuring belong.
objective_label: Our goal is to determine if there are distinctions between homogeneous comparisons (where both concepts belong to the same group) and heterogeneous ones (where the concepts are in different groups).
method_label: Our hypothesis is that the similarity measures will be significantly affected since they rely on hierarchical is-a relations, whereas relatedness measures should be less impacted since they utilize a wider range of relations.
method_label: In addition, we also evaluate the effect of combining different measures of similarity and relatedness.
result_label: Our hypothesis is that these combined measures will more closely correlate with human judgment, since they better reflect the rich variety of information humans use when assessing similarity and relatedness.
method_label: METHOD We evaluate our method on four reference standards.
background_label: Three of the reference standards were annotated by human judges for relatedness and one was annotated for similarity.
result_label: RESULTS We found significant differences in the correlation of semantic similarity and relatedness measures with human judgment, depending on which semantic groups were involved.
result_label: We also found that combining a definition based relatedness measure with an information content similarity measure resulted in significant improvements in correlation over individual measures.
other_label: AVAILABILITY The semantic similarity and relatedness package is an open source program available from http://umls-similarity.sourceforge.net/.
other_label: The reference standards are available at http://www.people.vcu.edu/∼{}btmcinnes/downloads.html.

===================================
paper_id: 20121139; YEAR: 2017
adju relevance: Related (+1)
difference: 1; annotator1: 0; annotator3: 1
sources: specter - abs_cbow200 - abs_tfidf
TITLE: Compositional Approaches for Representing Relations Between Words: A Comparative Study
ABSTRACT: background_label: Identifying the relations that exist between words (or entities) is important for various natural language processing tasks such as, relational search, noun-modifier classification and analogy detection.
background_label: A popular approach to represent the relations between a pair of words is to extract the patterns in which the words co-occur with from a corpus, and assign each word-pair a vector of pattern frequencies.
background_label: Despite the simplicity of this approach, it suffers from data sparseness, information scalability and linguistic creativity as the model is unable to handle previously unseen word pairs in a corpus.
method_label: In contrast, a compositional approach for representing relations between words overcomes these issues by using the attributes of each individual word to indirectly compose a representation for the common relations that hold between the two words.
objective_label: This study aims to compare different operations for creating relation representations from word-level representations.
method_label: We investigate the performance of the compositional methods by measuring the relational similarities using several benchmark datasets for word analogy.
result_label: Moreover, we evaluate the different relation representations in a knowledge base completion task.

===================================
paper_id: 1359050; YEAR: 1997
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator3: 0
sources: cited - title_cbow200 - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy
ABSTRACT: objective_label: This paper presents a new approach for measuring semantic similarity/distance between words and concepts.
objective_label: It combines a lexical taxonomy structure with corpus statistical information so that the semantic distance between nodes in the semantic space constructed by the taxonomy can be better quantified with the computational evidence derived from a distributional analysis of corpus data.
method_label: Specifically, the proposed measure is a combined approach that inherits the edge-based approach of the edge counting scheme, which is then enhanced by the node-based approach of the information content calculation.
method_label: When tested on a common data set of word pair similarity ratings, the proposed approach outperforms other computational models.
result_label: It gives the highest correlation value (r = 0.828) with a benchmark based on human similarity judgements, whereas an upper bound (r = 0.885) is observed when human subjects replicate the same task.

===================================
paper_id: 145580646; YEAR: 1991
adju relevance: Related (+1)
difference: 0; annotator1: 1; annotator3: 1
sources: title_cbow200 - title_tfidfcbow200 - abs_tfidf - title_tfidf - specter
TITLE: Contextual correlates of semantic similarity
ABSTRACT: background_label: Abstract The relationship between semantic and contextual similarity is investigated for pairs of nouns that vary from high to low semantic similarity.
method_label: Semantic similarity is estimated by subjective ratings; contextual similarity is estimated by the method of sorting sentential contexts.
result_label: The results show an inverse linear relationship between similarity of meaning and the discriminability of contexts.
result_label: This relation, is obtained for two separate corpora of sentence contexts.
result_label: It is concluded that, on average, for words in the same language drawn from the same syntactic and semantic categories, the more often two words can be substituted into the same contexts the more similar in meaning they are judged to be.

===================================
paper_id: 7992957; YEAR: 2006
adju relevance: Related (+1)
difference: 0; annotator1: 1; annotator3: 1
sources: title_tfidfcbow200 - title_cbow200 - title_tfidf
TITLE: Ontologizing Semantic Relations
ABSTRACT: background_label: Many algorithms have been developed to harvest lexical semantic resources, however few have linked the mined knowledge into formal knowledge repositories.
objective_label: In this paper, we propose two algorithms for automatically ontologizing (attaching) semantic relations into WordNet.
result_label: We present an empirical evaluation on the task of attaching part-of and causation relations, showing an improvement on F-score over a baseline model.

===================================
paper_id: 3391337; YEAR: 2018
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator3: 0
sources: abs_cbow200 - abs_tfidf - specter
TITLE: Calculating the similarity between words and sentences using a lexical database and corpus statistics
ABSTRACT: background_label: Calculating the semantic similarity between sentences is a long dealt problem in the area of natural language processing.
background_label: The semantic analysis field has a crucial role to play in the research related to the text analytics.
background_label: The semantic similarity differs as the domain of operation differs.
objective_label: In this paper, we present a methodology which deals with this issue by incorporating semantic similarity and corpus statistics.
method_label: To calculate the semantic similarity between words and sentences, the proposed method follows an edge-based approach using a lexical database.
method_label: The methodology can be applied in a variety of domains.
method_label: The methodology has been tested on both benchmark standards and mean human similarity dataset.
method_label: When tested on these two datasets, it gives highest correlation value for both word and sentence similarity outperforming other similar models.
result_label: For word similarity, we obtained Pearson correlation coefficient of 0.8753 and for sentence similarity, the correlation obtained is 0.8794.

===================================
paper_id: 28649575; YEAR: 2011
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200 - specter
TITLE: Entropy, semantic relatedness and proximity.
ABSTRACT: background_label: Although word co-occurrences within a document have been demonstrated to be semantically useful, word interactions over a local range have been largely neglected by psychologists due to practical challenges.
background_label: Shannon's (Bell Systems Technical Journal, 27, 379-423, 623-665, 1948) conceptualization of information theory suggests that these interactions should be useful for understanding communication.
background_label: Computational advances make an examination of local word-word interactions possible for a large text corpus.
method_label: We used Brants and Franz's (2006) dataset to generate conditional probabilities for 62,474 word pairs and entropy calculations for 9,917 words in Nelson, McEvoy, and Schreiber's (Behavior Research Methods, Instruments, & Computers, 36, 402-407, 2004) free association norms.
result_label: Semantic associativity correlated moderately with the probabilities and was stronger when the two words were not adjacent.
result_label: The number of semantic associates for a word and the entropy of a word were also correlated.
result_label: Finally, language entropy decreases from 11 bits for single words to 6 bits per word for four-word sequences.
result_label: The probabilities and entropies discussed here are included in the supplemental materials for the article.

===================================
paper_id: 195717148; YEAR: 2003
adju relevance: Related (+1)
difference: 0; annotator1: 1; annotator3: 1
sources: cited - title_cbow200 - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Kernel methods for relation extraction
ABSTRACT: background_label: We present an application of kernel methods to extracting relations from unstructured natural language sources.
method_label: We introduce kernels defined over shallow parse representations of text, and design efficient algorithms for computing the kernels.
method_label: We use the devised kernels in conjunction with Support Vector Machine and Voted Perceptron learning algorithms for the task of extracting person-affiliation and organization-location relations from text.
result_label: We experimentally evaluate the proposed methods and compare them with feature-based learning algorithms, with promising results.

===================================
paper_id: 44134877; YEAR: 2018
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Querying Word Embeddings for Similarity and Relatedness
ABSTRACT: background_label: AbstractWord embeddings obtained from neural network models such as Word2Vec Skipgram have become popular representations of word meaning and have been evaluated on a variety of word similarity and relatedness norming data.
background_label: Skipgram generates a set of word and context embeddings, the latter typically discarded after training.
method_label: We demonstrate the usefulness of context embeddings in predicting asymmetric association between words from a recently published dataset of production norms (Jouravlev and McRae, 2016) .
result_label: Our findings suggest that humans respond with words closer to the cue within the context embedding space (rather than the word embedding space), when asked to generate thematically related words.

===================================
paper_id: 16113725; YEAR: 2010
adju relevance: Related (+1)
difference: 2; annotator1: 2; annotator3: 0
sources: specter
TITLE: Robust Measurement and Comparison of Context Similarity for Finding Translation Pairs
ABSTRACT: background_label: AbstractIn cross-language information retrieval it is often important to align words that are similar in meaning in two corpora written in different languages.
background_label: Previous research shows that using context similarity to align words is helpful when no dictionary entry is available.
method_label: We suggest a new method which selects a subset of words (pivot words) associated with a query and then matches these words across languages.
method_label: To detect word associations, we demonstrate that a new Bayesian method for estimating Point-wise Mutual Information provides improved accuracy.
method_label: In the second step, matching is done in a novel way that calculates the chance of an accidental overlap of pivot words using the hypergeometric distribution.
method_label: We implemented a wide variety of previously suggested methods.
result_label: Testing in two conditions, a small comparable corpora pair and a large but unrelated corpora pair, both written in disparate languages, we show that our approach consistently outperforms the other systems.

===================================
paper_id: 9289724; YEAR: 2016
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator3: 0
sources: abs_tfidfcbow200 - specter
TITLE: RelSim: Relation Similarity Search in Schema-Rich Heterogeneous Information Networks
ABSTRACT: background_label: AbstractRecent studies have demonstrated the power of modeling real world data as heterogeneous information networks (HINs) consisting of multiple types of entities and relations.
background_label: Unfortunately, most of such studies (e.g., similarity search) confine discussions on the networks with only a few entity and relationship types, such as DBLP.
background_label: In the real world, however, the network schema can be rather complex, such as Freebase.
background_label: In such HINs with rich schema, it is often too much burden to ask users to provide explicit guidance in selecting relations for similarity search.
objective_label: In this paper, we study the problem of relation similarity search in schema-rich HINs.
method_label: Under our problem setting, users are only asked to provide some simple relation instance examples (e.g., Barack Obama, John Kerry and George W. Bush, Condoleezza Rice ) as a query, and we automatically detect the latent semantic relation (L-SR) implied by the query (e.g., "president vs. secretary-ofstate").
method_label: Such LSR will help to find other similar relation instances (e.g., Bill Clinton, Madeleine Albright ).
method_label: In order to solve the problem, we first define a new meta-path-based relation similarity measure, RelSim, to measure the similarity between relation instances in schema-rich HINs.
method_label: Then given a query, we propose an optimization model to efficiently learn LSR implied in the query through linear programming, and perform fast relation similarity search using RelSim based on the learned LSR.
result_label: The experiments on real world datasets derived from Freebase demonstrate the effectiveness and efficiency of our approach.

===================================
paper_id: 16702013; YEAR: 2016
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200
TITLE: SERGIOJIMENEZ at SemEval-2016 Task 1: Effectively Combining Paraphrase Database, String Matching, WordNet, and Word Embedding for Semantic Textual Similarity
ABSTRACT: background_label: In this paper, a system for semantic textual similarity, which participated in Task1 in SemEval 2016 (monolingual and crosslingual sub-tasks) is described.
method_label: The system contains a preprocessing step that simplifies text using PPDB 2.0 and detects negations.
method_label: Also, six lexical similarity functions were constructed using string matching, word embedding and synonyms-antonyms relations in WordNet.
method_label: These lexical similarity functions are projected to sentence level using a new method called Polarized Soft Cardinality that supports negative similarities between words to model opposites.
method_label: We also introduce a novel L 2 -norm “cardinality” for vector space representations.
method_label: The system extracts a set of 660 features from each pair of text snippets using the proposed cardinality measures.
method_label: From this set, a subset of 12 features was selected in a supervised manner.
method_label: These features are combined by SVR and, alternatively, by using the arithmetic mean to produce similarity predictions.
result_label: Our team ranked second in the crosslingual sub-task and got close to the best official results in the monolingual sub-task.

===================================
paper_id: 18077631; YEAR: 1990
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator3: 0
sources: cited - title_cbow200 - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Similarity involving attributes and relations : Judgments of similarity and difference are not inverses
ABSTRACT: background_label: Conventional wisdom and previous research suggest that similarity judgments and difference judgments are inverses of one another.
background_label: An exception to this rule arises when both relational similarity and attributional similarity are considered.
background_label: When presented with choices that are relationally or attributionally similar to a standard, human subjects tend to pick the relationally similar choice as more similar to the standard and as more different from the standard.
result_label: These results not only reinforce the general distinction between attributes and relations but also show that attributes and relations are dynamically distinct in the processes that give rise to similarity and difference judgments.

===================================
paper_id: 1189582; YEAR: 2012
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200 - specter
TITLE: Roget's Thesaurus and Semantic Similarity
ABSTRACT: background_label: We have implemented a system that measures semantic similarity using a computerized 1987 Roget's Thesaurus, and evaluated it by performing a few typical tests.
method_label: We compare the results of these tests with those produced by WordNet-based similarity measures.
method_label: One of the benchmarks is Miller and Charles' list of 30 noun pairs to which human judges had assigned similarity measures.
method_label: We correlate these measures with those computed by several NLP systems.
method_label: The 30 pairs can be traced back to Rubenstein and Goodenough's 65 pairs, which we have also studied.
result_label: Our Roget's-based system gets correlations of .878 for the smaller and .818 for the larger list of noun pairs; this is quite close to the .885 that Resnik obtained when he employed humans to replicate the Miller and Charles experiment.
result_label: We further evaluate our measure by using Roget's and WordNet to answer 80 TOEFL, 50 ESL and 300 Reader's Digest questions: the correct synonym must be selected amongst a group of four words.
result_label: Our system gets 78.75%, 82.00% and 74.33% of the questions respectively.

===================================
paper_id: 2522663; YEAR: 2008
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator3: 0
sources: title_tfidf - title_cbow200
TITLE: Identifying Word Relations in Software: A Comparative Study of Semantic Similarity Tools
ABSTRACT: background_label: Modern software systems are typically large and complex, making comprehension of these systems extremely difficult.
background_label: Experienced programmers comprehend code by seamlessly processing synonyms and other word relations.
background_label: Thus, we believe that automated comprehension and software tools can be significantly improved by leveraging word relations in software.
objective_label: In this paper, we perform a comparative study of six state of the art, English-based semantic similarity techniques and evaluate their effectiveness on words from the comments and identifiers in software.
result_label: Our results suggest that applying English-based semantic similarity techniques to software without any customization could be detrimental to the performance of the client software tools.
result_label: We propose strategies to customize the existing semantic similarity techniques to software, and describe how various program comprehension tools can benefit from word relation information.

===================================
paper_id: 15908805; YEAR: 2016
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator3: 0
sources: specter - abs_tfidf
TITLE: A Proposal for Linguistic Similarity Datasets Based on Commonality Lists
ABSTRACT: background_label: Similarity is a core notion that is used in psychology and two branches of linguistics: theoretical and computational.
background_label: The similarity datasets that come from the two fields differ in design: psychological datasets are focused around a certain topic such as fruit names, while linguistic datasets contain words from various categories.
background_label: The later makes humans assign low similarity scores to the words that have nothing in common and to the words that have contrast in meaning, making similarity scores ambiguous.
method_label: In this work we discuss the similarity collection procedure for a multi-category dataset that avoids score ambiguity and suggest changes to the evaluation procedure to reflect the insights of psychological literature for word, phrase and sentence similarity.
method_label: We suggest to ask humans to provide a list of commonalities and differences instead of numerical similarity scores and employ the structure of human judgements beyond pairwise similarity for model evaluation.
result_label: We believe that the proposed approach will give rise to datasets that test meaning representation models more thoroughly with respect to the human treatment of similarity.

===================================
paper_id: 2103785; YEAR: 2017
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: ClaC: Semantic Relatedness of Words and Phrases
ABSTRACT: background_label: The measurement of phrasal semantic relatedness is an important metric for many natural language processing applications.
method_label: In this paper, we present three approaches for measuring phrasal semantics, one based on a semantic network model, another on a distributional similarity model, and a hybrid between the two.
result_label: Our hybrid approach achieved an F-measure of 77.4% on the task of evaluating the semantic similarity of words and compositional phrases.

===================================
paper_id: 24226646; YEAR: 2017
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator3: 0
sources: abs_tfidfcbow200 - specter
TITLE: Novel Ranking-Based Lexical Similarity Measure for Word Embedding
ABSTRACT: background_label: Distributional semantics models derive word space from linguistic items in context.
background_label: Meaning is obtained by defining a distance measure between vectors corresponding to lexical entities.
background_label: Such vectors present several problems.
objective_label: In this paper we provide a guideline for post process improvements to the baseline vectors.
method_label: We focus on refining the similarity aspect, address imperfections of the model by applying the hubness reduction method, implementing relational knowledge into the model, and providing a new ranking similarity definition that give maximum weight to the top 1 component value.
method_label: This feature ranking is similar to the one used in information retrieval.
result_label: All these enrichments outperform current literature results for joint ESL and TOEF sets comparison.
result_label: Since single word embedding is a basic element of any semantic task one can expect a significant improvement of results for these tasks.
result_label: Moreover, our improved method of text processing can be translated to continuous distributed representation of biological sequences for deep proteomics and genomics.

===================================
paper_id: 144082511; YEAR: 1973
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200 - title_tfidf
TITLE: Semantic Distance and the Verification of Semantic Relations.
ABSTRACT: background_label: Four experiments dealt with the verification of semantic relations.
background_label: In Experiment I, subjects decided whether an instance was a member of a specified category.
background_label: For some categories (for example, birds) verification was faster when the target category was a direct superordinate (bird) than a higher level superordinate (animal), while for another category (mammal) this finding reversed.
method_label: Experiment II obtained ratings of semantic distance that accounted for the previously obtained verification results.
method_label: Multidimensional scaling of the ratings suggested that semantic distance could be represented as Euclidean distance in a semantic space.
result_label: Experiments III and IV indicated that semantic distance could predict RTs in another categorization task and choices in an analogies task.
result_label: These results place constraints on a theory of semantic memory.

===================================
paper_id: 1653033; YEAR: 2015
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: Learning Lexical Embeddings with Syntactic and Lexicographic Knowledge
ABSTRACT: background_label: We propose two improvements on lexical association used in embedding learning: factorizing individual dependency relations and using lexicographic knowledge from monolingual dictionaries.
background_label: Both proposals provide low-entropy lexical cooccurrence information, and are empirically shown to improve embedding learning by performing notably better than several popular embedding models in similarity tasks.
background_label: 1 Lexical Embeddings and Relatedness Lexical embeddings are essentially real-valued distributed representations of words.
method_label: As a vectorspace model, an embedding model approximates semantic relatedness with the Euclidean distance between embeddings, the result of which helps better estimate the real lexical distribution in various NLP tasks.
method_label: In recent years, researchers have developed efficient and effective algorithms for learning embeddings (Mikolov et al., 2013a; Pennington et al., 2014) and extended model applications from language modelling to various areas in NLP including lexical semantics (Mikolov et al., 2013b) and parsing (Bansal et al., 2014).
method_label: To approximate semantic relatedness with geometric distance, objective functions are usually chosen to correlate positively with the Euclidean similarity between the embeddings of related words.
method_label: Maximizing such an objective function is then equivalent to adjusting the embeddings so that those of the related words will be geometrically closer.
result_label: The definition of relatedness among words can have a profound influence on the quality of the resulting embedding models.
background_label: In most existing studies, relatedness is defined by co-occurrence within a window frame sliding over texts.
background_label: Although supported by the distributional hypothesis (Harris, 1954), this definition suffers from two major limitations.
background_label: Firstly, the window frame size is usually rather small (for efficiency and sparsity considerations), which increases the false negative rate by missing long-distance dependencies.
background_label: Secondly, a window frame can (and often does) span across different constituents in a sentence, resulting in an increased false positive rate by associating unrelated words.
background_label: The problem is worsened as the size of the window increases since each false-positive n-gram will appear in two subsuming false-positive (n+1)-grams.
method_label: Several existing studies have addressed these limitations of window-based contexts.
objective_label: Nonetheless, we hypothesize that lexical embedding learning can further benefit from (1) factorizing syntactic relations into individual relations for structured syntactic information and (2) defining relatedness using lexicographic knowledge.
result_label: We will show that implementation of these ideas brings notable improvement in lexical similarity tasks.

===================================
paper_id: 20101616; YEAR: 2007
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200 - abs_cbow200 - abs_tfidf - title_tfidf - specter
TITLE: Measuring Semantic Similarity in Wordnet
ABSTRACT: background_label: Semantic similarity between words is a generic problem for many applications of computational linguistics and artificial intelligence.
background_label: The difficulty of this task lies in how to find an effective way to simulate the process of human judgment of word similarity by combining and processing a number of information sources.
objective_label: This paper presents a novel model to measure semantic similarity between words in the WordNet, using edge-counting techniques.
method_label: The fundamental idea of this model is based on the assumption that human judgment process for semantic similarity can be simulated by the ratio of common features to the total features between words.
method_label: According to the experiment against a benchmark set by human similarity judgment, our measure achieves a better result.
result_label: The correlation is 0.926 with average human judgment on a standard 28 word-pair dataset, which outperforms other previous reported methods.

===================================
paper_id: 5370625; YEAR: 2006
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200 - specter
TITLE: Similarity of Objects and the Meaning of Words
ABSTRACT: background_label: We survey the emerging area of compression-based, parameter-free, similarity distance measures useful in data-mining, pattern recognition, learning and automatic semantics extraction.
background_label: Given a family of distances on a set of objects, a distance is universal up to a certain precision for that family if it minorizes every distance in the family between every two objects in the set, up to the stated precision (we do not require the universal distance to be an element of the family).
method_label: We consider similarity distances for two types of objects: literal objects that as such contain all of their meaning, like genomes or books, and names for objects.
method_label: The latter may have literal embodyments like the first type, but may also be abstract like ``red'' or ``christianity.''
method_label: For the first type we consider a family of computable distance measures corresponding to parameters expressing similarity according to particular featuresdistances generated by web users corresponding to particular semantic relations between the (names for) the designated objects.
method_label: For both families we give universal similarity distance measures, incorporating all particular distance measures in the family.
method_label: In the first case the universal distance is based on compression and in the second case it is based on Google page counts related to search terms.
result_label: In both cases experiments on a massive scale give evidence of the viability of the approaches.
result_label: between pairs of literal objects.
result_label: For the second type we consider similarity

===================================
paper_id: 1003611; YEAR: 2009
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator3: 2
sources: specter - title_cbow200 - abs_tfidf
TITLE: A Relational Model of Semantic Similarity between Words using Automatically Extracted Lexical Pattern Clusters from the Web
ABSTRACT: background_label: Semantic similarity is a central concept that extends across numerous fields such as artificial intelligence, natural language processing, cognitive science and psychology.
background_label: Accurate measurement of semantic similarity between words is essential for various tasks such as, document clustering, information retrieval, and synonym extraction.
objective_label: We propose a novel model of semantic similarity using the semantic relations that exist among words.
method_label: Given two words, first, we represent the semantic relations that hold between those words using automatically extracted lexical pattern clusters.
method_label: Next, the semantic similarity between the two words is computed using a Mahalanobis distance measure.
method_label: We compare the proposed similarity measure against previously proposed semantic similarity measures on Miller-Charles benchmark dataset and WordSimilarity-353 collection.
result_label: The proposed method outperforms all existing web-based semantic similarity measures, achieving a Pearson correlation coefficient of 0.867 on the Millet-Charles dataset.

===================================
paper_id: 16470894; YEAR: 2013
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator3: 0
sources: title_tfidf - title_cbow200 - title_tfidfcbow200 - abs_tfidf - specter
TITLE: Cross-Lingual Semantic Similarity of Words as the Similarity of Their Semantic Word Responses
ABSTRACT: objective_label: AbstractWe propose a new approach to identifying semantically similar words across languages.
objective_label: The approach is based on an idea that two words in different languages are similar if they are likely to generate similar words (which includes both source and target language words) as their top semantic word responses.
method_label: Semantic word responding is a concept from cognitive science which addresses detecting most likely words that humans output as free word associations given some cue word.
method_label: The method consists of two main steps: (1) it utilizes a probabilistic multilingual topic model trained on comparable data to learn and quantify the semantic word responses, (2) it provides ranked lists of similar words according to the similarity of their semantic word response vectors.
result_label: We evaluate our approach in the task of bilingual lexicon extraction (BLE) for a variety of language pairs.
result_label: We show that in the cross-lingual settings without any language pair dependent knowledge the response-based method of similarity is more robust and outperforms current state-of-the art methods that directly operate in the semantic space of latent cross-lingual concepts/topics.

===================================
paper_id: 29074; YEAR: 2003
adju relevance: Related (+1)
difference: 0; annotator1: 1; annotator3: 1
sources: cited - title_cbow200 - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Combining Independent Modules to Solve Multiple-choice Synonym and Analogy Problems
ABSTRACT: background_label: Existing statistical approaches to natural language problems are very coarse approximations to the true complexity of language processing.
background_label: As such, no single technique will be best for all problem instances.
background_label: Many researchers are examining ensemble methods that combine the output of successful, separately developed modules to create more accurate solutions.
objective_label: This paper examines three merging rules for combining probability distributions: the well known mixture rule, the logarithmic rule, and a novel product rule.
method_label: These rules were applied with state-of-the-art results to two problems commonly used to assess human mastery of lexical semantics -- synonym questions and analogy questions.
result_label: All three merging rules result in ensembles that are more accurate than any of their component modules.
result_label: The differences among the three rules are not statistically significant, but it is suggestive that the popular mixture rule is not the best rule for either of the two problems.

===================================
paper_id: 820832; YEAR: 2008
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator3: 0
sources: specter - abs_tfidf - abs_tfidfcbow200
TITLE: Using Web-Search Results to Measure Word-Group Similarity
ABSTRACT: background_label: Semantic relatedness between words is important to many NLP tasks, and numerous measures exist which use a variety of resources.
background_label: Thus far, such work is confined to measuring similarity between two words (or two texts), and only a handful utilize the web as a corpus.
objective_label: This paper introduces a distributional similarity measure which uses internet search counts and also extends to calculating the similarity within word-groups.
result_label: The evaluation results are encouraging: for word-pairs, the correlations with human judgments are comparable with state-of-the-art web-search page-count heuristics.
result_label: When used to measure similarities within sets of 10 words, the results correlate highly (up to 0.8) with those expected.
result_label: Relatively little comparison has been made between the results of different search-engines.
result_label: Here, we compare experimental results from Google, Windows Live Search and Yahoo and find noticeable differences.

===================================
paper_id: 958931; YEAR: 2009
adju relevance: Related (+1)
difference: 0; annotator1: 1; annotator3: 1
sources: title_tfidf - title_cbow200 - title_tfidfcbow200 - abs_tfidf - specter
TITLE: Using Lexical and Relational Similarity to Classify Semantic Relations
ABSTRACT: background_label: Many methods are available for computing semantic similarity between individual words, but certain NLP tasks require the comparison of word pairs.
objective_label: This paper presents a kernel-based framework for application to relational reasoning tasks of this kind.
method_label: The model presented here combines information about two distinct types of word pair similarity: lexical similarity and relational similarity.
result_label: We present an efficient and flexible technique for implementing relational similarity and show the effectiveness of combining lexical and relational models by demonstrating state-of-the-art results on a compound noun interpretation task.

===================================
paper_id: 14435672; YEAR: 2008
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator3: 0
sources: title_tfidf - title_cbow200 - abs_cbow200 - title_tfidfcbow200 - specter
TITLE: Comparing measures of semantic similarity
ABSTRACT: objective_label: The aim of this paper is to compare different methods for automatic extraction of semantic similarity measures from corpora.
background_label: The semantic similarity measure is proven to be very useful for many tasks in natural language processing like information retrieval, information extraction, machine translation etc.
background_label: Additionally, one of the main problems in natural language processing is data sparseness since no language sample is large enough to seize all possible language combinations.
method_label: In our research we experiment with four different measures of association with context and eight different measures of vector similarity.
result_label: The results show that the Jensen-Shannon divergence and L1 and L2 norm outperform other measures of vector similarity regardless of the measure of association with context used.
result_label: Maximum likelihood estimate and t-test show better results than other measures of association with context.

===================================
paper_id: 9549307; YEAR: 2014
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator3: 0
sources: specter - abs_tfidf
TITLE: Learning Word Representations from Relational Graphs
ABSTRACT: background_label: Attributes of words and relations between two words are central to numerous tasks in Artificial Intelligence such as knowledge representation, similarity measurement, and analogy detection.
background_label: Often when two words share one or more attributes in common, they are connected by some semantic relations.
background_label: On the other hand, if there are numerous semantic relations between two words, we can expect some of the attributes of one of the words to be inherited by the other.
method_label: Motivated by this close connection between attributes and relations, given a relational graph in which words are inter- connected via numerous semantic relations, we propose a method to learn a latent representation for the individual words.
method_label: The proposed method considers not only the co-occurrences of words as done by existing approaches for word representation learning, but also the semantic relations in which two words co-occur.
method_label: To evaluate the accuracy of the word representations learnt using the proposed method, we use the learnt word representations to solve semantic word analogy problems.
result_label: Our experimental results show that it is possible to learn better word representations by using semantic semantics between words.

===================================
paper_id: 8410942; YEAR: 2011
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator3: 0
sources: title_tfidf - title_cbow200 - abs_tfidf - specter
TITLE: Comparison of the Baseline Knowledge-, Corpus-, and Web-based Similarity Measures for Semantic Relations Extraction
ABSTRACT: background_label: AbstractUnsupervised methods of semantic relations extraction rely on a similarity measure between lexical units.
background_label: Similarity measures differ both in kinds of information they use and in the ways how this information is transformed into a similarity score.
objective_label: This paper is making a step further in the evaluation of the available similarity measures within the context of semantic relation extraction.
method_label: We compare 21 baseline measures -8 knowledge-based, 4 corpus-based, and 9 web-based metrics with the BLESS dataset.
result_label: Our results show that existing similarity measures provide significantly different results, both in general performances and in relation distributions.
result_label: We conclude that the results suggest developing a combined similarity measure.

===================================
paper_id: 14469599; YEAR: 2014
adju relevance: Related (+1)
difference: 0; annotator1: 1; annotator3: 1
sources: abs_tfidfcbow200 - abs_tfidf - specter
TITLE: Experiments with Three Approaches to Recognizing Lexical Entailment
ABSTRACT: background_label: Inference in natural language often involves recognizing lexical entailment (RLE); that is, identifying whether one word entails another.
background_label: For example,"buy"entails"own".
objective_label: Two general strategies for RLE have been proposed: One strategy is to manually construct an asymmetric similarity measure for context vectors (directional similarity) and another is to treat RLE as a problem of learning to recognize semantic relations using supervised machine learning techniques (relation classification).
method_label: In this paper, we experiment with two recent state-of-the-art representatives of the two general strategies.
method_label: The first approach is an asymmetric similarity measure (an instance of the directional similarity strategy), designed to capture the degree to which the contexts of a word, a, form a subset of the contexts of another word, b.
method_label: The second approach (an instance of the relation classification strategy) represents a word pair, a:b, with a feature vector that is the concatenation of the context vectors of a and b, and then applies supervised learning to a training set of labeled feature vectors.
background_label: Additionally, we introduce a third approach that is a new instance of the relation classification strategy.
method_label: The third approach represents a word pair, a:b, with a feature vector in which the features are the differences in the similarities of a and b to a set of reference words.
method_label: All three approaches use vector space models (VSMs) of semantics, based on word-context matrices.
method_label: We perform an extensive evaluation of the three approaches using three different datasets.
result_label: The proposed new approach (similarity differences) performs significantly better than the other two approaches on some datasets and there is no dataset for which it is significantly worse.
result_label: Our results suggest it is beneficial to make connections between the research in lexical entailment and the research in semantic relation classification.

===================================
paper_id: 8777199; YEAR: 2000
adju relevance: Related (+1)
difference: 1; annotator1: 1; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200 - title_tfidf
TITLE: Categories of Relations and Functional Relations
ABSTRACT: background_label: We define relations and their composition in a category with (E, M)-factorization structure, with M consisting of monomorphisms, but E not restricted to epimorphisms.
method_label: We obtain an associativity criterion for composition of relations, and we study functional and induced relations.
result_label: We show that under our assumptions, the categories of relations on functional and induced relations are isomorphic to the category of relations for the given category.

===================================
paper_id: 11272770; YEAR: 2010
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: abs_cbow200 - abs_tfidf - abs_tfidfcbow200 - specter
TITLE: Fast single-pair simrank computation
ABSTRACT: background_label: AbstractSimRank is an intuitive and effective measure for link-based similarity that scores similarity between two nodes as the first-meeting probability of two random surfers, based on the random surfer model.
background_label: However, when a user queries the similarity of a given node-pair based on SimRank, the existing approaches need to compute the similarities of other node-pairs beforehand, which we call an all-pair style.
objective_label: In this paper, we propose a Single-Pair SimRank approach.
method_label: Without accuracy loss, this approach performs an iterative computation to obtain the similarity of a single node-pair.
method_label: The time cost of our Single-Pair SimRank is always less than All-Pair SimRank and obviously efficient when we only need to assess similarity of one or a few node-pairs.
result_label: We confirm the accuracy and efficiency of our approach in extensive experimental studies over synthetic and real datasets.
background_label: IntroductionThe measure of similarity between objects plays a significant role in many real-world applications, including clustering, classification, information retrieval, and recommendation systems.
background_label: The current similarity measures can be generalized into two broad categories: content-based similarity measures and link-based similarity measures [14] .
background_label: The former is based on the content using a vector space model [16] , and the latter is based on a link graph in which objects and relationships are modeled as nodes and edges.
method_label: Examples of link graphs include citations between papers, social relationships in human or web hyperlinks.
result_label: Effective and efficient similarity measures between objects in a link graph can greatly assist users for searching and analyzing information [20, 9, 19, 6, 5, 13] , in particular when the relationships among objects are complex.Among the link-based similarity measures in the literature, SimRank [9] has attracted a considerable attention due to its intuition and sturdy theoretical foundation.
result_label: The basic intuition behind SimRank is "two objects are similar if they are referenced by similar

===================================
paper_id: 11279854; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: abs_tfidfcbow200 - abs_tfidf
TITLE: Efficient search algorithm for SimRank
ABSTRACT: background_label: Graphs are a fundamental data structure and have been employed to model objects as well as their relationships.
background_label: The similarity of objects on the web (e.g., webpages, photos, music, micro-blogs, and social networking service users) is the key to identifying relevant objects in many recent applications.
background_label: SimRank, proposed by Jeh and Widom, provides a good similarity score and has been successfully used in many applications such as web spam detection, collaborative tagging analysis, link prediction, and so on.
background_label: SimRank computes similarities iteratively, and it needs O(N4T) time and O(N2) space for similarity computation where N and T are the number of nodes and iterations, respectively.
background_label: Unfortunately, this iterative approach is computationally expensive.
objective_label: The goal of this work is to process top-k search and range search efficiently for a given node.
method_label: Our solution, SimMat, is based on two ideas: (1) It computes the approximate similarity of a selected node pair efficiently in non-iterative style based on the Sylvester equation, and (2) It prunes unnecessary approximate similarity computations when searching for the high similarity nodes by exploiting estimations based on the Cauchy-Schwarz inequality.
method_label: These two ideas reduce the time and space complexities of the proposed approach to O(Nn) where n is the target rank of the low-rank approximation (n ≪ N in practice).
result_label: Our experiments show that our approach is much faster, by several orders of magnitude, than previous approaches in finding the high similarity nodes.

===================================
paper_id: 982153; YEAR: 2005
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: title_tfidf - title_cbow200 - title_tfidfcbow200
TITLE: A Semantic Similarity Measure for Semantic Web Services
ABSTRACT: background_label: Establishing the compatibility of services is an essential prerequisite to service composition.
background_label: By formally defining the similarity of semantic services, useful information can be obtained about their compatibility.
objective_label: In this paper we propose a metric for measuring the similarity of semantic services annotated with OWL ontology.
method_label: Similarity is calculated by defining the intrinsic information value of a service description based on the “inferencibility” of each of OWL Lite constructs.
method_label: We apply this technique to OWL-S, an emerging standard for defining semantic service metadata and demonstrate how to measure the similarity of OWL-S annotated services.

===================================
paper_id: 1235293; YEAR: 2005
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200 - abs_tfidf - title_tfidf
TITLE: Semantic Text Similarity Using Corpus-Based Word Similarity and String Similarity
ABSTRACT: method_label: We present a method for measuring the semantic similarity of texts using a corpus-based measure of semantic word similarity and a normalized and modified version of the Longest Common Subsequence (LCS) string matching algorithm.
method_label: Existing methods for computing text similarity have focused mainly on either large documents or individual words.
method_label: We focus on computing the similarity between two sentences or two short paragraphs.
method_label: The proposed method can be exploited in a variety of applications involving textual knowledge representation and knowledge discovery.
result_label: Evaluation results on two different data sets show that our method outperforms several competing methods.

===================================
paper_id: 10507844; YEAR: 2005
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: specter - abs_cbow200 - abs_tfidf - abs_tfidfcbow200
TITLE: Co-Occurrence Retrieval: A Flexible Framework For Lexical Distributional Similarity
ABSTRACT: background_label: Techniques that exploit knowledge of distributional similarity between words have been proposed in many areas of Natural Language Processing.
background_label: For example, in language modeling, the sparse data problem can be alleviated by estimating the probabilities of unseen co-occurrences of events from the probabilities of seen co-occurrences of similar events.
background_label: In other applications, distributional similarity is taken to be an approximation to semantic similarity.
method_label: However, due to the wide range of potential applications and the lack of a strict definition of the concept of distributional similarity, many methods of calculating distributional similarity have been proposed or adopted.
method_label: In this work, a flexible, parameterized framework for calculating distributional similarity is proposed.
method_label: Within this framework, the problem of finding distributionally similar words is cast as one of co-occurrence retrieval (CR) for which precision and recall can be measured by analogy with the way they are measured in document retrieval.
result_label: As will be shown, a number of popular existing measures of distributional similarity are simulated with parameter settings within the CR framework.
background_label: In this article, the CR framework is then used to systematically investigate three fundamental questions concerning distributional similarity.
background_label: First, is the relationship of lexical similarity necessarily symmetric, or are there advantages to be gained from considering it as an asymmetric relationship?
background_label: Second, are some co-occurrences inherently more salient than others in the calculation of distributional similarity?
method_label: Third, is it necessary to consider the difference in the extent to which each word occurs in each co-occurrence type?
method_label: Two application-based tasks are used for evaluation: automatic thesaurus generation and pseudo-disambiguation.
result_label: It is possible to achieve significantly better results on both these tasks by varying the parameters within the CR framework rather than using other existing distributional similarity measures; it will also be shown that any single unparameterized measure is unlikely to be able to do better on both tasks.
result_label: This is due to an inherent asymmetry in lexical substitutability and therefore also in lexical distributional similarity.

===================================
paper_id: 9426034; YEAR: 2004
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: cited - title_cbow200 - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: The Web as a Baseline: Evaluating the Performance of Unsupervised Web-based Models for a Range of NLP Tasks
ABSTRACT: background_label: AbstractPrevious work demonstrated that web counts can be used to approximate bigram frequencies, and thus should be useful for a wide variety of NLP tasks.
background_label: So far, only two generation tasks (candidate selection for machine translation and confusion-set disambiguation) have been tested using web-scale data sets.
objective_label: The present paper investigates if these results generalize to tasks covering both syntax and semantics, both generation and analysis, and a larger range of n-grams.
result_label: For the majority of tasks, we find that simple, unsupervised models perform better when n-gram frequencies are obtained from the web rather than from a large corpus.
result_label: However, in most cases, web-based models fail to outperform more sophisticated state-of-theart models trained on small corpora.
result_label: We argue that web-based models should therefore be used as a baseline for, rather than an alternative to, standard models.

===================================
paper_id: 13220010; YEAR: 2006
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200 - title_tfidf
TITLE: Measures of semantic similarity and relatedness in the biomedical domain
ABSTRACT: background_label: Measures of semantic similarity between concepts are widely used in Natural Language Processing.
background_label: In this article, we show how six existing domain-independent measures can be adapted to the biomedical domain.
background_label: These measures were originally based on WordNet, an English lexical database of concepts and relations.
method_label: In this research, we adapt these measures to the SNOMED-CT ontology of medical concepts.
method_label: The measures include two path-based measures, and three measures that augment path-based measures with information content statistics from corpora.
method_label: We also derive a context vector measure based on medical corpora that can be used as a measure of semantic relatedness.
method_label: These six measures are evaluated against a newly created test bed of 30 medical concept pairs scored by three physicians and nine medical coders.
result_label: We find that the medical coders and physicians differ in their ratings, and that the context vector measure correlates most closely with the physicians, while the path-based measures and one of the information content measures correlates most closely with the medical coders.
result_label: We conclude that there is a role both for more flexible measures of relatedness based on information derived from corpora, as well as for measures that rely on existing ontological structures.

===================================
paper_id: 15190020; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: abs_cbow200
TITLE: Semantic Composition and Decomposition: From Recognition to Generation
ABSTRACT: background_label: Semantic composition is the task of understanding the meaning of text by composing the meanings of the individual words in the text.
background_label: Semantic decomposition is the task of understanding the meaning of an individual word by decomposing it into various aspects (factors, constituents, components) that are latent in the meaning of the word.
background_label: We take a distributional approach to semantics, in which a word is represented by a context vector.
background_label: Much recent work has considered the problem of recognizing compositions and decompositions, but we tackle the more difficult generation problem.
method_label: For simplicity, we focus on noun-modifier bigrams and noun unigrams.
method_label: A test for semantic composition is, given context vectors for the noun and modifier in a noun-modifier bigram ("red salmon"), generate a noun unigram that is synonymous with the given bigram ("sockeye").
background_label: A test for semantic decomposition is, given a context vector for a noun unigram ("snifter"), generate a noun-modifier bigram that is synonymous with the given unigram ("brandy glass").
background_label: With a vocabulary of about 73,000 unigrams from WordNet, there are 73,000 candidate unigram compositions for a bigram and 5,300,000,000 (73,000 squared) candidate bigram decompositions for a unigram.
method_label: We generate ranked lists of potential solutions in two passes.
method_label: A fast unsupervised learning algorithm generates an initial list of candidates and then a slower supervised learning algorithm refines the list.
result_label: We evaluate the candidate solutions by comparing them to WordNet synonym sets.
result_label: For decomposition (unigram to bigram), the top 100 most highly ranked bigrams include a WordNet synonym of the given unigram 50.7% of the time.
result_label: For composition (bigram to unigram), the top 100 most highly ranked unigrams include a WordNet synonym of the given bigram 77.8% of the time.

===================================
paper_id: 8814739; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: title_tfidf - title_tfidfcbow200
TITLE: Context-based image semantic similarity
ABSTRACT: objective_label: In this work we propose Context-based Image Similarity, a scheme for discovering and evaluating image similarity in terms of the associated groups of concepts.
method_label: Several semantic proximity/similarity among image concepts and different concept ontology - WordNet Distance, Wikipedia Distance, Flickr Distance, Confidence, Normalized Google Distance (NGD), Pointwise Mutual Information (PMI) and PMING, have been considered as elementary metrics for the context.
method_label: Comparing to Content Based Image Retrieval (CBIR), which measures the image content similarities by low level features, the proposed Context-based Image Similarity outperformed CBIR in measuring the deep concept similarity and relationship of images.
result_label: Experimental results, obtained in the domain of images semantic similarity using search engine based tag similarity, show the adequacy of the proposed approach in order to reflect the collective notion of semantic similarity.

===================================
paper_id: 159041087; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200 - abs_tfidf - title_tfidf - specter
TITLE: Correlation Coefficients and Semantic Textual Similarity
ABSTRACT: background_label: A large body of research into semantic textual similarity has focused on constructing state-of-the-art embeddings using sophisticated modelling, careful choice of learning signals and many clever tricks.
background_label: By contrast, little attention has been devoted to similarity measures between these embeddings, with cosine similarity being used unquestionably in the majority of cases.
method_label: In this work, we illustrate that for all common word vectors, cosine similarity is essentially equivalent to the Pearson correlation coefficient, which provides some justification for its use.
method_label: We thoroughly characterise cases where Pearson correlation (and thus cosine similarity) is unfit as similarity measure.
method_label: Importantly, we show that Pearson correlation is appropriate for some word vectors but not others.
method_label: When it is not appropriate, we illustrate how common non-parametric rank correlation coefficients can be used instead to significantly improve performance.
result_label: We support our analysis with a series of evaluations on word-level and sentence-level semantic textual similarity benchmarks.
result_label: On the latter, we show that even the simplest averaged word vectors compared by rank correlation easily rival the strongest deep representations compared by cosine similarity.

===================================
paper_id: 9075575; YEAR: 2007
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: abs_tfidfcbow200
TITLE: ProCKSI: a decision support system for Protein (Structure) Comparison, Knowledge, Similarity and Information
ABSTRACT: background_label: We introduce the decision support system for Protein (Structure) Comparison, Knowledge, Similarity and Information (ProCKSI).
background_label: ProCKSI integrates various protein similarity measures through an easy to use interface that allows the comparison of multiple proteins simultaneously.
method_label: It employs the Universal Similarity Metric (USM), the Maximum Contact Map Overlap (MaxCMO) of protein structures and other external methods such as the DaliLite and the TM-align methods, the Combinatorial Extension (CE) of the optimal path, and the FAST Align and Search Tool (FAST).
method_label: Additionally, ProCKSI allows the user to upload a user-defined similarity matrix supplementing the methods mentioned, and computes a similarity consensus in order to provide a rich, integrated, multicriteria view of large datasets of protein structures.
method_label: We present ProCKSI's architecture and workflow describing its intuitive user interface, and show its potential on three distinct test-cases.
result_label: In the first case, ProCKSI is used to evaluate the results of a previous CASP competition, assessing the similarity of proposed models for given targets where the structures could have a large deviation from one another.
result_label: To perform this type of comparison reliably, we introduce a new consensus method.
background_label: The second study deals with the verification of a classification scheme for protein kinases, originally derived by sequence comparison by Hanks and Hunter, but here we use a consensus similarity measure based on structures.
method_label: In the third experiment using the Rost and Sander dataset (RS126), we investigate how a combination of different sets of similarity measures influences the quality and performance of ProCKSI's new consensus measure.
method_label: ProCKSI performs well with all three datasets, showing its potential for complex, simultaneous multi-method assessment of structural similarity in large protein datasets.
method_label: Furthermore, combining different similarity measures is usually more robust than relying on one single, unique measure.
method_label: Based on a diverse set of similarity measures, ProCKSI computes a consensus similarity profile for the entire protein set.
result_label: All results can be clustered, visualised, analysed and easily compared with each other through a simple and intuitive interface.
result_label: ProCKSI is publicly available at http://www.procksi.net for academic and non-commercial use.

===================================
paper_id: 118703188; YEAR: 1997
adju relevance: Irrelevant (0)
difference: 2; annotator1: 2; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: Latent Semantic Analysis Approaches to Categorization
ABSTRACT: background_label: Many computational models of semantic memory rely on vector representations of concepts based on explicit encoding of arbitrary feature sets.
background_label: Latent Semantic Analysis (LSA) creates high dimensional (n = 300+) vectors for concepts in semantic memory through statistical analysis of a large representative corpus of text rather than subjective feature sets linked to object names (for details see Landauer & Dumais, 1997; Landauer, Foltz, & Laham, in press).
background_label: Concepts can be compared in the semantic space and their similarity indexed by the cosine of the angle between vectors.
method_label: Computational models of concept relations using LSA representations demonstrate that categories can be emergent and self-organizing based exclusively on the way language is used in the corpus without explicit hand-coding of category membership or semantic features.
method_label: LSA categorization is context dependent and occurs through a dynamic process of induction.
method_label: Semantic “meaning” is not encapsulated within an object representation, but emerges as the set of relationships between selected objects in a context-based sub-space.
other_label: Neuropsychological studies (e.g.
background_label: Warrington & Shallice, 1984) point to a class of patients who exhibit disnomias for specific categories of objects (natural kinds) while retaining the ability to name other objects (man-made artifacts).
result_label: The objects from natural kind categories tend to be significantly more clustered in LSA space than are those from artifact categories.
result_label: If brain structure corresponds to LSA structure, the identification of concepts belonging to strongly clustered categories should suffer more than weakly clustered concepts when their representations are partially damaged.
background_label: Three types of modeling experiments were conducted: matching base concept names to superordinate categories in forced-choice testing, correlating LSA similarity measures to human judgments of typicality, and multivariate analyses of similarity matrices to capture category boundaries.
method_label: For the forced-choice matching of concept names to superordinate categories, a selection of 140 objects (rated as most typical in their category) from 14 categories was used.
method_label: Each object name was compared to each of the 14 category names (apple—flower, apple—mammal, etc.).
method_label: The LSA match was considered correct when the highest cosine comparison in the set was between an object and its relevant superordinate (apple—fruit).
result_label: The results show that in all 14 categories, LSA predicts membership well above chance (chance = 7%), however, there are differences in the degree of clustering: the percent correct for animate natural kinds (flowers, mammals, fruit, trees, vegetables, and birds) = 92%; for inanimate natural kinds with observed deficits in neuropsychological patients (gemstones, musical instruments) = 100%; and for man-made artifacts (furniture, vehicles, weapons, tools, toys, and clothing) = 53%.
result_label: Correlations between LSA similarity judgments and human typicality judgments were consistently better for the natural kinds than for the artifacts.
result_label: For natural categories, LSA similarities (cosine between concept and either superordinate name, most typical member, or centroid of all members) showed high correlations with human judgments (e.g.
result_label: fruit: r = .82), while artifact similarities showed low to near-zero correlations with human judgments.
result_label: As illustrated in Figure 1, multivariate analyses of LSAbased similarity matrices show more cohesive structure for natural kinds than for artifacts.
result_label: Factors 4-6 in this analysis load high on concepts in the bird category—additional factors (7-15) load on specific artifact concepts (not shown).

===================================
paper_id: 10648980; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: cited - title_cbow200 - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Probabilistic Latent Semantic Indexing
ABSTRACT: background_label: Probabilistic Latent Semantic Indexing is a novel approach to automated document indexing which is based on a statistical latent class model for factor analysis of count data.
method_label: Fitted from a training corpus of text documents by a generalization of the Expectation Maximization algorithm, the utilized model is able to deal with domain{specific synonymy as well as with polysemous words.
method_label: In contrast to standard Latent Semantic Indexing (LSI) by Singular Value Decomposition, the probabilistic variant has a solid statistical foundation and defines a proper generative data model.
result_label: Retrieval experiments on a number of test collections indicate substantial performance gains over direct term matching methods as well as over LSI.
result_label: In particular, the combination of models with different dimensionalities has proven to be advantageous.

===================================
paper_id: 9638912; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200 - title_tfidf
TITLE: From Ontology to Semantic Similarity: Calculation of Ontology-Based Semantic Similarity
ABSTRACT: background_label: Advances in high-throughput experimental techniques in the past decade have enabled the explosive increase of omics data, while effective organization, interpretation, and exchange of these data require standard and controlled vocabularies in the domain of biological and biomedical studies.
background_label: Ontologies, as abstract description systems for domain-specific knowledge composition, hence receive more and more attention in computational biology and bioinformatics.
background_label: Particularly, many applications relying on domain ontologies require quantitative measures of relationships between terms in the ontologies, making it indispensable to develop computational methods for the derivation of ontology-based semantic similarity between terms.
method_label: Nevertheless, with a variety of methods available, how to choose a suitable method for a specific application becomes a problem.
method_label: With this understanding, we review a majority of existing methods that rely on ontologies to calculate semantic similarity between terms.
method_label: We classify existing methods into five categories: methods based on semantic distance, methods based on information content, methods based on properties of terms, methods based on ontology hierarchy, and hybrid methods.
method_label: We summarize characteristics of each category, with emphasis on basic notions, advantages and disadvantages of these methods.
method_label: Further, we extend our review to software tools implementing these methods and applications using these methods.

===================================
paper_id: 8065523; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: abs_tfidf
TITLE: Robust Joint Resource Allocation for OFDMA-CDMA Spectrum Refarming System
ABSTRACT: background_label: In this paper, we investigate a spectrum refarming system where an OFDMA system shares the licensed band of a CDMA system.
background_label: Both systems share the same cell site, but each has different base station (BS) antennas.
method_label: Joint resource allocation problem is formulated to optimize the CDMA receive power, OFDMA transmit power, and subcarrier assignment.
method_label: Conventional interference control to protect the primary CDMA system relies on the availability of cross-channel gains (CCGs) from each secondary OFDMA transmitter to the CDMA BS receiver.
result_label: However, the CCGs are difficult to be obtained due to lack of intersystem cooperation.
method_label: To address this problem, we first decouple the original problem into higher and lower-level problems via primal decomposition.
method_label: Then, a robust lower-level resource allocation (R-LRA) scheme, which controls interference without using CCGs is proposed, with which CDMA users can be sufficiently protected.
method_label: Thereafter, an enhanced R-LRA (ER-LRA) scheme is proposed to decrease the conservation of R-LRA scheme.
method_label: Assisted by the CDMA inner power control and based on the ER-LRA, efficient algorithm is designed to solve the higher-level problem.
method_label: Extensions of the ER-LRA for other scenarios are also studied.
result_label: Simulation results are provided to validate the proposed schemes in facilitating and improving the spectrum sharing performance.

===================================
paper_id: 12382167; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200 - specter
TITLE: A Study of Metrics of Distance and Correlation Between Ranked Lists for Compositionality Detection
ABSTRACT: background_label: Compositionality in language refers to how much the meaning of some phrase can be decomposed into the meaning of its constituents and the way these constituents are combined.
background_label: Based on the premise that substitution by synonyms is meaning-preserving, compositionality can be approximated as the semantic similarity between a phrase and a version of that phrase where words have been replaced by their synonyms.
objective_label: Different ways of representing such phrases exist (e.g., vectors [1] or language models [2]), and the choice of representation affects the measurement of semantic similarity.
method_label: We propose a new compositionality detection method that represents phrases as ranked lists of term weights.
method_label: Our method approximates the semantic similarity between two ranked list representations using a range of well-known distance and correlation metrics.
method_label: In contrast to most state-of-the-art approaches in compositionality detection, our method is completely unsupervised.
result_label: Experiments with a publicly available dataset of 1048 human-annotated phrases shows that, compared to strong supervised baselines, our approach provides superior measurement of compositionality using any of the distance and correlation metrics considered.

===================================
paper_id: 53712492; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200 - title_tfidf - specter
TITLE: Not just a matter of semantics: the relationship between visual similarity and semantic similarity
ABSTRACT: background_label: Knowledge transfer, zero-shot learning and semantic image retrieval are methods that aim at improving accuracy by utilizing semantic information, e.g.
background_label: from WordNet.
background_label: It is assumed that this information can augment or replace missing visual data in the form of labeled training images because semantic similarity correlates with visual similarity.
background_label: This assumption may seem trivial, but is crucial for the application of such semantic methods.
background_label: Any violation can cause mispredictions.
background_label: Thus, it is important to examine the visual-semantic relationship for a certain target problem.
method_label: In this paper, we use five different semantic and visual similarity measures each to thoroughly analyze the relationship without relying too much on any single definition.
method_label: We postulate and verify three highly consequential hypotheses on the relationship.
result_label: Our results show that it indeed exists and that WordNet semantic similarity carries more information about visual similarity than just the knowledge of"different classes look different".
result_label: They suggest that classification is not the ideal application for semantic methods and that wrong semantic information is much worse than none.

===================================
paper_id: 196147469; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: title_tfidf - title_cbow200 - title_tfidfcbow200
TITLE: Semantic Similarity Definition
ABSTRACT: background_label: In bioinformatics, semantic similarity has been used to compare different types of biomedical entities, such as proteins, compounds and phenotypes, based on their biological role instead on what they look like.
objective_label: This manuscript presents a definition of semantic similarity between biomedical entities described by a common semantic base (e.g., ontology) following an information-theoretic perspective of semantic similarity.
method_label: It defines the amount of information content two entries share in a semantic base, and, by extension, how to compare biomedical entities represented outside the semantic base but linked through a set of annotations.
other_label: Software to check how semantic similarity works in practice is available at: https://github.com/lasigeBioTM/DiShIn/ .

===================================
paper_id: 11039292; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200 - title_tfidfcbow200 - specter
TITLE: Word similarity using constructions as contextual features
ABSTRACT: background_label: Abstract 1We propose and implement an alternative source of contextual features for word similarity detection based on the notion of lexicogrammatical construction.
background_label: On the assumption that selectional restrictions provide indicators of the semantic similarity of words attested in selected positions, we extend the notion of selection beyond that of single selecting heads to multiword constructions exerting selectional preferences.
method_label: Our model of 92 million cross-indexed hybrid n-grams (serving as our machine-tractable proxy for constructions) extracted from BNC provides the source of contextual features.
method_label: We compare results with those of a grammatical dependency approach (Lin 1998), testing both against WordNetbased similarity rankings (Lin 1998; Resnik 1995) .
result_label: Averaged over the entire set of target nouns and 10-best candidate similar words, Lin's approach gives overall similarity results closer to WordNet rankings than the constructional approach does, while the constructional approach overtakes Lin's in approximating WordNet similarity for target nouns with a frequency over 3000.
result_label: While this suggests feature sparseness for constructions that resolves with higher frequency nouns, constructions as shared contextual features render a much higher yield in similarity performance in approximating WordNet similarity than grammatical relations do.
result_label: We examine some cases in detail showing the sorts of similarity detected by a constructional approach that are undetected by a grammatical relations approach or by WordNet or both and thus overlooked in benchmark evaluations.

===================================
paper_id: 14394781; YEAR: 1995
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: cited - title_cbow200 - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Lexical Chains as Representations of Context for the Detection And Correction of Malapropisms
ABSTRACT: background_label: Natural language utterances are, in general, highlyambiguous, and a unique interpretationcan usuallybe determined only by taking into account the constraining inﬂuence of the context in which theutterance occurred.
background_label: Much of the research in natural language understanding in the last twenty yearscan be thought of as attempts to characterize and represent context and then derive interpretationsthatﬁt best with that context.
background_label: Typically, this research was heavy with AI, taking context to be nothing lessthan a complete conceptual understanding of the preceding utterances.
background_label: This was reasonable, as suchan understanding of a text was often the main task anyway.
background_label: However, there are many text-processingtasksthatrequireonlya partialunderstandingofthetext, andhencea ‘lighter’representationofcontextis sufﬁcient.
objective_label: In this paper, we examine the idea oflexical chains as such a representation.
background_label: We showhow they can be constructed by means of WordNet, and how they can be applied in one particularlinguistic task: the detection and correction of malapropisms.A malapropism is the confounding of an intended word with another word of similar sound orsimilar spelling that has a quite different and malapropos meaning, e.g., an ingenuous [for ingenious]machine forpeelingoranges.
background_label: In thisexample, there isaone-letterdifference betweenthe malapropismand the correct word.
background_label: Ignorance, or a simple typing mistake, might cause such errors.
result_label: However, sinceingenuous is a correctly spelled word, traditional spelling checkers cannot detect this kind of mistake.In section 4, we will propose an algorithm for detecting and correcting malapropisms that is based onthe construction of lexical chains.

===================================
paper_id: 4428232; YEAR: 1999
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: cited - title_cbow200 - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Learning the parts of objects by non-negative matrix factorization.
ABSTRACT: background_label: Is perception of the whole based on perception of its parts?
background_label: There is psychological and physiological evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations.
background_label: But little is known about how brains or computers might learn the parts of objects.
method_label: Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text.
method_label: This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations.
method_label: Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints.
method_label: These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations.
result_label: When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.

===================================
paper_id: 4948322; YEAR: 2005
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200 - abs_tfidf - title_tfidf
TITLE: Measuring The Semantic Similarity Of Texts
ABSTRACT: background_label: This paper presents a knowledge-based method for measuring the semantic-similarity of texts.
background_label: While there is a large body of previous work focused on finding the semantic similarity of concepts and words, the application of these word-oriented methods to text similarity has not been yet explored.
method_label: In this paper, we introduce a method that combines word-to-word similarity metrics into a text-to-text metric, and we show that this method outperforms the traditional text similarity metrics based on lexical matching.

===================================
paper_id: 5509836; YEAR: 2002
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: cited - title_cbow200 - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Mining the Web for Synonyms: PMI-IR versus LSA on TOEFL
ABSTRACT: background_label: This paper presents a simple unsupervised learning algorithm for recognizing synonyms, based on statistical data acquired by querying a Web search engine.
method_label: The algorithm, called PMI-IR, uses Pointwise Mutual Information (PMI) and Information Retrieval (IR) to measure the similarity of pairs of words.
method_label: PMI-IR is empirically evaluated using 80 synonym test questions from the Test of English as a Foreign Language (TOEFL) and 50 synonym test questions from a collection of tests for students of English as a Second Language (ESL).
method_label: On both tests, the algorithm obtains a score of 74%.
method_label: PMI-IR is contrasted with Latent Semantic Analysis (LSA), which achieves a score of 64% on the same 80 TOEFL questions.
result_label: The paper discusses potential applications of the new unsupervised learning algorithm and some implications of the results for LSA and LSI (Latent Semantic Indexing).

===================================
paper_id: 10380306; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: abs_tfidf
TITLE: A German Corpus for Text Similarity Detection Tasks
ABSTRACT: background_label: Text similarity detection aims at measuring the degree of similarity between a pair of texts.
background_label: Corpora available for text similarity detection are designed to evaluate the algorithms to assess the paraphrase level among documents.
objective_label: In this paper we present a textual German corpus for similarity detection.
objective_label: The purpose of this corpus is to automatically assess the similarity between a pair of texts and to evaluate different similarity measures, both for whole documents or for individual sentences.
method_label: Therefore we have calculated several simple measures on our corpus based on a library of similarity functions.

===================================
paper_id: 10335672; YEAR: 2000
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: cited - title_cbow200 - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Latent semantic space: iterative scaling improves precision of inter-document similarity measurement
ABSTRACT: background_label: We present a novel algorithm that creates document vectors with reduced dimensionality.
objective_label: This work was motivated by an application characterizing relationships among documents in a collection.
method_label: Our algorithm yielded inter-document similarities with an average precision up to 17.8% higher than that of singular value decomposition (SVD) used for Latent Semantic Indexing.
method_label: The best performance was achieved with dimensional reduction rates that were 43% higher than SVD on average.
method_label: Our algorithm creates basis vectors for a reduced space by iteratively “scaling” vectors and computing eigenvectors.
method_label: Unlike SVD, it breaks the symmetry of documents and terms to capture information more evenly across documents.
result_label: We also discuss correlation with a probabilistic model and evaluate a method for selecting the dimensionality using log-likelihood estimation.

===================================
paper_id: 16222472; YEAR: 2012
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200 - title_tfidf
TITLE: A novel semantic similarity measure within sentences
ABSTRACT: background_label: This paper presents a novel sentence similarity computation algorithm.
method_label: We thought that the complete expression of a short sentence, not only depends on the syntactic structure, but also relies on the words and their weight, thus this method take word similarity feature and syntactic feature into account.
method_label: The proposed method can be used in a variety of applications involving text knowledge representation, automatic document summarization, and knowledge discovery.
result_label: The experiment demonstrates that the proposed algorithm has outstanding performance in handling with short sentences with complex syntax.

===================================
paper_id: 145446571; YEAR: 1993
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: abs_cbow200
TITLE: Predicting Clustering From Semantic Structure
ABSTRACT: background_label: This study presents a process model for predicting the strength of semantic clustering within homogeneous semantic domains.
objective_label: The key element of the model is the assumption that clustering between adjacent items in recall is a function of their semantic similarity defined by proximity in a multidimensional space.
method_label: Data from 17 word lists drawn from various homogeneous semantic domains were collected by a method that simultaneously provides interitem proximity data for similarity scaling and creates a memory list for later recall.
method_label: Wide variation in the strength of semantic clustering was observed among the 17 word lists.
result_label: Subjects' clustering performance was in close correspondence to predictions derived from the process model.
result_label: Moreover, the degree of observed and simulated clustering across lists was strongly associated with distributional features of the semantic structure of the word lists.

===================================
paper_id: 1398439; YEAR: 2003
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: cited - title_cbow200 - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Counter-Training in Discovery of Semantic Patterns
ABSTRACT: background_label: This paper presents a method for unsupervised discovery of semantic patterns.
background_label: Semantic patterns are useful for a variety of text understanding tasks, in particular for locating events in text for information extraction.
method_label: The method builds upon previously described approaches to iterative unsupervised pattern acquisition.
method_label: One common characteristic of prior approaches is that the output of the algorithm is a continuous stream of patterns, with gradually degrading precision.Our method differs from the previous pattern acquisition algorithms in that it introduces competition among several scenarios simultaneously.
method_label: This provides natural stopping criteria for the unsupervised learners, while maintaining good precision levels at termination.
result_label: We discuss the results of experiments with several scenarios, and examine different aspects of the new procedure.

===================================
paper_id: 61448109; YEAR: 2012
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: abs_cbow200
TITLE: Analysis of Similarity Measures with WordNet Based Text Document Clustering
ABSTRACT: background_label: Text Document Clustering aids in reorganizing the large collections of documents into a smaller number of manageable clusters.
background_label: While several clustering methods and the associated similarity measures have been proposed in the past, the partition clustering algorithms are reported performing well on document clustering.
background_label: Usually cosine function is used to measure the similarity between two documents in the criterion function, but it may not work well when the clusters are not well separated.
background_label: Word meanings are better than word forms in terms of representing the topics of documents.
method_label: Thus, here we have involved ontology into the text clustering algorithm.
method_label: In this research WordNet based document representation is attempted by assigning each word a part-ofspeech (POS) tag and by enriching the ‘bag-of-words’ data representation with synset concept which corresponds to synonym set that is introduced by WordNet.
method_label: After replacing the ‘bag of words’ with their respective Synset IDs a variant of K-Means algorithm is used for document clustering.
result_label: Then we compare the three popular similarity measures (Cosine, Pearson Correlation Coefficient and extended Jaccard) in conjunction with different types of vector space representation (Term Frequency and Term Frequency-Inverse Document Frequency) of documents.

===================================
paper_id: 21126286; YEAR: 2012
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200 - title_tfidf
TITLE: Finding disease similarity based on implicit semantic similarity.
ABSTRACT: background_label: Genomics has contributed to a growing collection of gene-function and gene-disease annotations that can be exploited by informatics to study similarity between diseases.
background_label: This can yield insight into disease etiology, reveal common pathophysiology and/or suggest treatment that can be appropriated from one disease to another.
background_label: Estimating disease similarity solely on the basis of shared genes can be misleading as variable combinations of genes may be associated with similar diseases, especially for complex diseases.
method_label: This deficiency can be potentially overcome by looking for common biological processes rather than only explicit gene matches between diseases.
result_label: The use of semantic similarity between biological processes to estimate disease similarity could enhance the identification and characterization of disease similarity.
background_label: We present functions to measure similarity between terms in an ontology, and between entities annotated with terms drawn from the ontology, based on both co-occurrence and information content.
background_label: The similarity measure is shown to outperform other measures used to detect similarity.
method_label: A manually curated dataset with known disease similarities was used as a benchmark to compare the estimation of disease similarity based on gene-based and Gene Ontology (GO) process-based comparisons.
result_label: The detection of disease similarity based on semantic similarity between GO Processes (Recall=55%, Precision=60%) performed better than using exact matches between GO Processes (Recall=29%, Precision=58%) or gene overlap (Recall=88% and Precision=16%).
result_label: The GO-Process based disease similarity scores on an external test set show statistically significant Pearson correlation (0.73) with numeric scores provided by medical residents.
result_label: GO-Processes associated with similar diseases were found to be significantly regulated in gene expression microarray datasets of related diseases.

===================================
paper_id: 2653224; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: title_tfidf - title_tfidfcbow200 - specter
TITLE: Wordform Similarity Increases With Semantic Similarity: An Analysis of 100 Languages.
ABSTRACT: background_label: Although the mapping between form and meaning is often regarded as arbitrary, there are in fact well-known constraints on words which are the result of functional pressures associated with language use and its acquisition.
background_label: In particular, languages have been shown to encode meaning distinctions in their sound properties, which may be important for language learning.
objective_label: Here, we investigate the relationship between semantic distance and phonological distance in the large-scale structure of the lexicon.
method_label: We show evidence in 100 languages from a diverse array of language families that more semantically similar word pairs are also more phonologically similar.
result_label: This suggests that there is an important statistical trend for lexicons to have semantically similar words be phonologically similar as well, possibly for functional reasons associated with language learning.

===================================
paper_id: 385116; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: abs_tfidfcbow200 - specter
TITLE: Boosting the Quality of Approximate String Matching by Synonyms
ABSTRACT: background_label: A string-similarity measure quantifies the similarity between two text strings for approximate string matching or comparison.
background_label: For example, the strings “Sam” and “Samuel” can be considered to be similar.
background_label: Most existing work that computes the similarity of two strings only considers syntactic similarities, for example, number of common words or q-grams.
background_label: While this is indeed an indicator of similarity, there are many important cases where syntactically-different strings can represent the same real-world object.
objective_label: For example, “Bill” is a short form of “William,” and “Database Management Systems” can be abbreviated as “DBMS.” Given a collection of predefined synonyms, the purpose of this article is to explore such existing knowledge to effectively evaluate the similarity between two strings and efficiently perform similarity searches and joins, thereby boosting the quality of approximate string matching.
method_label: In particular, we first present an expansion-based framework to measure string similarities efficiently while considering synonyms.
method_label: We then study efficient algorithms for similarity searches and joins by proposing two novel indexes, called SI-trees and QP-trees, which combine signature-filtering and length-filtering strategies.
method_label: In order to improve the efficiency of our algorithms, we develop an estimator to estimate the size of candidates to enable an online selection of signature filters.
method_label: This estimator provides strong low-error, high-confidence guarantees while requiring only logarithmic space and time costs, thus making our method attractive both in theory and in practice.
result_label: Finally, the experimental results from a comprehensive study of the algorithms with three real datasets verify the effectiveness and efficiency of our approaches.

===================================
paper_id: 1499545; YEAR: 2004
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200 - specter
TITLE: WordNet::Similarity - Measuring The Relatedness Of Concepts
ABSTRACT: background_label: WordNet::Similarity is a freely available software package that makes it possible to measure the semantic similarity and relatedness between a pair of concepts (or synsets).
background_label: It provides six measures of similarity, and three measures of relatedness, all of which are based on the lexical database WordNet.
method_label: These measures are implemented as Perl modules which take as input two concepts, and return a numeric value that represents the degree to which they are similar or related.

===================================
paper_id: 142200453; YEAR: 1991
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200 - title_tfidf - specter
TITLE: A Taxonomy of Semantic Relations
ABSTRACT: background_label: Relations between ideas have long been viewed as basic to thought.
background_label: Aristotle explained the sequence of ideas in recall in terms of contiguity, similarity, and contrast (Aristotle, 1928–1952, Chap.
background_label: 2).
other_label: John Locke used relations between ideas to account for the formation of complex ideas from simple ones and for the ability to reason (Rapaport, 1974, pp.
background_label: 68–85).
background_label: This reliance on relations between ideas as an explanatory construct led to an enduring interest in the classification of the different types of relations.
method_label: The early experimental psychologists proposed a variety of taxonomies, relying on both associative and introspective data.
method_label: At least 13 different classification systems were proposed prior to 1911 by Wundt, Kraeplin, Jung, and others (Warren, 1921).
result_label: Interest in the variety of relations declined during the following decades as behaviorists focused on understanding the relation of contiguity (e.g., Skinner, 1938).

===================================
paper_id: 61285239; YEAR: 2011
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: specter - title_cbow200 - abs_tfidf
TITLE: The semantics of similarity in geographic information retrieval
ABSTRACT: background_label: Similarity measures have a long tradition in fields such as information retrieval, artificial intelligence, and cognitive science.
background_label: Within the last years, these measures have been extended and reused to measure semantic similarity; i.e., for comparing meanings rather than syntactic differences.
background_label: Various measures for spatial applications have been de- veloped, but a solid foundation for answering what they measure; how they are best ap- plied in information retrieval; which role contextual information plays; and how similarity values or rankings should be interpreted is still missing.
background_label: It is therefore difficult to decide which measure should be used for a particular application or to compare results from dif- ferent similarity theories.
method_label: Based on a review of existing similarity measures, we introduce a framework to specify the semantics of similarity.
method_label: We discuss similarity-based information retrieval paradigms as well as their implementation in web-based user interfaces for geo- graphic information retrieval to demonstrate the applicability of the framework.
result_label: Finally, we formulate open challenges for similarity research.

===================================
paper_id: 2091942; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: abs_tfidfcbow200 - abs_tfidf - specter
TITLE: String similarity measures and joins with synonyms
ABSTRACT: background_label: A string similarity measure quantifies the similarity between two text strings for approximate string matching or comparison.
background_label: For example, the strings "Sam" and "Samuel" can be considered similar.
background_label: Most existing work that computes the similarity of two strings only considers syntactic similarities, e.g., number of common words or q-grams.
background_label: While these are indeed indicators of similarity, there are many important cases where syntactically different strings can represent the same real-world object.
method_label: For example, "Bill" is a short form of "William".
objective_label: Given a collection of predefined synonyms, the purpose of the paper is to explore such existing knowledge to evaluate string similarity measures more effectively and efficiently, thereby boosting the quality of string matching.
background_label: In particular, we first present an expansion-based framework to measure string similarities efficiently while considering synonyms.
objective_label: Because using synonyms in similarity measures is, while expressive, computationally expensive (NP-hard), we propose an efficient algorithm, called selective-expansion, which guarantees the optimality in many real scenarios.
method_label: We then study a novel indexing structure called SI-tree, which combines both signature and length filtering strategies, for efficient string similarity joins with synonyms.
method_label: We develop an estimator to approximate the size of candidates to enable an online selection of signature filters to further improve the efficiency.
method_label: This estimator provides strong low-error, high-confidence guarantees while requiring only logarithmic space and time costs, thus making our method attractive both in theory and in practice.
result_label: Finally, the results from an empirical study of the algorithms verify the effectiveness and efficiency of our approach.

===================================
paper_id: 7302240; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: abs_tfidf
TITLE: On the Complexity of Robust PCA and $\ell_1$-norm Low-Rank Matrix Approximation
ABSTRACT: background_label: The low-rank matrix approximation problem with respect to the component-wise $\ell_1$-norm ($\ell_1$-LRA), which is closely related to robust principal component analysis (PCA), has become a very popular tool in data mining and machine learning.
objective_label: Robust PCA aims at recovering a low-rank matrix that was perturbed with sparse noise, with applications for example in foreground-background video separation.
background_label: Although $\ell_1$-LRA is strongly believed to be NP-hard, there is, to the best of our knowledge, no formal proof of this fact.
method_label: In this paper, we prove that $\ell_1$-LRA is NP-hard, already in the rank-one case, using a reduction from MAX CUT.
result_label: Our derivations draw interesting connections between $\ell_1$-LRA and several other well-known problems, namely, robust PCA, $\ell_0$-LRA, binary matrix factorization, a particular densest bipartite subgraph problem, the computation of the cut norm of $\{-1,+1\}$ matrices, and the discrete basis problem, which we all prove to be NP-hard.

===================================
paper_id: 49340961; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: abs_tfidf
TITLE: The Corpus Replication Task
ABSTRACT: background_label: In the field of Natural Language Processing (NLP), we revisit the well-known word embedding algorithm word2vec.
background_label: Word embeddings identify words by vectors such that the words' distributional similarity is captured.
background_label: Unexpectedly, besides semantic similarity even relational similarity has been shown to be captured in word embeddings generated by word2vec, whence two questions arise.
background_label: Firstly, which kind of relations are representable in continuous space and secondly, how are relations built.
objective_label: In order to tackle these questions we propose a bottom-up point of view.
method_label: We call generating input text for which word2vec outputs target relations solving the Corpus Replication Task.
result_label: Deeming generalizations of this approach to any set of relations possible, we expect solving of the Corpus Replication Task to provide partial answers to the questions.

===================================
paper_id: 6795850; YEAR: 2012
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: abs_cbow200 - abs_tfidf - specter
TITLE: Content-based Text Categorization using Wikitology
ABSTRACT: background_label: A major computational burden, while performing document clustering, is the calculation of similarity measure between a pair of documents.
background_label: Similarity measure is a function that assign a real number between 0 and 1 to a pair of documents, depending upon the degree of similarity between them.
background_label: A value of zero means that the documents are completely dissimilar whereas a value of one indicates that the documents are practically identical.
method_label: Traditionally, vector-based models have been used for computing the document similarity.
method_label: The vector-based models represent several features present in documents.
result_label: These approaches to similarity measures, in general, cannot account for the semantics of the document.
background_label: Documents written in human languages contain contexts and the words used to describe these contexts are generally semantically related.
background_label: Motivated by this fact, many researchers have proposed semantic-based similarity measures by utilizing text annotation through external thesauruses like WordNet (a lexical database).
objective_label: In this paper, we define a semantic similarity measure based on documents represented in topic maps.
method_label: Topic maps are rapidly becoming an industrial standard for knowledge representation with a focus for later search and extraction.
method_label: The documents are transformed into a topic map based coded knowledge and the similarity between a pair of documents is represented as a correlation between the common patterns.
result_label: The experimental studies on the text mining datasets reveal that this new similarity measure is more effective as compared to commonly used similarity measures in text clustering.

===================================
paper_id: 5630662; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: title_tfidf - title_cbow200 - title_tfidfcbow200
TITLE: SEMILAR: The Semantic Similarity Toolkit
ABSTRACT: background_label: AbstractWe present in this paper SEMILAR, the SEMantic simILARity toolkit.
background_label: SEMILAR implements a number of algorithms for assessing the semantic similarity between two texts.
method_label: It is available as a Java library and as a Java standalone ap-plication offering GUI-based access to the implemented semantic similarity methods.
method_label: Furthermore, it offers facilities for manual se-mantic similarity annotation by experts through its component SEMILAT (a SEMantic simILarity Annotation Tool).

===================================
paper_id: 13888748; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: abs_tfidf
TITLE: An Efficient and Expressive Similarity Measure for Relational Clustering Using Neighbourhood Trees
ABSTRACT: background_label: Clustering is an underspecified task: there are no universal criteria for what makes a good clustering.
background_label: This is especially true for relational data, where similarity can be based on the features of individuals, the relationships between them, or a mix of both.
background_label: Existing methods for relational clustering have strong and often implicit biases in this respect.
objective_label: In this paper, we introduce a novel similarity measure for relational data.
method_label: It is the first measure to incorporate a wide variety of types of similarity, including similarity of attributes, similarity of relational context, and proximity in a hypergraph.
method_label: We experimentally evaluate how using this similarity affects the quality of clustering on very different types of datasets.
result_label: The experiments demonstrate that (a) using this similarity in standard clustering methods consistently gives good results, whereas other measures work well only on datasets that match their bias; and (b) on most datasets, the novel similarity outperforms even the best among the existing ones.

===================================
paper_id: 14713935; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: abs_tfidf - abs_cbow200 - specter
TITLE: Soft Similarity and Soft Cosine Measure: Similarity of Features in Vector Space Model
ABSTRACT: background_label: We show how to consider similarity between features for calculation of similarity of objects in the Vector Space Model (VSM) for machine learning algorithms and other classes of methods that involve similarity between objects.
background_label: Unlike LSA, we assume that similarity between features is known (say, from a synonym dictionary) and does not need to be learned from the data.We call the proposed similarity measure soft similarity.
background_label: Similarity between features is common, for example, in natural language processing: words, n-grams, or syntactic n-grams can be somewhat different (which makes them different features) but still have much in common: for example, words “play” and “game” are different but related.
method_label: When there is no similarity between features then our soft similarity measure is equal to the standard similarity.
method_label: For this, we generalize the well-known cosine similarity measure in VSM by introducing what we call “soft cosine measure”.
method_label: We propose various formulas for exact or approximate calculation of the soft cosine measure.
method_label: For example, in one of them we consider for VSM a new feature space consisting of pairs of the original features weighted by their similarity.
method_label: Again, for features that bear no similarity to each other, our formulas reduce to the standard cosine measure.
result_label: Our experiments show that our soft cosine measure provides better performance in our case study: entrance exams question answering task at CLEF.
result_label: In these experiments, we use syntactic n-grams as features and Levenshtein distance as the similarity between n-grams, measured either in characters or in elements of n-grams.

===================================
paper_id: 13263951; YEAR: 1994
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: cited - title_cbow200 - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Algorithm For Automatic Interpretation Of Noun Sequences
ABSTRACT: background_label: This paper describes an algorithm for automatically interpreting noun sequences in unrestricted text.
background_label: This system uses broadcoverage semantic information which has been acquired automatically by analyzing the definitions in an on-line dictionary.
method_label: Previously, computational studies of noun sequences made use of hand-coded semantic information, and they applied the analysis rules sequentially.
result_label: In contrast, the task of analyzing noun sequences in unrestricted text strongly favors an algorithm according to which the rules are applied in parallel and the best interpretation is determined by weights associated with rule applications.

===================================
paper_id: 25479094; YEAR: 2009
adju relevance: Irrelevant (0)
difference: 2; annotator1: 2; annotator3: 0
sources: abs_tfidf
TITLE: Knowledge-based vector space model for text clustering
ABSTRACT: background_label: This paper presents a new knowledge-based vector space model (VSM) for text clustering.
background_label: In the new model, semantic relationships between terms (e.g., words or concepts) are included in representing text documents as a set of vectors.
objective_label: The idea is to calculate the dissimilarity between two documents more effectively so that text clustering results can be enhanced.
method_label: In this paper, the semantic relationship between two terms is defined by the similarity of the two terms.
method_label: Such similarity is used to re-weight term frequency in the VSM.
background_label: We consider and study two different similarity measures for computing the semantic relationship between two terms based on two different approaches.
background_label: The first approach is based on the existing ontologies like WordNet and MeSH.
method_label: We define a new similarity measure that combines the edge-counting technique, the average distance and the position weighting method to compute the similarity of two terms from an ontology hierarchy.
method_label: The second approach is to make use of text corpora to construct the relationships between terms and then calculate their semantic similarities.
method_label: Three clustering algorithms, bisecting k-means, feature weighting k-means and a hierarchical clustering algorithm, have been used to cluster real-world text data represented in the new knowledge-based VSM.
result_label: The experimental results show that the clustering performance based on the new model was much better than that based on the traditional term-based VSM.

===================================
paper_id: 9500567; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: specter - abs_cbow200 - title_tfidfcbow200 - abs_tfidfcbow200 - title_tfidf
TITLE: Cross level semantic similarity: an evaluation framework for universal measures of similarity
ABSTRACT: background_label: Semantic similarity has typically been measured across items of approximately similar sizes.
background_label: As a result, similarity measures have largely ignored the fact that different types of linguistic item can potentially have similar or even identical meanings, and therefore are designed to compare only one type of linguistic item.
background_label: Furthermore, nearly all current similarity benchmarks within NLP contain pairs of approximately the same size, such as word or sentence pairs, preventing the evaluation of methods that are capable of comparing different sized items.
method_label: To address this, we introduce a new semantic evaluation called cross-level semantic similarity (CLSS), which measures the degree to which the meaning of a larger linguistic item, such as a paragraph, is captured by a smaller item, such as a sentence.
method_label: Our pilot CLSS task was presented as part of SemEval-2014, which attracted 19 teams who submitted 38 systems.
method_label: CLSS data contains a rich mixture of pairs, spanning from paragraphs to word senses to fully evaluate similarity measures that are capable of comparing items of any type.
method_label: Furthermore, data sources were drawn from diverse corpora beyond just newswire, including domain-specific texts and social media.
method_label: We describe the annotation process and its challenges, including a comparison with crowdsourcing, and identify the factors that make the dataset a rigorous assessment of a method’s quality.
method_label: Furthermore, we examine in detail the systems participating in the SemEval task to identify the common factors associated with high performance and which aspects proved difficult to all systems.
result_label: Our findings demonstrate that CLSS poses a significant challenge for similarity methods and provides clear directions for future work on universal similarity methods that can compare any pair of items.

===================================
paper_id: 121129390; YEAR: 1994
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: cited - title_cbow200 - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: THE CELL TRANSMISSION MODEL: A DYNAMIC REPRESENTATION OF HIGHWAY TRAFFIC CONSISTENT WITH THE HYDRODYNAMIC THEORY
ABSTRACT: background_label: This paper presents a simple representation of traffic on a highway with a single entrance and exit.
background_label: The representation can be used to predict traffic's evolution over time and space, including transient phenomena such as the building, propagation, and dissipation of queues.
method_label: The easy-to-solve difference equations used to predict traffic's evolution are shown to be the discrete analog of the differential equations arising from a special case of the hydrodynamic model of traffic flow.
method_label: The proposed method automatically generates appropriate changes in density at locations where the hydrodynamic theory would call for a shockwave; i.e., a jump in density such as those typically seen at the end of every queue.
method_label: The complex side calculations required by classical methods to keep track of shockwaves are thus eliminated.
result_label: The paper also shows how the equations can mimic the real-life development of stop-and-go traffic within moving queues.

===================================
paper_id: 61894598; YEAR: 1990
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: cited - title_cbow200 - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Indexing by Latent Semantic Analysis
ABSTRACT: method_label: A new method for automatic indexing and retrieval is described.
objective_label: The approach is to take advantage of implicit higher-order structure in the association of terms with documents (“semantic structure”) in order to improve the detection of relevant documents on the basis of terms found in queries.
method_label: The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca.
method_label: 100 orthogonal factors from which the original matrix can be approximated by linear combination.
method_label: Documents are represented by ca.
method_label: 100 item vectors of factor weights.
method_label: Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned.
result_label: initial tests find this completely automatic method for retrieval to be promising.

===================================
paper_id: 51725395; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: title_tfidf - title_cbow200 - title_tfidfcbow200 - specter
TITLE: Concurrent Learning of Semantic Relations
ABSTRACT: background_label: Discovering whether words are semantically related and identifying the specific semantic relation that holds between them is of crucial importance for NLP as it is essential for tasks like query expansion in IR.
background_label: Within this context, different methodologies have been proposed that either exclusively focus on a single lexical relation (e.g.
background_label: hypernymy vs. random) or learn specific classifiers capable of identifying multiple semantic relations (e.g.
background_label: hypernymy vs. synonymy vs. random).
objective_label: In this paper, we propose another way to look at the problem that relies on the multi-task learning paradigm.
other_label: In particular, we want to study whether the learning process of a given semantic relation (e.g.
other_label: hypernymy) can be improved by the concurrent learning of another semantic relation (e.g.
method_label: co-hyponymy).
method_label: Within this context, we particularly examine the benefits of semi-supervised learning where the training of a prediction function is performed over few labeled data jointly with many unlabeled ones.
result_label: Preliminary results based on simple learning strategies and state-of-the-art distributional feature representations show that concurrent learning can lead to improvements in a vast majority of tested situations.

===================================
paper_id: 9984860; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: title_tfidf - title_tfidfcbow200
TITLE: GADES: A Graph-based Semantic Similarity Measure
ABSTRACT: background_label: Knowledge graphs encode semantics that describes resources in terms of several aspects, e.g., neighbors, class hierarchies, or node degrees.
background_label: Assessing relatedness of knowledge graph entities is crucial for several data-driven tasks, e.g., ranking, clustering, or link discovery.
background_label: However, existing similarity measures consider aspects in isolation when determining entity relatedness.
objective_label: We address the problem of similarity assessment between knowledge graph entities, and devise GADES.
method_label: GADES relies on aspect similarities and computes a similarity measure as the combination of these similarity values.
method_label: We empirically evaluate the accuracy of GADES on knowledge graphs from different domains, e.g., proteins, and news.
result_label: Experiment results indicate that GADES exhibits higher correlation with gold standards than studied existing approaches.
result_label: Thus, these results suggest that similarity measures should not consider aspects in isolation, but combinations of them to precisely determine relatedness.

===================================
paper_id: 8570237; YEAR: 2001
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: cited - title_cbow200 - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Classifying The Semantic Relations In Noun Compounds Via A Domain-Specific Lexical Hierarchy
ABSTRACT: background_label: AbstractWe are developing corpus-based techniques for identifying semantic relations at an intermediate level of description (more specific than those used in case frames, but more general than those used in traditional knowledge representation systems).
objective_label: In this paper we describe a classification algorithm for identifying relationships between two-word noun compounds.
result_label: We find that a very simple approach using a machine learning algorithm and a domain-specific lexical hierarchy successfully generalizes from training instances, performing better on previously unseen words than a baseline consisting of training on the words themselves.

===================================
paper_id: 51604635; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: abs_tfidf
TITLE: Learning SMT(LRA) Constraints using SMT Solvers
ABSTRACT: background_label: AbstractWe introduce the problem of learning SMT(LRA) constraints from data.
background_label: SMT(LRA) extends propositional logic with (in)equalities between numerical variables.
background_label: Many relevant formal verification problems can be cast as SMT(LRA) instances and SMT(LRA) has supported recent developments in optimization and counting for hybrid Boolean and numerical domains.
method_label: We introduce SMT(LRA) learning, the task of learning SMT(LRA) formulas from examples of feasible and infeasible instances, and we contribute INCAL, an exact non-greedy algorithm for this setting.
method_label: Our approach encodes the learning task itself as an SMT(LRA) satisfiability problem that can be solved directly by SMT solvers.
method_label: INCAL is an incremental algorithm that achieves exact learning by looking only at a small subset of the data, leading to significant speed-ups.
result_label: We empirically evaluate our approach on both synthetic instances and benchmark problems taken from the SMT-LIB benchmarks repository.

===================================
paper_id: 526032; YEAR: 1991
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: cited - title_cbow200 - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Evaluating Text Categorization I
ABSTRACT: background_label: While certain standard procedures are widely used for evaluating text retrieval systems and algorithms, the same is not true for text categorization.
background_label: Omission of important data from reports is common and methods of measuring effectiveness vary widely.
method_label: This has made judging the relative merits of techniques for text categorization difficult and has disguised important research issues.In this paper I discuss a variety of ways of evaluating the effectiveness of text categorization systems, drawing both on reported categorization experiments and on methods used in evaluating query-driven retrieval.
method_label: I also consider the extent to which the same evaluation methods may be used with systems for text extraction, a more complex task.
result_label: In evaluating either kind of system, the purpose for which the output is to be used is crucial in choosing appropriate evaluation methods.

===================================
paper_id: 18621281; YEAR: 1995
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: cited - title_cbow200 - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Metaphor as an Emergent Property of Machine-Readable Dictionaries
ABSTRACT: background_label: Previous computational attempts to handle nonliteral word usage have been restricted to "toy" systems that combine hand-coded lexicons with restricted sets of metaphor types that can be used to sanction specific classes of semantic subcategorization violations.
background_label: These hand-coded efforts are unlikely to ever scale up to the rigors of real, free text.
objective_label: We describe an example-based approach to metaphor interpretation which exploits a large lexical knowledge base derived from a machine-readable dictionary.
method_label: We first present an extended example which is meant to demonstrate the scope of evidence about common English metaphors which is encoded in ordinary dictionaries.
method_label: We then go on to demonstrate how novel instances of metaphor can be interpreted by accessing this information.
method_label: A given violation of some default semantic expectation can be checked against the LKB to determine whether this violation is an instance of some more systematic mapping of English word meanings from one semantic domain to another.
result_label: We argue that this approach to metaphor interpretation obviates the need for the traditional "metaphor-handling component" in natural language understanding systems, and will allow these systems to overcome the britdeness of hand-coded approaches.

===================================
paper_id: 53786958; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator1: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200 - title_tfidf - specter
TITLE: Similarity measures for semantic relation extraction
ABSTRACT: background_label: Semantic relations, such as synonyms, hypernyms and co-hyponyms proved to be useful for text processing applications, including text similarity, query expansion, question answering and word sense disambiguation.
background_label: Such relations are practical because of the gap between lexical surface of the text and its meaning.
background_label: Indeed, the same concept is often represented by different terms.
background_label: However, existing resources often do not cover a vocabulary required by a given system.
background_label: Manual resource construction is prohibitively expensive for many projects.
result_label: On the other hand, precision of the existing extractors still do not meet quality of the hand-crafted resources.
background_label: All these factors motivate the development of novel extraction methods.
objective_label: This thesis deals with similarity measures for semantic relation extraction.
objective_label: The main research question we address, is how to improve precision and coverage of such measures.
method_label: First, we perform a large-scale study the baseline techniques.
method_label: Second, we propose four novel measures.
method_label: One of them significantly outperforms the baselines, the others perform comparably to the state-of-the-art techniques.
method_label: Finally, we successfully apply one of the novel measures in two text processing systems.

===================================
paper_id: 16029757; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 2; annotator1: 2; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200 - specter
TITLE: Alternative measures of word relatedness in distributional semantics
ABSTRACT: background_label: AbstractThis paper presents an alternative method to measuring word-word semantic relatedness in distributional semantics framework.
method_label: The main idea is to represent target words as rankings of all co-occurring words in a text corpus, ordered by their tf -idf weight and use a metric between rankings (such as Jaro distance or Rank distance) to compute semantic relatedness.
method_label: This method has several advantages over the standard approach that uses cosine measure in a vector space, mainly in that it is computationally less expensive (i.e.
method_label: does not require working in a high dimensional space, employing only rankings and a distance which is linear in the rank's length) and presumably more robust.
method_label: We tested this method on the standard WS-353 Test, obtaining the co-occurrence frequency from the Wacky corpus.
result_label: The results are comparable to the methods which use vector space models; and, most importantly, the method can be extended to the very challenging task of measuring phrase semantic relatedness.

===================================
paper_id: 143253950; YEAR: 1966
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: cited - title_cbow200 - abs_cbow200 - title_tfidfcbow200 - abs_tfidf - abs_tfidfcbow200 - title_tfidf - specter
TITLE: Cognition and Thought: An Information-Processing Approach.
ABSTRACT: background_label: This book covers the very interesting subject of computer simulation of such human functions as cognition and thought.
background_label: Its stated purpose is ".
background_label: .
objective_label: .
background_label: to introduce a general psychological audience to the psychological implications of information-processing concepts and computer simulation and to what theories and theorists in this area are about."
method_label: At this it succeeds reasonably well, setting forth some background on the theoretical framework involved, and going on to somewhat more concrete discussion of specific information processing models such as the General Problem Solver (GPS) and Argus.
method_label: There is an appendix on Information Processing Language-V (IPL-V).
result_label: For the potential reader, it might be important to give some idea of whether this is a book for casual reading as an entertaining introduction to the field, or a book for serious study.
other_label: Unfortunately —and I believe this is its major failing—it is neither.
result_label: It requires a good deal of concentrated effort to follow,

===================================
paper_id: 15241812; YEAR: 2008
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200 - title_tfidf
TITLE: New model of semantic similarity measuring in wordnet
ABSTRACT: objective_label: This paper presents a new model of measuring semantic similarity in the taxonomy of WordNet.
method_label: The model takes the path length between two concepts and IC value of each concept as its metric, furthermore, the weight of two metrics can be adapted artificially.
method_label: In order to evaluate our model, traditional and widely used datasets are used.
result_label: Firstly, coefficients of correlation between human ratings of similarity and six computational models are calculated, the result shows our new model outperforms their homologues.
result_label: Then, the distribution graphs of similarity value of 65 word pairs are discussed our model having no faulted zone more centralized than other five methods.
result_label: So our model can make up the insufficient of other methods which only using one metric(path length or IC value) in their model.

===================================
paper_id: 52902384; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: abs_tfidfcbow200 - specter
TITLE: Text Similarity in Vector Space Models: A Comparative Study
ABSTRACT: background_label: Automatic measurement of semantic text similarity is an important task in natural language processing.
objective_label: In this paper, we evaluate the performance of different vector space models to perform this task.
method_label: We address the real-world problem of modeling patent-to-patent similarity and compare TFIDF (and related extensions), topic models (e.g., latent semantic indexing), and neural models (e.g., paragraph vectors).
method_label: Contrary to expectations, the added computational cost of text embedding methods is justified only when: 1) the target text is condensed; and 2) the similarity comparison is trivial.
method_label: Otherwise, TFIDF performs surprisingly well in other cases: in particular for longer and more technical texts or for making finer-grained distinctions between nearest neighbors.
result_label: Unexpectedly, extensions to the TFIDF method, such as adding noun phrases or calculating term weights incrementally, were not helpful in our context.

===================================
paper_id: 17118309; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 1; annotator1: 1; annotator3: 0
sources: abs_cbow200 - abs_tfidf - specter
TITLE: An improved semantic similarity measure for document clustering based on topic maps
ABSTRACT: background_label: A major computational burden, while performing document clustering, is the calculation of similarity measure between a pair of documents.
background_label: Similarity measure is a function that assigns a real number between 0 and 1 to a pair of documents, depending upon the degree of similarity between them.
background_label: A value of zero means that the documents are completely dissimilar whereas a value of one indicates that the documents are practically identical.
method_label: Traditionally, vector-based models have been used for computing the document similarity.
method_label: The vector-based models represent several features present in documents.
result_label: These approaches to similarity measures, in general, cannot account for the semantics of the document.
background_label: Documents written in human languages contain contexts and the words used to describe these contexts are generally semantically related.
background_label: Motivated by this fact, many researchers have proposed seman-tic-based similarity measures by utilizing text annotation through external thesauruses like WordNet (a lexical database).
objective_label: In this paper, we define a semantic similarity measure based on documents represented in topic maps.
method_label: Topic maps are rapidly becoming an industrial standard for knowledge representation with a focus for later search and extraction.
method_label: The documents are transformed into a topic map based coded knowledge and the similarity between a pair of documents is represented as a correlation between the common patterns (sub-trees).
result_label: The experimental studies on the text mining datasets reveal that this new similarity measure is more effective as compared to commonly used similarity measures in text clustering.

