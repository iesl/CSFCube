======================================================================
paper_id: 13949438; YEAR: 2013
TITLE: Robust Logistic Regression using Shift Parameters (Long Version)
ABSTRACT: background_label: Annotation errors can significantly hurt classifier performance, yet datasets are only growing noisier with the increased use of Amazon Mechanical Turk and techniques like distant supervision that automatically generate labels.
objective_label: In this paper, we present a robust extension of logistic regression that incorporates the possibility of mislabelling directly into the objective.
method_label: Our model can be trained through nearly the same means as logistic regression, and retains its efficiency on high-dimensional datasets.
result_label: Through named entity recognition experiments, we demonstrate that our approach can provide a significant improvement over the standard model when annotation errors are present.
===================================
paper_id: 49557308; YEAR: 2018
adju relevance: Similar (+2)
difference: 1; annotator4: 1; annotator3: 2
sources: specter
TITLE: Training a Neural Network in a Low-Resource Setting on Automatically Annotated Noisy Data
ABSTRACT: background_label: Manually labeled corpora are expensive to create and often not available for low-resource languages or domains.
background_label: Automatic labeling approaches are an alternative way to obtain labeled data in a quicker and cheaper way.
background_label: However, these labels often contain more errors which can deteriorate a classifier's performance when trained on this data.
method_label: We propose a noise layer that is added to a neural network architecture.
method_label: This allows modeling the noise and train on a combination of clean and noisy data.
result_label: We show that in a low-resource NER task we can improve performance by up to 35% by using additional, noisy data and handling the noise.

===================================
paper_id: 28853346; YEAR: 2015
adju relevance: Similar (+2)
difference: 2; annotator4: 0; annotator3: 2
sources: abs_tfidf
TITLE: Probabilistic learning from mislabelled data for multimedia content recognition
ABSTRACT: background_label: There have been considerable advances in multimedia recognition recently as powerful computing capabilities and large, representative datasets become ubiquitous.
background_label: A fundamental assumption of traditional recognition techniques is that the data available for training are accurately labelled.
background_label: Given the scale and diversity of web data, it takes considerable annotation effort to reduce label noise to acceptable levels.
method_label: In this work, we propose a novel method to work around this issue by utilizing approximate apriori estimates of the mislabelling probabilities to design a noise-aware learning framework.
result_label: We demonstrate the proposed framework's effectiveness on several datasets of various modalities and show that it is able to achieve high levels of accuracy even when faced with significant mislabelling in the data.

===================================
paper_id: 10911081; YEAR: 2012
adju relevance: Similar (+2)
difference: 0; annotator4: 2; annotator3: 2
sources: specter - abs_tfidf - title_tfidf
TITLE: Label-noise Robust Logistic Regression and Its Applications
ABSTRACT: other_label: Abstract.
background_label: The classical problem of learning a classifier relies on a set of labelled examples, without ever questioning the correctness of the provided label assignments.
background_label: However, there is an increasing realisation that labelling errors are not uncommon in real situations.
method_label: In this paper we consider a label-noise robust version of the logistic regression and multinomial logistic regression classifiers and develop the following contributions: (i) We derive efficient multiplicative updates to estimate the label flipping probabilities, and we give a proof of convergence for our algorithm.
method_label: (ii) We develop a novel sparsity-promoting regularisation approach which allows us to tackle challenging high dimensional noisy settings.
result_label: (iii) Finally, we throughly evaluate the performance of our approach in synthetic experiments and we demonstrate several real applications including gene expression analysis, class topology discovery and learning from crowdsourcing data.

===================================
paper_id: 120362896; YEAR: 1998
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: title_tfidf
TITLE: Mixed Logistic Regression Models
ABSTRACT: background_label: This article studies binomial mixture models that include covariates in binomial parameters and mixing probabilities.
method_label: This model contains logistic regression, nonparametric mixed logistic regression (Follmann and Lambert 1989) and independent binomial mixture models as special cases, and provides an alternative to quasi-likelihood and betabinomial regression for modeling extra-binomial variation.
method_label: Estimation methods based on the EM and quasi-Newton algorithms, properties of these estimates, a model selection procedure, residual analysis, and goodness of fit are discussed.
method_label: This methodology is motivated and illustrated with an example.
result_label: A Monte Carlo study investigates behavior of the estimates and model selection criteria.

===================================
paper_id: 42639; YEAR: 2009
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: cited
TITLE: Regularization paths for generalized linear models via coordinate descent
ABSTRACT: background_label: We develop fast algorithms for estimation of generalized linear models with convex penalties.
method_label: The models include linear regression, two-class logistic regression, and multinomial regression problems while the penalties include ℓ(1) (the lasso), ℓ(2) (ridge regression) and mixtures of the two (the elastic net).
method_label: The algorithms use cyclical coordinate descent, computed along a regularization path.
method_label: The methods can handle large problems and can also deal efficiently with sparse features.
result_label: In comparative timings we find that the new algorithms are considerably faster than competing methods.

===================================
paper_id: 34426930; YEAR: 1999
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: title_tfidf
TITLE: Dynamic logistic regression
ABSTRACT: objective_label: We propose an online learning algorithm for training a logistic regression model on nonstationary classification problems.
method_label: The nonstationarity is captured by modelling the weights in a logistic regression classifier as evolving according to a first order Markov process.
method_label: The weights are updated using the extended Kalman filter formalism and nonstationarities are tracked by inferring a time-varying state noise variance parameter.
method_label: We describe an algorithm for doing this based on maximising the evidence of updated predictions.
method_label: The algorithm is illustrated on a number of synthetic problems.

===================================
paper_id: 8024801; YEAR: 2012
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Genomic selection using regularized linear regression models: ridge regression, lasso, elastic net and their extensions
ABSTRACT: background_label: BACKGROUND Genomic selection (GS) is emerging as an efficient and cost-effective method for estimating breeding values using molecular markers distributed over the entire genome.
background_label: In essence, it involves estimating the simultaneous effects of all genes or chromosomal segments and combining the estimates to predict the total genomic breeding value (GEBV).
background_label: Accurate prediction of GEBVs is a central and recurring challenge in plant and animal breeding.
objective_label: The existence of a bewildering array of approaches for predicting breeding values using markers underscores the importance of identifying approaches able to efficiently and accurately predict breeding values.
method_label: Here, we comparatively evaluate the predictive performance of six regularized linear regression methods-- ridge regression, ridge regression BLUP, lasso, adaptive lasso, elastic net and adaptive elastic net-- for predicting GEBV using dense SNP markers.
method_label: METHODS We predicted GEBVs for a quantitative trait using a dataset on 3000 progenies of 20 sires and 200 dams and an accompanying genome consisting of five chromosomes with 9990 biallelic SNP-marker loci simulated for the QTL-MAS 2011 workshop.
method_label: We applied all the six methods that use penalty-based (regularization) shrinkage to handle datasets with far more predictors than observations.
result_label: The lasso, elastic net and their adaptive extensions further possess the desirable property that they simultaneously select relevant predictive markers and optimally estimate their effects.
background_label: The regression models were trained with a subset of 2000 phenotyped and genotyped individuals and used to predict GEBVs for the remaining 1000 progenies without phenotypes.
method_label: Predictive accuracy was assessed using the root mean squared error, the Pearson correlation between predicted GEBVs and (1) the true genomic value (TGV), (2) the true breeding value (TBV) and (3) the simulated phenotypic values based on fivefold cross-validation (CV).
result_label: RESULTS The elastic net, lasso, adaptive lasso and the adaptive elastic net all had similar accuracies but outperformed ridge regression and ridge regression BLUP in terms of the Pearson correlation between predicted GEBVs and the true genomic value as well as the root mean squared error.
result_label: The performance of RR-BLUP was also somewhat better than that of ridge regression.
result_label: This pattern was replicated by the Pearson correlation between predicted GEBVs and the true breeding values (TBV) and the root mean squared error calculated with respect to TBV, except that accuracy was lower for all models, most especially for the adaptive elastic net.
result_label: The correlation between the predicted GEBV and simulated phenotypic values based on the fivefold CV also revealed a similar pattern except that the adaptive elastic net had lower accuracy than both the ridge regression methods.
result_label: CONCLUSIONS All the six models had relatively high prediction accuracies for the simulated data set.
result_label: Accuracy was higher for the lasso type methods than for ridge regression and ridge regression BLUP.

===================================
paper_id: 35768531; YEAR: 2016
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Applying Variant Variable Regularized Logistic Regression for Modeling Software Defect Predictor
ABSTRACT: background_label: Abstract-Empirical studies on software defect prediction models have come up with various predictors.
background_label: In this study we examined variable regularized factors in conjunction with Logistic regression.
background_label: Our work was built on eight public NASA datasets commonly used in this field.
method_label: We used one of the datasets for our learning classification out of which we selected the regularization factor with the best predictor model; we then used the same regularization factor to classify the other seven datasets.
method_label: Our proposed algorithm Variant Variable Regularized Logistic Regression (VVRLR) and modified VVRLR; were then used in the following metrics to measure the effectiveness of our predictor model: accuracy, precision, recall and F-Measure for each dataset.
method_label: We measured above metrics using three Weka models, namely: BayesianLogisticRegression, NaiveBayes and Simple Logistic and then compared these results with VVRLR.
result_label: VRLR and modified VVRLR outperformed the weka algorithms per our metric measurements.
result_label: The VVRLR produced the best accuracy of 100.00%, and an average accuracy of 91.65 %; we had an individual highest precision of 100.00%, highest individual recall of 100.00% and F-measure of 100.00% as the overall best with an average value of 76.41% was recorded by VVRLR for some datasets used in our experiments.
result_label: Our proposed modified VVRLR and variant VVRLR algorithms for F-measures outperformed the three weka algorithms.Index Terms-F-measure, precision, recall, variant variable regularized logistic regression.

===================================
paper_id: 17588364; YEAR: 2014
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidf - title_tfidf
TITLE: Clinical risk prediction with multilinear sparse logistic regression
ABSTRACT: background_label: Logistic regression is one core predictive modeling technique that has been used extensively in health and biomedical problems.
background_label: Recently a lot of research has been focusing on enforcing sparsity on the learned model to enhance its effectiveness and interpretability, which results in sparse logistic regression model.
background_label: However, no matter the original or sparse logistic regression, they require the inputs to be in vector form.
background_label: This limits the applicability of logistic regression in the problems when the data cannot be naturally represented vectors (e.g., functional magnetic resonance imaging and electroencephalography signals).
method_label: To handle the cases when the data are in the form of multi-dimensional arrays, we propose MulSLR: Multilinear Sparse Logistic Regression.
method_label: MulSLR can be viewed as a high order extension of sparse logistic regression.
method_label: Instead of solving one classification vector as in conventional logistic regression, we solve for K classification vectors in MulSLR (K is the number of modes in the data).
method_label: We propose a block proximal descent approach to solve the problem and prove its convergence.
method_label: The convergence rate of the proposed algorithm is also analyzed.
result_label: Finally we validate the efficiency and effectiveness of MulSLR on predicting the onset risk of patients with Alzheimer's disease and heart failure.

===================================
paper_id: 25755275; YEAR: 2018
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200
TITLE: Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination
ABSTRACT: background_label: Neural net classifiers trained on data with annotated class labels can also capture apparent visual similarity among categories without being directed to do so.
objective_label: We study whether this observation can be extended beyond the conventional domain of supervised learning: Can we learn a good feature representation that captures apparent similarity among instances, instead of classes, by merely asking the feature to be discriminative of individual instances?
method_label: We formulate this intuition as a non-parametric classification problem at the instance-level, and use noise-contrastive estimation to tackle the computational challenges imposed by the large number of instance classes.
result_label: Our experimental results demonstrate that, under unsupervised learning settings, our method surpasses the state-of-the-art on ImageNet classification by a large margin.
result_label: Our method is also remarkable for consistently improving test performance with more training data and better network architectures.
result_label: By fine-tuning the learned feature, we further obtain competitive results for semi-supervised learning and object detection tasks.
result_label: Our non-parametric model is highly compact: With 128 features per image, our method requires only 600MB storage for a million images, enabling fast nearest neighbour retrieval at the run time.

===================================
paper_id: 15024318; YEAR: 2015
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: title_tfidf - abs_tfidf
TITLE: Distributionally Robust Logistic Regression
ABSTRACT: objective_label: This paper proposes a distributionally robust approach to logistic regression.
method_label: We use the Wasserstein distance to construct a ball in the space of probability distributions centered at the uniform distribution on the training samples.
method_label: If the radius of this ball is chosen judiciously, we can guarantee that it contains the unknown data-generating distribution with high confidence.
method_label: We then formulate a distributionally robust logistic regression model that minimizes a worst-case expected logloss function, where the worst case is taken over all distributions in the Wasserstein ball.
method_label: We prove that this optimization problem admits a tractable reformulation and encapsulates the classical as well as the popular regularized logistic regression problems as special cases.
method_label: We further propose a distributionally robust approach based on Wasserstein balls to compute upper and lower confidence bounds on the misclassification probability of the resulting classifier.
method_label: These bounds are given by the optimal values of two highly tractable linear programs.
result_label: We validate our theoretical out-of-sample guarantees through simulated and empirical experiments.

===================================
paper_id: 31665444; YEAR: 1979
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: title_cbow200 - title_tfidfcbow200
TITLE: Robust Locally Weighted Regression and Smoothing Scatterplots
ABSTRACT: background_label: Abstract The visual information on a scatterplot can be greatly enhanced, with little additional cost, by computing and plotting smoothed points.
method_label: Robust locally weighted regression is a method for smoothing a scatterplot, (x i , y i ), i = 1, …, n, in which the fitted value at z k is the value of a polynomial fit to the data using weighted least squares, where the weight for (x i , y i ) is large if x i is close to x k and small if it is not.
method_label: A robust fitting procedure is used that guards against deviant points distorting the smoothed points.
method_label: Visual, computational, and statistical issues of robust locally weighted regression are discussed.
result_label: Several examples, including data on lead intoxication, are used to illustrate the methodology.

===================================
paper_id: 119797853; YEAR: 2015
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: title_tfidf
TITLE: Ordinal Logistic Regression
ABSTRACT: background_label: Many medical and epidemiologic studies incorporate an ordinal response variable.
background_label: In some cases an ordinal response Y represents levels of a standard measurement scale such as severity of pain (none, mild, moderate, severe).
background_label: In other cases, ordinal responses are constructed by specifying a hierarchy of separate endpoints.
background_label: For example, clinicians may specify an ordering of the severity of several component events and assign patients to the worst event present from among none, heart attack, disabling stroke, and death.
method_label: Still another use of ordinal response methods is the application of rank-based methods to continuous responses so as to obtain robust inferences.
method_label: For example, the proportional odds model described later allows for a continuous Y and is really a generalization of the Wilcoxon–Mann–Whitney rank test.
result_label: Thus the semiparametric proportional odds model is a direct competitor of ordinary linear models.

===================================
paper_id: 1087924; YEAR: 2014
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 0
sources: title_tfidf - abs_tfidf
TITLE: Relational Logistic Regression
ABSTRACT: background_label: Logistic regression is a commonly used representation for aggregators in Bayesian belief networks when a child has multiple parents.
background_label: In this paper we consider extending logistic regression to relational models, where we want to model varying populations and interactions among parents.
objective_label: In this paper, we first examine the representational problems caused by population variation.
method_label: We show how these problems arise even in simple cases with a single parametrized parent, and propose a linear relational logistic regression which we show can represent arbitrary linear (in population size) decision thresholds, whereas the traditional logistic regression cannot.
method_label: Then we examine representing interactions among the parents of a child node, and representing non-linear dependency on population size.
method_label: We propose a multi-parent relational logistic regression which can represent interactions among parents and arbitrary polynomial decision thresholds.
result_label: Finally, we show how other well-known aggregators can be represented using this relational logistic regression.

===================================
paper_id: 299825; YEAR: 2016
adju relevance: Related (+1)
difference: 1; annotator4: 2; annotator3: 1
sources: title_tfidf
TITLE: Robust Regression.
ABSTRACT: background_label: Discriminative methods (e.g., kernel regression, SVM) have been extensively used to solve problems such as object recognition, image alignment and pose estimation from images.
background_label: These methods typically map image features ( X) to continuous (e.g., pose) or discrete (e.g., object category) values.
background_label: A major drawback of existing discriminative methods is that samples are directly projected onto a subspace and hence fail to account for outliers common in realistic training sets due to occlusion, specular reflections or noise.
method_label: It is important to notice that existing discriminative approaches assume the input variables X to be noise free.
background_label: Thus, discriminative methods experience significant performance degradation when gross outliers are present.
background_label: Despite its obvious importance, the problem of robust discriminative learning has been relatively unexplored in computer vision.
objective_label: This paper develops the theory of robust regression (RR) and presents an effective convex approach that uses recent advances on rank minimization.
method_label: The framework applies to a variety of problems in computer vision including robust linear discriminant analysis, regression with missing data, and multi-label classification.
result_label: Several synthetic and real examples with applications to head pose estimation from images, image and video classification and facial attribute classification with missing data are used to illustrate the benefits of RR.

===================================
paper_id: 14838761; YEAR: 2016
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: abs_tfidf - title_tfidf
TITLE: Fault-tolerant distributed logistic regression using unreliable components
ABSTRACT: background_label: We consider the problem of computing distributed logistic regression using unreliable components.
background_label: We consider both faults in the memory units and faults in the processing units.
method_label: We show that using a real-number-coding technique, we can suppress errors during the computation and ensure that logistic regression converges with bounded error if the number of faults that happen during each iteration of the logistic regression is bounded, even when the faults happen in an adversarial manner.
method_label: Moreover, since the coding technique is based on computation with real numbers, we show that the error-correction can be carried out at the algorithmic level (or block-level) based on the results from intermediate steps of logistic regression.
result_label: Therefore, we only need to add redundant hardware at block-level, not the circuit level, for achieving fault-tolerance in the computation of logistic regression.

===================================
paper_id: 16616883; YEAR: 2007
adju relevance: Related (+1)
difference: 0; annotator4: 1; annotator3: 1
sources: cited
TITLE: Class Noise Mitigation through Instance Weighting
ABSTRACT: other_label: Abstract.
background_label: We describe a novel framework for class noise mitigation that assigns a vector of class membership probabilities to each training instance, and uses the confidence on the current label as a weight during training.
method_label: The probability vector should be calculated such that clean instances have a high confidence on its current label, while mislabeled instances have a low confidence on its current label and a high confidence on its correct label.
method_label: Past research focuses on techniques that either discard or correct instances.
objective_label: This paper proposes that discarding and correcting are special cases of instance weighting, and thus, part of this framework.
method_label: We propose a method that uses clustering to calculate a probability distribution over the class labels for each instance.
method_label: We demonstrate that our method improves classifier accuracy over the original training set.
result_label: We also demonstrate that instance weighting can outperform discarding.

===================================
paper_id: 1134221; YEAR: 2007
adju relevance: Related (+1)
difference: 1; annotator4: 1; annotator3: 2
sources: abs_tfidf
TITLE: Sparse Logistic Regression with Lp Penalty for Biomarker Identification
ABSTRACT: objective_label: In this paper, we propose a novel method for sparse logistic regression with non-convex regularization Lp (p <1).
method_label: Based on smooth approximation, we develop several fast algorithms for learning the classifier that is applicable to high dimensional dataset such as gene expression.
method_label: To the best of our knowledge, these are the first algorithms to perform sparse logistic regression with an Lp and elastic net (Le) penalty.
method_label: The regularization parameters are decided through maximizing the area under the ROC curve (AUC) of the test data.
result_label: Experimental results on methylation and microarray data attest the accuracy, sparsity, and efficiency of the proposed algorithms.
result_label: Biomarkers identified with our methods are compared with that in the literature.
result_label: Our computational results show that Lp Logistic regression (p <1) outperforms the L1 logistic regression and SCAD SVM.
result_label: Software is available upon request from the first author.

===================================
paper_id: 17440613; YEAR: 2011
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: C 3 E: A Framework for Combining Ensembles of Classifiers and Clusterers
ABSTRACT: other_label: Abstract.
background_label: The combination of multiple classifiers to generate a single classifier has been shown to be very useful in practice.
background_label: Similarly, several efforts have shown that cluster ensembles can improve the quality of results as compared to a single clustering solution.
objective_label: These observations suggest that ensembles containing both classifiers and clusterers are potentially useful as well.
method_label: Specifically, clusterers provide supplementary constraints that can improve the generalization capability of the resulting classifier.
method_label: This paper introduces a new algorithm named C 3 E that combines ensembles of classifiers and clusterers.
result_label: Our experimental evaluation of C 3 E shows that it provides good classification accuracies in eleven tasks derived from three real-world applications.
result_label: In addition, C 3 E produces better results than the recently introduced Bipartite Graphbased Consensus Maximization (BGCM) Algorithm, which combines multiple supervised and unsupervised models and is the algorithm most closely related to C 3 E.

===================================
paper_id: 1446909; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: Joint Deep Learning for Car Detection
ABSTRACT: background_label: Traditional object recognition approaches apply feature extraction, part deformation handling, occlusion handling and classification sequentially while they are independent from each other.
background_label: Ouyang and Wang proposed a model for jointly learning of all of the mentioned processes using one deep neural network.
method_label: We utilized, and manipulated their toolbox in order to apply it in car detection scenarios where it had not been tested.
method_label: Creating a single deep architecture from these components, improves the interaction between them and can enhance the performance of the whole system.
method_label: We believe that the approach can be used as a general purpose object detection toolbox.
method_label: We tested the algorithm on UIUC car dataset, and achieved an outstanding result.
result_label: The accuracy of our method was 97 % while the previously reported results showed an accuracy of up to 91 %.
result_label: We strongly believe that having an experiment on a larger dataset can show the advantage of using deep models over shallow ones.

===================================
paper_id: 73516151; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200
TITLE: f‐AnoGAN: Fast unsupervised anomaly detection with generative adversarial networks
ABSTRACT: background_label: &NA; Obtaining expert labels in clinical imaging is difficult since exhaustive annotation is time‐consuming.
background_label: Furthermore, not all possibly relevant markers may be known and sufficiently well described a priori to even guide annotation.
background_label: While supervised learning yields good results if expert labeled training data is available, the visual variability, and thus the vocabulary of findings, we can detect and exploit, is limited to the annotated lesions.
objective_label: Here, we present fast AnoGAN (f‐AnoGAN), a generative adversarial network (GAN) based unsupervised learning approach capable of identifying anomalous images and image segments, that can serve as imaging biomarker candidates.
method_label: We build a generative model of healthy training data, and propose and evaluate a fast mapping technique of new data to the GAN's latent space.
method_label: The mapping is based on a trained encoder, and anomalies are detected via a combined anomaly score based on the building blocks of the trained model – comprising a discriminator feature residual error and an image reconstruction error.
method_label: In the experiments on optical coherence tomography data, we compare the proposed method with alternative approaches, and provide comprehensive empirical evidence that f‐AnoGAN outperforms alternative approaches and yields high anomaly detection accuracy.
result_label: In addition, a visual Turing test with two retina experts showed that the generated images are indistinguishable from real normal retinal OCT images.
other_label: The f‐AnoGAN code is available at https://github.com/tSchlegl/f‐AnoGAN.

===================================
paper_id: 4410104; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: Efficient Interactive Annotation of Segmentation Datasets with Polygon-RNN++
ABSTRACT: background_label: Manually labeling datasets with object masks is extremely time consuming.
objective_label: In this work, we follow the idea of Polygon-RNN to produce polygonal annotations of objects interactively using humans-in-the-loop.
method_label: We introduce several important improvements to the model: 1) we design a new CNN encoder architecture, 2) show how to effectively train the model with Reinforcement Learning, and 3) significantly increase the output resolution using a Graph Neural Network, allowing the model to accurately annotate high-resolution objects in images.
method_label: Extensive evaluation on the Cityscapes dataset shows that our model, which we refer to as Polygon-RNN++, significantly outperforms the original model in both automatic (10% absolute and 16% relative improvement in mean IoU) and interactive modes (requiring 50% fewer clicks by annotators).
method_label: We further analyze the cross-domain scenario in which our model is trained on one dataset, and used out of the box on datasets from varying domains.
result_label: The results show that Polygon-RNN++ exhibits powerful generalization capabilities, achieving significant improvements over existing pixel-wise methods.
result_label: Using simple online fine-tuning we further achieve a high reduction in annotation time for new datasets, moving a step closer towards an interactive annotation tool to be used in practice.

===================================
paper_id: 174801519; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: Visually Grounded Neural Syntax Acquisition
ABSTRACT: background_label: We present the Visually Grounded Neural Syntax Learner (VG-NSL), an approach for learning syntactic representations and structures without any explicit supervision.
background_label: The model learns by looking at natural images and reading paired captions.
method_label: VG-NSL generates constituency parse trees of texts, recursively composes representations for constituents, and matches them with images.
method_label: We define concreteness of constituents by their matching scores with images, and use it to guide the parsing of text.
method_label: Experiments on the MSCOCO data set show that VG-NSL outperforms various unsupervised parsing approaches that do not use visual grounding, in terms of F1 scores against gold parse trees.
method_label: We find that VGNSL is much more stable with respect to the choice of random initialization and the amount of training data.
result_label: We also find that the concreteness acquired by VG-NSL correlates well with a similar measure defined by linguists.
result_label: Finally, we also apply VG-NSL to multiple languages in the Multi30K data set, showing that our model consistently outperforms prior unsupervised approaches.

===================================
paper_id: 17458743; YEAR: 2002
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: abs_tfidf - title_tfidf
TITLE: An Introduction to Logistic Regression Analysis and Reporting
ABSTRACT: objective_label: Abstract The purpose of this article is to provide researchers, editors, and readers with a set of guidelines for what to expect in an article using logistic regression techniques.
method_label: Tables, figures, and charts that should be included to comprehensively assess the results and assumptions to be verified are discussed.
method_label: This article demonstrates the preferred pattern for the application of logistic methods with an illustration of logistic regression applied to a data set in testing a research hypothesis.
method_label: Recommendations are also offered for appropriate reporting formats of logistic regression results and the minimum observation-to-predictor ratio.
result_label: The authors evaluated the use and interpretation of logistic regression presented in 8 articles published in The Journal of Educational Research between 1990 and 2000.
result_label: They found that all 8 studies met or exceeded recommended criteria.

===================================
paper_id: 5204434; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Neural Architectures for Fine-grained Entity Type Classification
ABSTRACT: background_label: In this work, we investigate several neural network architectures for fine-grained entity type classification.
background_label: Particularly, we consider extensions to a recently proposed attentive neural architecture and make three key contributions.
method_label: Previous work on attentive neural architectures do not consider hand-crafted features, we combine learnt and hand-crafted features and observe that they complement each other.
method_label: Additionally, through quantitative analysis we establish that the attention mechanism is capable of learning to attend over syntactic heads and the phrase containing the mention, where both are known strong hand-crafted features for our task.
method_label: We enable parameter sharing through a hierarchical label encoding method, that in low-dimensional projections show clear clusters for each type hierarchy.
method_label: Lastly, despite using the same evaluation dataset, the literature frequently compare models trained using different data.
result_label: We establish that the choice of training data has a drastic impact on performance, with decreases by as much as 9.85% loose micro F1 score for a previously proposed method.
result_label: Despite this, our best model achieves state-of-the-art results with 75.36% loose micro F1 score on the well- established FIGER (GOLD) dataset.

===================================
paper_id: 955946; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: A Discriminatively Trained Fully Connected Conditional Random Field Model for Blood Vessel Segmentation in Fundus Images
ABSTRACT: objective_label: Goal: In this work, we present an extensive description and evaluation of our method for blood vessel segmentation in fundus images based on a discriminatively trained fully connected conditional random field model.
background_label: Methods: Standard segmentation priors such as a Potts model or total variation usually fail when dealing with thin and elongated structures.
method_label: We overcome this difficulty by using a conditional random field model with more expressive potentials, taking advantage of recent results enabling inference of fully connected models almost in real time.
method_label: Parameters of the method are learned automatically using a structured output support vector machine, a supervised technique widely used for structured prediction in a number of machine learning applications.
result_label: Results: Our method, trained with state of the art features, is evaluated both quantitatively and qualitatively on four publicly available datasets: DRIVE, STARE, CHASEDB1, and HRF.
result_label: Additionally, a quantitative comparison with respect to other strategies is included.
result_label: Conclusion: The experimental results show that this approach outperforms other techniques when evaluated in terms of sensitivity, F1-score, G-mean, and Matthews correlation coefficient.
result_label: Additionally, it was observed that the fully connected model is able to better distinguish the desired structures than the local neighborhood-based approach.
result_label: Significance: Results suggest that this method is suitable for the task of segmenting elongated structures, a feature that can be exploited to contribute with other medical and biological applications.

===================================
paper_id: 1264229; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: Random Bits Regression: a Strong General Predictor for Big Data
ABSTRACT: background_label: To improve accuracy and speed of regressions and classifications, we present a data-based prediction method, Random Bits Regression (RBR).
method_label: This method first generates a large number of random binary intermediate/derived features based on the original input matrix, and then performs regularized linear/logistic regression on those intermediate/derived features to predict the outcome.
result_label: Benchmark analyses on a simulated dataset, UCI machine learning repository datasets and a GWAS dataset showed that RBR outperforms other popular methods in accuracy and robustness.
result_label: RBR (available on https://sourceforge.net/projects/rbr/) is very fast and requires reasonable memories, therefore, provides a strong, robust and fast predictor in the big data era.

===================================
paper_id: 14212009; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: PROBE-GK: Predictive Robust Estimation using Generalized Kernels
ABSTRACT: background_label: Many algorithms in computer vision and robotics make strong assumptions about uncertainty, and rely on the validity of these assumptions to produce accurate and consistent state estimates.
background_label: In practice, dynamic environments may degrade sensor performance in predictable ways that cannot be captured with static uncertainty parameters.
method_label: In this paper, we employ fast nonparametric Bayesian inference techniques to more accurately model sensor uncertainty.
method_label: By setting a prior on observation uncertainty, we derive a predictive robust estimator, and show how our model can be learned from sample images, both with and without knowledge of the motion used to generate the data.
result_label: We validate our approach through Monte Carlo simulations, and report significant improvements in localization accuracy relative to a fixed noise model in several settings, including on synthetic data, the KITTI dataset, and our own experimental platform.

===================================
paper_id: 15179402; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: MatchNet: Unifying feature and metric learning for patch-based matching
ABSTRACT: objective_label: Motivated by recent successes on learning feature representations and on learning feature comparison functions, we propose a unified approach to combining both for training a patch matching system.
method_label: Our system, dubbed Match-Net, consists of a deep convolutional network that extracts features from patches and a network of three fully connected layers that computes a similarity between the extracted features.
method_label: To ensure experimental repeatability, we train MatchNet on standard datasets and employ an input sampler to augment the training set with synthetic exemplar pairs that reduce overfitting.
method_label: Once trained, we achieve better computational efficiency during matching by disassembling MatchNet and separately applying the feature computation and similarity networks in two sequential stages.
method_label: We perform a comprehensive set of experiments on standard datasets to carefully study the contributions of each aspect of MatchNet, with direct comparisons to established methods.
result_label: Our results confirm that our unified approach improves accuracy over previous state-of-the-art results on patch matching datasets, while reducing the storage requirement for descriptors.
result_label: We make pre-trained MatchNet publicly available.

===================================
paper_id: 7697974; YEAR: 2001
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Iterative Double Clustering for Unsupervised and Semi-supervised Learning
ABSTRACT: background_label: AbstractWe present a powerful meta-clustering technique called Iterative Double Clustering (IDC).
method_label: The IDC method is a natural extension of the recent Double Clustering (DC) method of Slonim and Tishby that exhibited impressive performance on text categorization tasks [12] .
method_label: Using synthetically generated data we empirically find that whenever the DC procedure is successful in recovering some of the structure hidden in the data, the extended IDC procedure can incrementally compute a significantly more accurate classification.
method_label: IDC is especially advantageous when the data exhibits high attribute noise.
result_label: Our simulation results also show the effectiveness of IDC in text categorization problems.
result_label: Surprisingly, this unsupervised procedure can be competitive with a (supervised) SVM trained with a small training set.
result_label: Finally, we propose a simple and natural extension of IDC for semi-supervised and transductive learning where we are given both labeled and unlabeled examples.

===================================
paper_id: 11187887; YEAR: 2014
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Multilayer Perceptron for Robust Nonlinear Interval Regression Analysis Using Genetic Algorithms
ABSTRACT: background_label: On the basis of fuzzy regression, computational models in intelligence such as neural networks have the capability to be applied to nonlinear interval regression analysis for dealing with uncertain and imprecise data.
background_label: When training data are not contaminated by outliers, computational models perform well by including almost all given training data in the data interval.
background_label: Nevertheless, since training data are often corrupted by outliers, robust learning algorithms employed to resist outliers for interval regression analysis have been an interesting area of research.
background_label: Several approaches involving computational intelligence are effective for resisting outliers, but the required parameters for these approaches are related to whether the collected data contain outliers or not.
method_label: Since it seems difficult to prespecify the degree of contamination beforehand, this paper uses multilayer perceptron to construct the robust nonlinear interval regression model using the genetic algorithm.
method_label: Outliers beyond or beneath the data interval will impose slight effect on the determination of data interval.
result_label: Simulation results demonstrate that the proposed method performs well for contaminated datasets.

===================================
paper_id: 196127; YEAR: 2002
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited
TITLE: Detecting Errors In Corpora Using Support Vector Machines
ABSTRACT: background_label: While the corpus-based research relies on human annotated corpora, it is often said that a non-negligible amount of errors remain even in frequently used corpora such as Penn Treebank.
background_label: Detection of errors in annotated corpora is important for corpus-based natural language processing.
method_label: In this paper, we propose a method to detect errors in corpora using support vector machines (SVMs).
method_label: This method is based on the idea of extracting exceptional elements that violate consistency.
method_label: We propose a method of using SVMs to assign a weight to each element and to find errors in a POS tagged corpus.
method_label: We apply the method to English and Japanese POS-tagged corpora and achieve high precision in detecting errors.

===================================
paper_id: 636054; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Binary Classifier Calibration using an Ensemble of Near Isotonic Regression Models
ABSTRACT: background_label: Learning accurate probabilistic models from data is crucial in many practical tasks in data mining.
objective_label: In this paper we present a new non-parametric calibration method called \textit{ensemble of near isotonic regression} (ENIR).
method_label: The method can be considered as an extension of BBQ, a recently proposed calibration method, as well as the commonly used calibration method based on isotonic regression.
method_label: ENIR is designed to address the key limitation of isotonic regression which is the monotonicity assumption of the predictions.
method_label: Similar to BBQ, the method post-processes the output of a binary classifier to obtain calibrated probabilities.
background_label: Thus it can be combined with many existing classification models.
background_label: We demonstrate the performance of ENIR on synthetic and real datasets for the commonly used binary classification models.
method_label: Experimental results show that the method outperforms several common binary classifier calibration methods.
result_label: In particular on the real data, ENIR commonly performs statistically significantly better than the other methods, and never worse.
method_label: It is able to improve the calibration power of classifiers, while retaining their discrimination power.
result_label: The method is also computationally tractable for large scale datasets, as it is $O(N \log N)$ time, where $N$ is the number of samples.

===================================
paper_id: 49658963; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: Learning Multi-Instance Deep Ranking and Regression Network for Visual House Appraisal
ABSTRACT: objective_label: This paper presents a weakly supervised regression model for the visual house appraisal problem, which aims to predict the value of a house from its photos and textual descriptions (e.g., number of bedrooms).
objective_label: The key idea of our approach is a multi-layer neural network, called multi-instance Deep Ranking and Regression  (MiDRR) net, which jointly solves two coupled tasks: ranking and regression, in the multiple instance setting.
method_label: The network is trained using weakly supervised data, which do not require intensive human annotations.
method_label: We also design a set of human heuristics to promote deep features through imposing constraints over the solution space, e.g., a house with three bedrooms often has a higher value than that with only two bedrooms.
method_label: While these constraints are specific to the studied problem, the developed formula can be easily generalized to the other regression applications.
method_label: For test and evaluation purposes, we collect a comprehensive house image benchmark that includes 900,000 photos from 30,000 houses recently traded in the USA, and apply the proposed MiDRR net to predict house values.
result_label: Extensive evaluations with comparisons demonstrate that additional usage of imagery data as well as human heuristics can significantly boost system performance and that the proposed MiDRR net clearly outperforms the alternative methods.

===================================
paper_id: 88522306; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Online Algorithms For Parameter Mean And Variance Estimation In Dynamic Regression Models
ABSTRACT: background_label: We study the problem of estimating the parameters of a regression model from a set of observations, each consisting of a response and a predictor.
background_label: The response is assumed to be related to the predictor via a regression model of unknown parameters.
background_label: Often, in such models the parameters to be estimated are assumed to be constant.
method_label: Here we consider the more general scenario where the parameters are allowed to evolve over time, a more natural assumption for many applications.
method_label: We model these dynamics via a linear update equation with additive noise that is often used in a wide range of engineering applications, particularly in the well-known and widely used Kalman filter (where the system state it seeks to estimate maps to the parameter values here).
background_label: We derive an approximate algorithm to estimate both the mean and the variance of the parameter estimates in an online fashion for a generic regression model.
background_label: This algorithm turns out to be equivalent to the extended Kalman filter.
method_label: We specialize our algorithm to the multivariate exponential family distribution to obtain a generalization of the generalized linear model (GLM).
method_label: Because the common regression models encountered in practice such as logistic, exponential and multinomial all have observations modeled through an exponential family distribution, our results are used to easily obtain algorithms for online mean and variance parameter estimation for all these regression models in the context of time-dependent parameters.
method_label: Lastly, we propose to use these algorithms in the contextual multi-armed bandit scenario, where so far model parameters are assumed static and observations univariate and Gaussian or Bernoulli.
result_label: Both of these restrictions can be relaxed using the algorithms described here, which we combine with Thompson sampling to show the resulting performance on a simulation.

===================================
paper_id: 51612538; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Dictionary-Free MRI PERK: Parameter Estimation via Regression with Kernels
ABSTRACT: background_label: This paper introduces a fast, general method for dictionary-free parameter estimation in quantitative magnetic resonance imaging (QMRI) via regression with kernels (PERK).
method_label: PERK first uses prior distributions and the nonlinear MR signal model to simulate many parameter-measurement pairs.
method_label: Inspired by machine learning, PERK then takes these parameter-measurement pairs as labeled training points and learns from them a nonlinear regression function using kernel functions and convex optimization.
method_label: PERK admits a simple implementation as per-voxel nonlinear lifting of MRI measurements followed by linear minimum mean-squared error regression.
method_label: We demonstrate PERK for $T_1,T_2$ estimation, a well-studied application where it is simple to compare PERK estimates against dictionary-based grid search estimates.
result_label: Numerical simulations as well as single-slice phantom and in vivo experiments demonstrate that PERK and grid search produce comparable $T_1,T_2$ estimates in white and gray matter, but PERK is consistently at least $23\times$ faster.
result_label: This acceleration factor will increase by several orders of magnitude for full-volume QMRI estimation problems involving more latent parameters per voxel.

===================================
paper_id: 118988729; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: A Microphotonic Astrocomb
ABSTRACT: background_label: One of the essential prerequisites for detection of Earth-like extra-solar planets or direct measurements of the cosmological expansion is the accurate and precise wavelength calibration of astronomical spectrometers.
background_label: It has already been realized that the large number of exactly known optical frequencies provided by laser frequency combs ('astrocombs') can significantly surpass conventionally used hollow-cathode lamps as calibration light sources.
background_label: A remaining challenge, however, is generation of frequency combs with lines resolvable by astronomical spectrometers.
method_label: Here we demonstrate an astrocomb generated via soliton formation in an on-chip microphotonic resonator ('microresonator') with a resolvable line spacing of 23.7 GHz.
method_label: This comb is providing wavelength calibration on the 10 cm/s radial velocity level on the GIANO-B high-resolution near-infrared spectrometer.
result_label: As such, microresonator frequency combs have the potential of providing broadband wavelength calibration for the next-generation of astronomical instruments in planet-hunting and cosmological research.

===================================
paper_id: 6748242; YEAR: 2010
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: BagBoo: a scalable hybrid bagging-the-boosting model
ABSTRACT: background_label: In this paper, we introduce a novel machine learning approach for regression based on the idea of combining bagging and boosting that we call BagBoo.
background_label: Our BagBoo model borrows its high accuracy potential from.
background_label: Friedman's gradient boosting [2], and high efficiency and scalability through parallelism from Breiman's bagging [1].
method_label: We run empirical evaluations on large scale Web ranking data, and demonstrate that BagBoo is not only showing superior relevance than standalone bagging or boosting, but also outperforms most previously published results on these data sets.
result_label: We also emphasize that BagBoo is intrinsically scalable and parallelizable, allowing us to train order of half a million trees on 200 nodes in 2 hours CPU time and beat all of the competitors in the Internet Mathematics relevance competition sponsored by Yandex and be one of the top algorithms in both tracks of Yahoo ICML-2010 challenge.
result_label: We conclude the paper by stating that while impressive experimental evaluation results are presented here in the context of regression trees, the hybrid BagBoo model is applicable to other domains, such as classification, and base training models.

===================================
paper_id: 121896919; YEAR: 1982
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200
TITLE: Robust regression using repeated medians
ABSTRACT: background_label: Abstract : The repeated median algorithm is a robustified U-statistic in which nested medians replace the single mean.
background_label: Unlike many generalizations of the univariate median, repeated median estimates maintain the high 50% breakdown value and can resist the effects of outliers even when they comprise nearly half of the data.
method_label: Because they are calculated directly, not iteratively, repeated median procedures can be used as starting values for iterative robust estimation methods.
result_label: For bivariate linear regression with symmetric errors, repeated median estimates are unbiased and Fisher consistent, and their efficiency under Gaussian sampling can be comparable to the efficiency of the univariate median.
other_label: (Author)

===================================
paper_id: 23592472; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: CLR: coupled logistic regression model for CTR prediction
ABSTRACT: background_label: Online advertisement is a significant element of the Web browsing experience.
background_label: A good advertising can not only bring benefits to publisher but also improve user satisfaction and extends advertiser's product marketing.
background_label: To satisfy the desire of all three parties, the click through rate (CTR) prediction of a user to a specified ad in a specific context is of great importance.
objective_label: This challenging problem plays a key role in online advertising system and has to deal with several hard issues.
method_label: Firstly, the model must process very high dimensional features from frequently changing ad, user and context, most of which are category features having large cardinality and sparse nature extending the dimensionality by two orders of magnitude.
result_label: Secondly, nonlinear features such as conjunction information must be integrated into the model for a better prediction accuracy.
background_label: Finally, the model must be able to parallelized efficiently to train from very large scale data sets.
objective_label: To address these problems, we proposed a novel model called Coupled Logistic Regression (CLR), for accurate and efficient CTR prediction.
method_label: CLR can exploit all features from ad, user, context and nonlinear features among them by seamlessly integrate the conjunction information by employing factorization machine to achieve precise prediction result.
method_label: And the high-dimensional problem is avoided by decomposing the decision function into two sub ones.
method_label: Scalability of CLR is ensured through a newly invited MapReduce parallelization strategy, which can reduce communication and waiting time between nodes.
result_label: Experimental results on real-world data set show that our CLR model can guarantee both accuracy and efficiency on large scale CTR prediction problems.

===================================
paper_id: 15661469; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 1; annotator4: 0; annotator3: 1
sources: specter
TITLE: Unsupervised Document Classification with Informed Topic Models
ABSTRACT: background_label: AbstractDocument classification is an important and common application in natural language processing.
background_label: Scaling classification approaches to many targets faces a bottleneck in acquiring gold standard labels.
method_label: In this work, we develop and evaluate a method for using informed topic models to noisily label documents, creating a noisy but usable set of labels for training discriminative classifiers.
method_label: We investigate multiple ways to train this noisy classifier, and the best performing method uses Wikipedia-seeded topic models to approximately label training instances without any supervision.
result_label: We evaluate these methods on the classification task as well as in an active learning setting, in which they are shown to improve learning rates over traditional active learning.

===================================
paper_id: 118455072; YEAR: 1998
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Modified Likelihood and Related Methods for Handling Nuisance Parameters in the Linear Regression Model
ABSTRACT: background_label: In this paper, different approaches to dealing with nuisance parameters in the likelihood based inference are presented and illustrated by reference to the linear regression model with nonspherical errors.
method_label: The estimator of the error variance using each of the approaches is also derived for the linear regression model with spherical erors.
method_label: We observe that many of these estimators are unbiased.
result_label: A theoretical comparison of the likelihood functions is reported and we note that some of them are equivalent.

===================================
paper_id: 3986579; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf - title_tfidf
TITLE: Privacy-preserving logistic regression training
ABSTRACT: background_label: BACKGROUND Logistic regression is a popular technique used in machine learning to construct classification models.
background_label: Since the construction of such models is based on computing with large datasets, it is an appealing idea to outsource this computation to a cloud service.
background_label: The privacy-sensitive nature of the input data requires appropriate privacy preserving measures before outsourcing it.
method_label: Homomorphic encryption enables one to compute on encrypted data directly, without decryption and can be used to mitigate the privacy concerns raised by using a cloud service.
method_label: METHODS In this paper, we propose an algorithm (and its implementation) to train a logistic regression model on a homomorphically encrypted dataset.
background_label: The core of our algorithm consists of a new iterative method that can be seen as a simplified form of the fixed Hessian method, but with a much lower multiplicative complexity.
method_label: RESULTS We test the new method on two interesting real life applications: the first application is in medicine and constructs a model to predict the probability for a patient to have cancer, given genomic data as input; the second application is in finance and the model predicts the probability of a credit card transaction to be fraudulent.
method_label: The method produces accurate results for both applications, comparable to running standard algorithms on plaintext data.
objective_label: CONCLUSIONS This article introduces a new simple iterative algorithm to train a logistic regression model that is tailored to be applied on a homomorphically encrypted dataset.
method_label: This algorithm can be used as a privacy-preserving technique to build a binary classification model and can be applied in a wide range of problems that can be modelled with logistic regression.
result_label: Our implementation results show that our method can handle the large datasets used in logistic regression training.

===================================
paper_id: 28671436; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter
TITLE: On Calibration of Modern Neural Networks
ABSTRACT: background_label: Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications.
background_label: We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated.
method_label: Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration.
method_label: We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets.
result_label: Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions.

===================================
paper_id: 14332192; YEAR: 2009
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited
TITLE: A survey of cross-validation procedures for model selection
ABSTRACT: background_label: Used to estimate the risk of an estimator or to perform model selection, cross-validation is a widespread strategy because of its simplicity and its apparent universality.
background_label: Many results exist on the model selection performances of cross-validation procedures.
objective_label: This survey intends to relate these results to the most recent advances of model selection theory, with a particular emphasis on distinguishing empirical statements from rigorous theoretical results.
result_label: As a conclusion, guidelines are provided for choosing the best cross-validation procedure according to the particular features of the problem in hand.

===================================
paper_id: 14718765; YEAR: 2006
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: Backoff Model Training using Partially Observed Data: Application to Dialog Act Tagging
ABSTRACT: background_label: Dialog act (DA) tags are useful for many applications in natural language processing and automatic speech recognition.
method_label: In this work, we introduce hidden backoff models (HBMs) where a large generalized backoff model is trained, using an embedded expectation-maximization (EM) procedure, on data that is partially observed.
method_label: We use HBMs as word models conditioned on both DAs and (hidden) DA-segments.
result_label: Experimental results on the ICSI meeting recorder dialog act corpus show that our procedure can strictly increase likelihood on training data and can effectively reduce errors on test data.
result_label: In the best case, test error can be reduced by 6.1% relative to our baseline, an improvement on previously reported models that also use prosody.
result_label: We also compare with our own prosody-based model, and show that our HBM is competitive even without the use of prosody.
result_label: We have not yet succeeded, however, in combining the benefits of both prosody and the HBM.

===================================
paper_id: 6617574; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: Learning Whom to Trust with MACE
ABSTRACT: background_label: AbstractNon-expert annotation services like Amazon's Mechanical Turk (AMT) are cheap and fast ways to evaluate systems and provide categorical annotations for training data.
background_label: Unfortunately, some annotators choose bad labels in order to maximize their pay.
background_label: Manual identification is tedious, so we experiment with an item-response model.
method_label: It learns in an unsupervised fashion to a) identify which annotators are trustworthy and b) predict the correct underlying labels.
method_label: We match performance of more complex state-of-the-art systems and perform well even under adversarial conditions.
method_label: We show considerable improvements over standard baselines, both for predicted label accuracy and trustworthiness estimates.
method_label: The latter can be further improved by introducing a prior on model parameters and using Variational Bayes inference.
result_label: Additionally, we can achieve even higher accuracy by focusing on the instances our model is most confident in (trading in some recall), and by incorporating annotated control instances.
result_label: Our system, MACE (Multi-Annotator Competence Estimation), is available for download 1 .

===================================
paper_id: 6644438; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter
TITLE: A Lazy Man's Approach to Benchmarking: Semisupervised Classifier Evaluation and Recalibration
ABSTRACT: background_label: How many labeled examples are needed to estimate a classifier's performance on a new dataset?
background_label: We study the case where data is plentiful, but labels are expensive.
method_label: We show that by making a few reasonable assumptions on the structure of the data, it is possible to estimate performance curves, with confidence bounds, using a small number of ground truth labels.
method_label: Our approach, which we call Semi supervised Performance Evaluation (SPE), is based on a generative model for the classifier's confidence scores.
method_label: In addition to estimating the performance of classifiers on new datasets, SPE can be used to recalibrate a classifier by re-estimating the class-conditional confidence distributions.

===================================
paper_id: 3247997; YEAR: 2003
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: A robust nonlinear identification algorithm using PRESS statistic and forward regression.
ABSTRACT: background_label: This paper introduces a new robust nonlinear identification algorithm using the predicted residual sums of squares (PRESS) statistic and forward regression.
objective_label: The major contribution is to compute the PRESS statistic within a framework of a forward orthogonalization process and hence construct a model with a good generalization property.
result_label: Based on the properties of the PRESS statistic the proposed algorithm can achieve a fully automated procedure without resort to any other validation data set for iterative model evaluation.

===================================
paper_id: 10039376; YEAR: 2000
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited
TITLE: Support Vector Machine Classification and Validation of Cancer Tissue Samples Using Microarray Expression Data
ABSTRACT: background_label: MOTIVATION DNA microarray experiments generating thousands of gene expression measurements, are being used to gather information from tissue and cell samples regarding gene expression differences that will be useful in diagnosing disease.
method_label: We have developed a new method to analyse this kind of data using support vector machines (SVMs).
method_label: This analysis consists of both classification of the tissue samples, and an exploration of the data for mis-labeled or questionable tissue results.
result_label: RESULTS We demonstrate the method in detail on samples consisting of ovarian cancer tissues, normal ovarian tissues, and other normal tissues.
result_label: The dataset consists of expression experiment results for 97,802 cDNAs for each tissue.
result_label: As a result of computational analysis, a tissue sample is discovered and confirmed to be wrongly labeled.
background_label: Upon correction of this mistake and the removal of an outlier, perfect classification of tissues is achieved, but not with high confidence.
method_label: We identify and analyse a subset of genes from the ovarian dataset whose expression is highly differentiated between the types of tissues.
method_label: To show robustness of the SVM method, two previously published datasets from other types of tissues or cells are analysed.
method_label: The results are comparable to those previously obtained.
result_label: We show that other machine learning methods also perform comparably to the SVM on many of those datasets.
other_label: AVAILABILITY The SVM software is available at http://www.cs.
other_label: columbia.edu/ approximately bgrundy/svm.

===================================
paper_id: 15371885; YEAR: 2009
adju relevance: Irrelevant (0)
difference: 2; annotator4: 0; annotator3: 2
sources: specter
TITLE: Learning with Annotation Noise
ABSTRACT: background_label: It is usually assumed that the kind of noise existing in annotated data is random classification noise.
background_label: Yet there is evidence that differences between annotators are not always random attention slips but could result from different biases towards the classification categories, at least for the harder-to-decide cases.
background_label: Under an annotation generation model that takes this into account, there is a hazard that some of the training instances are actually hard cases with unreliable annotations.
result_label: We show that these are relatively unproblematic for an algorithm operating under the 0--1 loss model, whereas for the commonly used voted perceptron algorithm, hard training cases could result in incorrect prediction on the uncontroversial cases at test time.

===================================
paper_id: 15162836; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Estimating Random Delays in Modbus Network Using Experiments and General Linear Regression Neural Networks with Genetic Algorithm Smoothing
ABSTRACT: background_label: Time-varying delays adversely affect the performance of networked control sys-tems (NCS) and in the worst-case can destabilize the entire system.
background_label: Therefore, modelling network delays is important for designing NCS.
background_label: However, modelling time-varying delays is challenging because of their dependence on multiple pa-rameters such as length, contention, connected devices, protocol employed, and channel loading.
background_label: Further, these multiple parameters are inherently random and de-lays vary in a non-linear fashion with respect to time.
background_label: This makes estimating ran-dom delays challenging.
objective_label: This investigation presents a methodology to model de-lays in NCS using experiments and general regression neural network (GRNN) due to their ability to capture non-linear relationship.
method_label: To compute the optimal smoothing parameter that computes the best estimates, genetic algorithm is used.
method_label: The objective of the genetic algorithm is to compute the optimal smoothing pa-rameter that minimizes the mean absolute percentage error (MAPE).
result_label: Our results illustrate that the resulting GRNN is able to predict the delays with less than 3% error.
result_label: The proposed delay model gives a framework to design compensation schemes for NCS subjected to time-varying delays.

===================================
paper_id: 975467; YEAR: 1999
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited
TITLE: An adaptive version of the boost by majority algorithm
ABSTRACT: objective_label: We propose a new boosting algorithm.
method_label: This boosting algorithm is an adaptive version of the boost by majority algorithm and combines bounded goals of the boost by majority algorithm with the adaptivity of AdaBoost.The method used for making boost-by-majority adaptive is to consider the limit in which each of the boosting iterations makes an infinitesimally small contribution to the process as a whole.
method_label: This limit can be modeled using the differential equations that govern Brownian motion.
method_label: The new boosting algorithm, named BrownBoost, is based on finding solutions to these differential equations.The paper describes two methods for finding approximate solutions to the differential equations.
method_label: The first is a method that results in a provably polynomial time algorithm.
result_label: The second method, based on the Newton-Raphson minimization procedure, is much more efficient in practice but is not known to be polynomial.

===================================
paper_id: 8388284; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 1; annotator4: 0; annotator3: 1
sources: specter
TITLE: Combining Generative and Discriminative Model Scores for Distant Supervision
ABSTRACT: background_label: AbstractDistant supervision is a scheme to generate noisy training data for relation extraction by aligning entities of a knowledge base with text.
method_label: In this work we combine the output of a discriminative at-least-one learner with that of a generative hierarchical topic model to reduce the noise in distant supervision data.
method_label: The combination significantly increases the ranking quality of extracted facts and achieves state-of-the-art extraction performance in an end-to-end setting.
result_label: A simple linear interpolation of the model scores performs better than a parameter-free scheme based on nondominated sorting.

===================================
paper_id: 16358367; YEAR: 2012
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter
TITLE: Learning to count with regression forest and structured labels
ABSTRACT: background_label: Following [Lempitsky and Zisserman, 2010], we seek to count objects by integrating over an object density map that is predicted from an input image.
method_label: In contrast to that work, we propose to estimate the object density map by averaging over structured, namely patch-wise, predictions.
result_label: Using an ensemble of randomized regression trees that use dense features as input, we obtain results that are of similar quality, at a fraction of the training time, and with low implementation effort.
other_label: An open source implementation will be provided in the framework of http://ilastik.org.

===================================
paper_id: 15716093; YEAR: 2009
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Asymptotic distribution and sparsistency for l1-penalized parametric M-estimators with applications to linear SVM and logistic regression
ABSTRACT: background_label: Since its early use in least squares regression problems, the l1-penalization framework for variable selection has been employed in conjunction with a wide range of loss functions encompassing regression, classification and survival analysis.
background_label: While a well developed theory exists for the l1-penalized least squares estimates, few results concern the behavior of l1-penalized estimates for general loss functions.
objective_label: In this paper, we derive two results concerning penalized estimates for a wide array of penalty and loss functions.
method_label: Our first result characterizes the asymptotic distribution of penalized parametric M-estimators under mild conditions on the loss and penalty functions in the classical setting (fixed-p-large-n).
method_label: Our second result explicits necessary and sufficient generalized irrepresentability (GI) conditions for l1-penalized parametric M-estimates to consistently select the components of a model (sparsistency) as well as their sign (sign consistency).
method_label: In general, the GI conditions depend on the Hessian of the risk function at the true value of the unknown parameter.
method_label: Under Gaussian predictors, we obtain a set of conditions under which the GI conditions can be re-expressed solely in terms of the second moment of the predictors.
method_label: We apply our theory to contrast l1-penalized SVM and logistic regression classifiers and find conditions under which they have the same behavior in terms of their model selection consistency (sparsistency and sign consistency).
result_label: Finally, we provide simulation evidence for the theory based on these classification examples.

===================================
paper_id: 8421022; YEAR: 2006
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Detecting outliers when fitting data with nonlinear regression – a new method based on robust nonlinear regression and the false discovery rate
ABSTRACT: background_label: BackgroundNonlinear regression, like linear regression, assumes that the scatter of data around the ideal curve follows a Gaussian or normal distribution.
background_label: This assumption leads to the familiar goal of regression: to minimize the sum of the squares of the vertical or Y-value distances between the points and the curve.
background_label: Outliers can dominate the sum-of-the-squares calculation, and lead to misleading results.
method_label: However, we know of no practical method for routinely identifying outliers when fitting curves with nonlinear regression.ResultsWe describe a new method for identifying outliers when fitting data with nonlinear regression.
method_label: We first fit the data using a robust form of nonlinear regression, based on the assumption that scatter follows a Lorentzian distribution.
method_label: We devised a new adaptive method that gradually becomes more robust as the method proceeds.
method_label: To define outliers, we adapted the false discovery rate approach to handling multiple comparisons.
method_label: We then remove the outliers, and analyze the data using ordinary least-squares regression.
method_label: Because the method combines robust regression and outlier removal, we call it the ROUT method.When analyzing simulated data, where all scatter is Gaussian, our method detects (falsely) one or more outlier in only about 1–3% of experiments.
result_label: When analyzing data contaminated with one or several outliers, the ROUT method performs well at outlier identification, with an average False Discovery Rate less than 1%.ConclusionOur method, which combines a new method of robust nonlinear regression with a new method of outlier identification, identifies outliers from nonlinear curve fits with reasonable power and few false positives.

===================================
paper_id: 119425731; YEAR: 1972
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: Unzerlegbare Darstellungen I
ABSTRACT: background_label: LetK be the structure got by forgetting the composition law of morphisms in a given category.
background_label: A linear representation ofK is given by a map V associating with any morphism ϕ: a→e ofK a linear vector space map V(ϕ): V(a)→V(e).
method_label: We classify thoseK having only finitely many isomorphy classes of indecomposable linear representations.
other_label: This classification is related to an old paper by Yoshii [3].

===================================
paper_id: 9630356; YEAR: 2007
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Density-Driven Generalized Regression Neural Networks (DD-GRNN) for Function Approximation
ABSTRACT: objective_label: This paper proposes a new nonparametric regression method, based on the combination of generalized regression neural networks (GRNNs), density-dependent multiple kernel bandwidths, and regularization.
method_label: The presented model is generic and substitutes the very large number of bandwidths with a much smaller number of trainable weights that control the regression model.
method_label: It depends on sets of extracted data density features which reflect the density properties and distribution irregularities of the training data sets.
method_label: We provide an efficient initialization scheme and a second-order algorithm to train the model, as well as an overfitting control mechanism based on Bayesian regularization.
result_label: Numerical results show that the proposed network manages to reduce significantly the computational demands of having individual bandwidths, while at the same time, provides competitive function approximation accuracy in relation to existing methods.

===================================
paper_id: 63808179; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf - abs_tfidf
TITLE: Logistic Regression A Self Learning Text
ABSTRACT: background_label: Thank you for downloading logistic regression a self learning text.
background_label: As you may know, people have search numerous times for their favorite novels like this logistic regression a self learning text, but end up in infectious downloads.
background_label: Rather than reading a good book with a cup of tea in the afternoon, instead they juggled with some malicious bugs inside their desktop computer.
method_label: logistic regression a self learning text is available in our digital library an online access to it is set as public so you can get it instantly.
method_label: Our book servers saves in multiple countries, allowing you to get the most less latency time to download any of our books like this one.
result_label: Kindly say, the logistic regression a self learning text is universally compatible with any devices to read.

===================================
paper_id: 38586692; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Online sparse Gaussian process regression using FITC and PITC approximations
ABSTRACT: background_label: Abstract We provide a method which allows for online updating of sparse Gaussian Process (GP) regression algorithms for any set of inducing inputs.
method_label: This method is derived both for the Fully Independent Training Conditional (FITC) and the Partially Independent Training Conditional (PITC) approximation, and it allows the inclusion of a new measurement point xn+1 in O(m2) time, with m denoting the size of the set of inducing inputs.
method_label: Due to the online nature of the algorithms, it is possible to forget earlier measurement data, which means that also the memory space required is O(m2), both for FITC and PITC.
result_label: We show that this method is able to efficiently apply GP regression to a large data set with accurate results.

===================================
paper_id: 18358569; YEAR: 2002
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited
TITLE: Gate: An architecture for development of robust hlt applications
ABSTRACT: background_label: In this paper we present GATE, a framework and graphical development environment which enables users to develop and deploy language engineering components and resources in a robust fashion.
method_label: The GATE architecture has enabled us not only to develop a number of successful applications for various language processing tasks (such as Information Extraction), but also to build and annotate corpora and carry out evaluations on the applications generated.
method_label: The framework can be used to develop applications and resources in multiple languages, based on its thorough Unicode support.

===================================
paper_id: 4591284; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200
TITLE: Unsupervised Feature Learning via Non-parametric Instance Discrimination
ABSTRACT: background_label: Neural net classifiers trained on data with annotated class labels can also capture apparent visual similarity among categories without being directed to do so.
objective_label: We study whether this observation can be extended beyond the conventional domain of supervised learning: Can we learn a good feature representation that captures apparent similarity among instances, instead of classes, by merely asking the feature to be discriminative of individual instances?
method_label: We formulate this intuition as a non-parametric classification problem at the instance-level, and use noise-contrastive estimation to tackle the computational challenges imposed by the large number of instance classes.
result_label: Our experimental results demonstrate that, under unsupervised learning settings, our method surpasses the state-of-the-art on ImageNet classification by a large margin.
result_label: Our method is also remarkable for consistently improving test performance with more training data and better network architectures.
result_label: By fine-tuning the learned feature, we further obtain competitive results for semi-supervised learning and object detection tasks.
result_label: Our non-parametric model is highly compact: With 128 features per image, our method requires only 600MB storage for a million images, enabling fast nearest neighbour retrieval at the run time.

===================================
paper_id: 5709470; YEAR: 2004
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: cited
TITLE: Distinguishing Mislabeled Data from Correctly Labeled Data in Classifier Design
ABSTRACT: method_label: We have developed a method for distinguishing between correctly labeled and mislabeled data sampled from video sequences and used in the construction of a facial expression recognition classifier.
method_label: The novelty of our approach lies in training a single, optimal classifier type (a support vector machine, or SVM) on multiple representations of the data, involving different "discriminating" subspaces.
method_label: Results of a preliminary study on the discrimination of "high stress" vs. "low stress" facial expression data by this method confirms that our novel approach is able to distinguish subproblems where labeling is highly reliable from those where mislabeling can lead to high error rates.
result_label: In helping detect data subsamples which yield misleading classification results, the method is also a rapid, highly efficient cross-validated approach for eliminating outliers.

===================================
paper_id: 7172528; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200
TITLE: Deep Learning Earth Observation Classification Using ImageNet Pretrained Networks
ABSTRACT: background_label: Deep learning methods such as convolutional neural networks (CNNs) can deliver highly accurate classification results when provided with large enough data sets and respective labels.
background_label: However, using CNNs along with limited labeled data can be problematic, as this leads to extensive overfitting.
method_label: In this letter, we propose a novel method by considering a pretrained CNN designed for tackling an entirely different classification problem, namely, the ImageNet challenge, and exploit it to extract an initial set of representations.
method_label: The derived representations are then transferred into a supervised CNN classifier, along with their class labels, effectively training the system.
method_label: Through this two-stage framework, we successfully deal with the limited-data problem in an end-to-end processing scheme.
result_label: Comparative results over the UC Merced Land Use benchmark prove that our method significantly outperforms the previously best stated results, improving the overall accuracy from 83.1% up to 92.4%.
result_label: Apart from statistical improvements, our method introduces a novel feature fusion algorithm that effectively tackles the large data dimensionality by using a simple and computationally efficient approach.

===================================
paper_id: 14290733; YEAR: 2011
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter
TITLE: Random Forest Based Feature Induction
ABSTRACT: background_label: We propose a simple yet effective strategy to induce a task dependent feature representation using ensembles of random decision trees.
background_label: The new feature mapping is efficient in space and time, and provides a metric transformation that is non parametric and not implicit in nature (i.e.
method_label: not expressed via a kernel matrix), nor limited to the transductive setup.
method_label: The main advantage of the proposed mapping lies in its flexibility to adapt to several types of learning tasks ranging from regression to multi-label classification, and to deal in a natural way with missing values.
result_label: Finally, we provide an extensive empirical study of the properties of the learned feature representation over real and artificial datasets.

===================================
paper_id: 1936327; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200 - abs_tfidfcbow200
TITLE: Automatic Image Annotation via Label Transfer in the Semantic Space
ABSTRACT: background_label: Automatic image annotation is among the fundamental problems in computer vision and pattern recognition, and it is becoming increasingly important in order to develop algorithms that are able to search and browse large-scale image collections.
objective_label: In this paper, we propose a label propagation framework based on Kernel Canonical Correlation Analysis (KCCA), which builds a latent semantic space where correlation of visual and textual features are well preserved into a semantic embedding.
method_label: The proposed approach is robust and can work either when the training set is well annotated by experts, as well as when it is noisy such as in the case of user-generated tags in social media.
method_label: We report extensive results on four popular datasets.
result_label: Our results show that our KCCA-based framework can be applied to several state-of-the-art label transfer methods to obtain significant improvements.
result_label: Our approach works even with the noisy tags of social users, provided that appropriate denoising is performed.
result_label: Experiments on a large scale setting show that our method can provide some benefits even when the semantic space is estimated on a subset of training images.

===================================
paper_id: 85517077; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Combination of Multiple Global Descriptors for Image Retrieval
ABSTRACT: background_label: Recent studies in image retrieval task have shown that ensembling different models and combining multiple global descriptors lead to performance improvement.
background_label: However, training different models for the ensemble is not only difficult but also inefficient with respect to time and memory.
objective_label: In this paper, we propose a novel framework that exploits multiple global descriptors to get an ensemble effect while it can be trained in an end-to-end manner.
method_label: The proposed framework is flexible and expandable by the global descriptor, CNN backbone, loss, and dataset.
method_label: Moreover, we investigate the effectiveness of combining multiple global descriptors with quantitative and qualitative analysis.
result_label: Our extensive experiments show that the combined descriptor outperforms a single global descriptor, as it can utilize different types of feature properties.
result_label: In the benchmark evaluation, the proposed framework achieves the state-of-the-art performance on the CARS196, CUB200-2011, In-shop Clothes, and Stanford Online Products on image retrieval tasks.
result_label: Our model implementations and pretrained models are publicly available.

===================================
paper_id: 22101076; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Convergent Time-Varying Regression Models for Data Streams: Tracking Concept Drift by the Recursive Parzen-Based Generalized Regression Neural Networks.
ABSTRACT: background_label: One of the greatest challenges in data mining is related to processing and analysis of massive data streams.
background_label: Contrary to traditional static data mining problems, data streams require that each element is processed only once, the amount of allocated memory is constant and the models incorporate changes of investigated streams.
background_label: A vast majority of available methods have been developed for data stream classification and only a few of them attempted to solve regression problems, using various heuristic approaches.
method_label: In this paper, we develop mathematically justified regression models working in a time-varying environment.
method_label: More specifically, we study incremental versions of generalized regression neural networks, called IGRNNs, and we prove their tracking properties - weak (in probability) and strong (with probability one) convergence assuming various concept drift scenarios.
method_label: First, we present the IGRNNs, based on the Parzen kernels, for modeling stationary systems under nonstationary noise.
method_label: Next, we extend our approach to modeling time-varying systems under nonstationary noise.
method_label: We present several types of concept drifts to be handled by our approach in such a way that weak and strong convergence holds under certain conditions.
method_label: Finally, in the series of simulations, we compare our method with commonly used heuristic approaches, based on forgetting mechanism or sliding windows, to deal with concept drift.
result_label: Finally, we apply our concept in a real life scenario solving the problem of currency exchange rates prediction.

===================================
paper_id: 121586459; YEAR: 1986
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Optimally bounded score functions for generalized linear models with applications to logistic regression
ABSTRACT: background_label: SUMMARY We study optimally bounded score functions for estimating regression parameters in a generalized linear model.
background_label: Our work extends results obtained by Krasker & Welsch (1982) for the linear model and provides a simple proof of Krasker & Welsch's first-order condition for strong optimality.
method_label: The application of these results to logistic regression is studied in some detail with an example given comparing the bounded-influence estimator with maximum likelihood.
other_label: where h( .
method_label: ), q( . )
method_label: and c( . )
method_label: are known functions and 0 is a vector of regression parameters.
result_label: Models of this type include logistic and probit regression, Poisson regression, linear regression with known variance, and certain models for lifetime data.
background_label: Our motivation for seeking robust estimators is the same as that in the linear model; maximum likelihood estimation is sometimes sensitive to outlying data.
background_label: For logistic regression, Pregibon (1981,,1982) has documented the nonrobustness of the maximum likelihood estimator and expounded the benefits of diagnostics as well as robust or resistant fitting procedures; see also Johnson (1985).
background_label: Much of the work on robust estimation concerns finding estimators which sacrifice little efficiency at the assumed model while providing protection against outliers and model violations.
method_label: We follow this course finding bounded-influence estimators minimizing certain functionals of the asymptotic covariance matrix.
method_label: Related work includes that of Hampel (1978), Krasker (1980) and Krasker & Welsch (1982).
method_label: When fitting models to data, two important issues are identification of outliers and influential cases and accommodation of these observations.
result_label: Frequently when influential cases are present, the fitted model is not representative of the bulk of the data.
result_label: To rectify this, one can simply delete influential cases and refit via standard methods, but this

===================================
paper_id: 4744281; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200
TITLE: Scalable Factorized Hierarchical Variational Autoencoder Training
ABSTRACT: background_label: Deep generative models have achieved great success in unsupervised learning with the ability to capture complex nonlinear relationships between latent generating factors and observations.
background_label: Among them, a factorized hierarchical variational autoencoder (FHVAE) is a variational inference-based model that formulates a hierarchical generative process for sequential data.
background_label: Specifically, an FHVAE model can learn disentangled and interpretable representations, which have been proven useful for numerous speech applications, such as speaker verification, robust speech recognition, and voice conversion.
method_label: However, as we will elaborate in this paper, the training algorithm proposed in the original paper is not scalable to datasets of thousands of hours, which makes this model less applicable on a larger scale.
method_label: After identifying limitations in terms of runtime, memory, and hyperparameter optimization, we propose a hierarchical sampling training algorithm to address all three issues.
method_label: Our proposed method is evaluated comprehensively on a wide variety of datasets, ranging from 3 to 1,000 hours and involving different types of generating factors, such as recording conditions and noise types.
method_label: In addition, we also present a new visualization method for qualitatively evaluating the performance with respect to the interpretability and disentanglement.
result_label: Models trained with our proposed algorithm demonstrate the desired characteristics on all the datasets.

===================================
paper_id: 51614438; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200
TITLE: Bioimage Classification with Handcrafted and Learned Features
ABSTRACT: background_label: Bioimage classification is increasingly becoming more important in many biological studies including those that require accurate cell phenotype recognition, subcellular localization, and histopathological classification.
objective_label: In this paper, we present a new General Purpose GenP bioimage classification method that can be applied to a large range of classification problems.
method_label: The GenP system we propose is an ensemble that combines multiple texture features both handcrafted and learned descriptors for superior and generalizable discriminative power.
method_label: Our ensemble obtains a boosting of performance by combining local features, dense sampling features, and deep learning features.
method_label: Each descriptor is used to train a different Support Vector Machine that is then combined by sum rule.
result_label: We evaluate our method on a diverse set of bioimage classification tasks each represented by a benchmark database, including some of those available in the IICBU 2008 database.
method_label: Each bioimage classification task represents a typical subcellular, cellular, and tissue level classification problem.
result_label: Our evaluation on these datasets demonstrates that the proposed GenP bioimage ensemble obtains state-of-the-art performance without any ad-hoc dataset tuning of the parameters thereby avoiding any risk of overfitting/overtraining.
other_label: To reproduce the experiments reported in this paper, the MATLAB code of all the descriptors is available at https://github.com/LorisNanni and https://www.dropbox.com/s/bguw035yrqz0pwp/ElencoCode.docx?dl=0.

===================================
paper_id: 5244262; YEAR: 2011
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: Learning nonlinear distance functions using neural network for regression with application to robust human age estimation
ABSTRACT: method_label: In this paper, a robust regression method is proposed for human age estimation, in which, outlier samples are corrected by their neighbors, through asymptotically increasing the correlation coefficients between the desired distances and the distances of sample labels.
method_label: As another extension, we adopt a nonlinear distance function and approximate it by neural network.
result_label: For fair comparison, we also experiment on the regression problem of age estimation from face images, and the results are very competitive among the state of the art.

===================================
paper_id: 3666085; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200 - abs_cbow200
TITLE: Neural Ranking Models with Weak Supervision
ABSTRACT: background_label: Despite the impressive improvements achieved by unsupervised deep neural networks in computer vision and NLP tasks, such improvements have not yet been observed in ranking for information retrieval.
background_label: The reason may be the complexity of the ranking problem, as it is not obvious how to learn from queries and documents when no supervised signal is available.
objective_label: Hence, in this paper, we propose to train a neural ranking model using weak supervision, where labels are obtained automatically without human annotators or any external resources (e.g., click data).
method_label: To this aim, we use the output of an unsupervised ranking model, such as BM25, as a weak supervision signal.
method_label: We further train a set of simple yet effective ranking models based on feed-forward neural networks.
method_label: We study their effectiveness under various learning scenarios (point-wise and pair-wise models) and using different input representations (i.e., from encoding query-document pairs into dense/sparse vectors to using word embedding representation).
method_label: We train our networks using tens of millions of training instances and evaluate it on two standard collections: a homogeneous news collection(Robust) and a heterogeneous large-scale web collection (ClueWeb).
result_label: Our experiments indicate that employing proper objective functions and letting the networks to learn the input representation based on weakly supervised data leads to impressive performance, with over 13% and 35% MAP improvements over the BM25 model on the Robust and the ClueWeb collections.
result_label: Our findings also suggest that supervised neural ranking models can greatly benefit from pre-training on large amounts of weakly labeled data that can be easily obtained from unsupervised IR models.

===================================
paper_id: 16911166; YEAR: 2012
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter
TITLE: Semisupervised Classifier Evaluation and Recalibration
ABSTRACT: background_label: How many labeled examples are needed to estimate a classifier's performance on a new dataset?
background_label: We study the case where data is plentiful, but labels are expensive.
method_label: We show that by making a few reasonable assumptions on the structure of the data, it is possible to estimate performance curves, with confidence bounds, using a small number of ground truth labels.
method_label: Our approach, which we call Semisupervised Performance Evaluation (SPE), is based on a generative model for the classifier's confidence scores.
method_label: In addition to estimating the performance of classifiers on new datasets, SPE can be used to recalibrate a classifier by re-estimating the class-conditional confidence distributions.

===================================
paper_id: 166228660; YEAR: 2019
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidfcbow200
TITLE: On Mixup Training: Improved Calibration and Predictive Uncertainty for Deep Neural Networks
ABSTRACT: background_label: Mixup~\cite{zhang2017mixup} is a recently proposed method for training deep neural networks where additional samples are generated during training by convexly combining random pairs of images and their associated labels.
background_label: While simple to implement, it has shown to be a surprisingly effective method of data augmentation for image classification; DNNs trained with mixup show noticeable gains in classification performance on a number of image classification benchmarks.
objective_label: In this work, we discuss a hitherto untouched aspect of mixup training -- the calibration and predictive uncertainty of models trained with mixup.
method_label: We find that DNNs trained with mixup are significantly better calibrated -- i.e., the predicted softmax scores are much better indicators of the actual likelihood of a correct prediction -- than DNNs trained in the regular fashion.
method_label: We conduct experiments on a number of image classification architectures and datasets -- including large-scale datasets like ImageNet -- and find this to be the case.
result_label: Additionally, we find that merely mixing features does not result in the same calibration benefit and that the label smoothing in mixup training plays a significant role in improving calibration.
result_label: Finally, we also observe that mixup-trained DNNs are less prone to over-confident predictions on out-of-distribution and random-noise data.
result_label: We conclude that the typical overconfidence seen in neural networks, even on in-distribution data is likely a consequence of training with hard labels, suggesting that mixup training be employed for classification tasks where predictive uncertainty is a significant concern.

===================================
paper_id: 1037247; YEAR: 2011
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Efficient Learning of Generalized Linear and Single Index Models with Isotonic Regression
ABSTRACT: background_label: Generalized Linear Models (GLMs) and Single Index Models (SIMs) provide powerful generalizations of linear regression, where the target variable is assumed to be a (possibly unknown) 1-dimensional function of a linear predictor.
background_label: In general, these problems entail non-convex estimation procedures, and, in practice, iterative local search heuristics are often used.
method_label: Kalai and Sastry (2009) recently provided the first provably efficient method for learning SIMs and GLMs, under the assumptions that the data are in fact generated under a GLM and under certain monotonicity and Lipschitz constraints.
method_label: However, to obtain provable performance, the method requires a fresh sample every iteration.
method_label: In this paper, we provide algorithms for learning GLMs and SIMs, which are both computationally and statistically efficient.
result_label: We also provide an empirical study, demonstrating their feasibility in practice.

===================================
paper_id: 9947617; YEAR: 2013
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_cbow200
TITLE: Domain Adaptation for Structured Regression
ABSTRACT: background_label: Discriminative regression models have proved effective for many vision applications (here we focus on 3D full-body and head pose estimation from image and depth data).
background_label: However, dataset bias is common and is able to significantly degrade the performance of a trained model on target test sets.
background_label: As we show, covariate shift, a form of unsupervised domain adaptation (USDA), can be used to address certain biases in this setting, but is unable to deal with more severe structural biases in the data.
method_label: We propose an effective and efficient semi-supervised domain adaptation (SSDA) approach for addressing such more severe biases in the data.
method_label: Proposed SSDA is a generalization of USDA, that is able to effectively leverage labeled data in the target domain when available.
method_label: Our method amounts to projecting input features into a higher dimensional space (by construction well suited for domain adaptation) and estimating weights for the training samples based on the ratio of test and train marginals in that space.
background_label: The resulting augmented weighted samples can then be used to learn a model of choice, alleviating the problems of bias in the data; as an example, we introduce SSDA twin Gaussian process regression (SSDA-TGP) model.
method_label: With this model we also address the issue of data sharing, where we are able to leverage samples from certain activities (e.g., walking, jogging) to improve predictive performance on very different activities (e.g., boxing).
method_label: In addition, we analyze the relationship between domain similarity and effectiveness of proposed USDA versus SSDA methods.
method_label: Moreover, we propose a computationally efficient alternative to TGP (Bo and Sminchisescu 2010), and it’s variants, called the direct TGP.
result_label: We show that our model outperforms a number of baselines, on two public datasets: HumanEva and ETH Face Pose Range Image Dataset.
result_label: We can also achieve 8–15 times speedup in computation time, over the traditional formulation of TGP, using the proposed direct formulation, with little to no loss in performance.

===================================
paper_id: 120131744; YEAR: 2001
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf - title_tfidf
TITLE: Applied Logistic Regression Analysis
ABSTRACT: other_label: Series Editor's Introduction Author's Introduction to the Second Edition 1.
background_label: Linear Regression and Logistic Regression Model 2.
background_label: Summary Statistics for Evaluating the Logistic Regression Model 3.
method_label: Interpreting the Logistic Regression Coefficients 4.
other_label: An Introduction to Logistic Regression Diagnosis Ch 5.
other_label: Polytomous Logistic Regression and Alternatives to Logistic Regression 6.
other_label: Notes Appendix A References Tables Figures

===================================
paper_id: 52011076; YEAR: 2018
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf
TITLE: Distantly Supervised NER with Partial Annotation Learning and Reinforcement Learning
ABSTRACT: background_label: AbstractA bottleneck problem with Chinese named entity recognition (NER) in new domains is the lack of annotated data.
method_label: One solution is to utilize the method of distant supervision, which has been widely used in relation extraction, to automatically populate annotated training data without humancost.
method_label: The distant supervision assumption here is that if a string in text is included in a predefined dictionary of entities, the string might be an entity.
background_label: However, this kind of auto-generated data suffers from two main problems: incomplete and noisy annotations, which affect the performance of NER models.
objective_label: In this paper, we propose a novel approach which can partially solve the above problems of distant supervision for NER.
method_label: In our approach, to handle the incomplete problem, we apply partial annotation learning to reduce the effect of unknown labels of characters.
method_label: As for noisy annotation, we design an instance selector based on reinforcement learning to distinguish positive sentences from auto-generated annotations.
method_label: In experiments, we create two datasets for Chinese named entity recognition in two domains with the help of distant supervision.
result_label: The experimental results show that the proposed approach obtains better performance than the comparison systems on both two datasets.

===================================
paper_id: 122361533; YEAR: 2006
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Locally Adaptive Semiparametric Estimation of the Mean and Variance Functions in Regression Models
ABSTRACT: background_label: This article proposes a Bayesian method for estimating a heteroscedastic regression model with Gaussian errors, where the mean and the log variance are modeled as linear combinations of explanatory variables.
method_label: We use Bayesian variable selection priors and model averaging to make the estimation more efficient.
method_label: The model is made semiparametric by allowing explanatory variables to enter the mean and log variance flexibly by representing a covariate effect as a linear combination of basis functions.
method_label: Our methodology for estimating flexible effects is locally adaptive in the sense that it works well when the flexible effects vary rapidly in some parts of the predictor space but only slowly in other parts.
method_label: Our article develops an efficient Markov chain Monte Carlo simulation method to sample from the posterior distribution and applies the methodology to a number of simulated and real examples.

===================================
paper_id: 82456167; YEAR: 2007
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: Janeway's Immunobiology
ABSTRACT: background_label: Part I An Introduction to Immunobiology and Innate Immunity 1.
background_label: Basic Concepts in Immunology 2.
background_label: Innate Immunity Part II The Recognition of Antigen 3.
background_label: Antigen Recognition by B-cell and T-cell Receptors 4.
method_label: The Generation of Lymphocyte Antigen Receptors 5.
method_label: Antigen Presentation to T Lymphocytes Part III The Development of Mature Lymphocyte Receptor Repertoires 6.
method_label: Signaling Through Immune System Receptors 7.
result_label: The Development and Survival of Lymphocytes Part IV The Adaptive Immune Response 8.
background_label: T Cell-Mediated Immunity 9.
background_label: The Humoral Immune Response 10.
background_label: Dynamics of Adaptive Immunity 11.
other_label: The Mucosal Immune System Part V The Immune System in Health and Disease 12.
background_label: Failures of Host Defense Mechanism 13.
other_label: Allergy and Hypersensitivity 14.
other_label: Autoimmunity and Transplantation 15.
other_label: Manipulation of the Immune Response Part VI The Origins of Immune Responses 16.
other_label: Evolution of the Immune System Appendix I Immunologists' Toolbox Appendix II CD Antigens Appendix III Cytokines and their Receptors Appendix IV Chemokines and their Receptors Appendix V Immunological Constants

===================================
paper_id: 8229346; YEAR: 2015
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter
TITLE: New Transfer Learning Techniques for Disparate Label Sets
ABSTRACT: background_label: AbstractIn natural language understanding (NLU), a user utterance can be labeled differently depending on the domain or application (e.g., weather vs. calendar).
background_label: Standard domain adaptation techniques are not directly applicable to take advantage of the existing annotations because they assume that the label set is invariant.
method_label: We propose a solution based on label embeddings induced from canonical correlation analysis (CCA) that reduces the problem to a standard domain adaptation task and allows use of a number of transfer learning techniques.
method_label: We also introduce a new transfer learning technique based on pretraining of hidden-unit CRFs (HUCRFs).
result_label: We perform extensive experiments on slot tagging on eight personal digital assistant domains and demonstrate that the proposed methods are superior to strong baselines.

===================================
paper_id: 8016815; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf - title_tfidfcbow200 - title_tfidf
TITLE: Machine Learning, Linear and Bayesian Models for Logistic Regression in Failure Detection Problems
ABSTRACT: background_label: In this work, we study the use of logistic regression in manufacturing failures detection.
method_label: As a data set for the analysis, we used the data from Kaggle competition Bosch Production Line Performance.
method_label: We considered the use of machine learning, linear and Bayesian models.
method_label: For machine learning approach, we analyzed XGBoost tree based classifier to obtain high scored classification.
method_label: Using the generalized linear model for logistic regression makes it possible to analyze the influence of the factors under study.
method_label: The Bayesian approach for logistic regression gives the statistical distribution for the parameters of the model.
result_label: It can be useful in the probabilistic analysis, e.g.
result_label: risk assessment.

===================================
paper_id: 53346974; YEAR: 2000
adju relevance: Irrelevant (0)
difference: 2; annotator4: 2; annotator3: 0
sources: title_tfidf - title_cbow200 - abs_tfidf
TITLE: Robust Logistic Regression for Binomial Responses
ABSTRACT: background_label: n this paper robustness properties of the maximum likelihood estimator (MLE) and several robust estimators for the logistic regression model when the responses are binary are analysed analytically by means of the Influence Function (IF) and empirically by means of simulations.
background_label: It is found that the MLE and the classical Rao’s score test can be misleading in the presence of model misspecification which in the context of logistic regression means either misclassification errors in the responses or extreme data points in the design space.
method_label: A general framework for robust estimation and testing is presented and a robust estimator as well as a robust testing procedure are presented.
result_label: It is shown that they are less influenced by model misspecifications than their classical counterparts and they are applied to the analysis of binary data from a study on breastfeeding.

===================================
paper_id: 36117198; YEAR: 2017
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_tfidf
TITLE: DeepMind_Commentary
ABSTRACT: background_label: We agree with Lake and colleagues on their list of key ingredients for building humanlike intelligence, including the idea that model-based reasoning is essential.
background_label: However, we favor an approach that centers on one additional ingredient: autonomy.
objective_label: In particular, we aim toward agents that can both build and exploit their own internal models, with minimal human hand-engineering.
method_label: We believe an approach centered on autonomous learning has the greatest chance of success as we scale toward real-world complexity, tackling domains for which ready-made formal models are not available.
result_label: Here we survey several important examples of the progress that has been made toward building autonomous agents with humanlike abilities, and highlight some outstanding challenges.

===================================
paper_id: 9858052; YEAR: 2009
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: abs_tfidf - title_tfidf
TITLE: Compression and Aggregation for Logistic Regression Analysis in Data Cubes
ABSTRACT: background_label: Logistic regression is an important technique for analyzing and predicting data with categorical attributes.
background_label: In this paper, We consider supporting online analytical processing (OLAP) of logistic regression analysis for multi-dimensional data in a data cube where it is expensive in time and space to build logistic regression models for each cell from the raw data.
objective_label: We propose a novel scheme to compress the data in such a way that we can reconstruct logistic regression models to answer any OLAP query without accessing the raw data.
method_label: Based on a first-order approximation to the maximum likelihood estimating equations, we develop a compression scheme that compresses each base cell into a small compressed data block with essential information to support the aggregation of logistic regression models.
method_label: Aggregation formulae for deriving high-level logistic regression models from lower level component cells are given.
method_label: We prove that the compression is nearly lossless in the sense that the aggregated estimator deviates from the true model by an error that is bounded and approaches to zero when the data size increases.
result_label: The results show that the proposed compression and aggregation scheme can make feasible OLAP of logistic regression in a data cube.
result_label: Further, it supports real-time logistic regression analysis of stream data, which can only be scanned once and cannot be permanently retained.
result_label: Experimental results validate our theoretical analysis and demonstrate that our method can dramatically save time and space costs with almost no degradation of the modeling accuracy.

===================================
paper_id: 381716; YEAR: 2016
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: specter
TITLE: Stacking With Auxiliary Features
ABSTRACT: background_label: Ensembling methods are well known for improving prediction accuracy.
background_label: However, they are limited in the sense that they cannot discriminate among component models effectively.
objective_label: In this paper, we propose stacking with auxiliary features that learns to fuse relevant information from multiple systems to improve performance.
method_label: Auxiliary features enable the stacker to rely on systems that not just agree on an output but also the provenance of the output.
method_label: We demonstrate our approach on three very different and difficult problems -- the Cold Start Slot Filling, the Tri-lingual Entity Discovery and Linking and the ImageNet object detection tasks.
result_label: We obtain new state-of-the-art results on the first two tasks and substantial improvements on the detection task, thus verifying the power and generality of our approach.

===================================
paper_id: 13196181; YEAR: 2008
adju relevance: Irrelevant (0)
difference: 0; annotator4: 0; annotator3: 0
sources: title_cbow200 - title_tfidfcbow200
TITLE: An accurate PSF model with few parameters for axially shift-variant deconvolution
ABSTRACT: background_label: Accurate knowledge of an imaging system's point spread function (PSF) is crucial for successful deconvolution.
background_label: For fluorescence microscopy, PSF estimations based on either theoretical models or experimental measurements are available.
background_label: However, due to the axially shift-variant nature of the PSF, neither method guarantees an estimate that is valid for the entire object space.
objective_label: In this work, we present a reduced- parameter version of a state-of-the-art theoretical model.
method_label: We give a maximum-likelihood based algorithm for the estimation of its parameters, and we show how a fit of our model to two axially isolated point source measurements in an experimental setup can be used to accurately reproduce measured PSFs within the entire specimen.

===================================
paper_id: 121565010; YEAR: 1999
adju relevance: Irrelevant (0)
difference: 1; annotator4: 1; annotator3: 0
sources: title_tfidfcbow200 - title_cbow200
TITLE: Nonlinear unbiased estimation in the linear regression model with nonnormal disturbances
ABSTRACT: background_label: Abstract In the application of the linear regression model there continues to be wide-spread use of the Least Squares Estimator (LSE) due to its theoretical optimality.
background_label: For example, it is well known that the LSE is the best unbiased estimator under normality while it remains best linear unbiased estimator (BLUE) when the normality assumption is dropped.
objective_label: In this paper we extend an approach given in Knautz (1993) that allows improvement of the LSE in the context of nonnormal and nonsymmetric error distributions.
method_label: It will be shown that there exist linear plus quadratic (LPQ) estimators, consisting of linear and quadratic terms in the dependent variable, which dominate the LS estimator, depending on second, third and fourth moments of the error distribution.
result_label: A simulation study illustrates that this remains true if the moments have to be estimated from the data.
result_label: Computation of confidence intervals using bootstrap methods reveal significant improvement compared with inference based on the LS especially for nonsymmetric distributions of the error term.

